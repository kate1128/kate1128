<font style="color:rgb(0,0,0);">人工智能正在从单一模态（如文本、语音、视觉等）向多种模态融合的通用人工智能方向演进。人工智能系统通过整合和理解多种模态的数据，能够在更广泛的领域中展现出更强大的能力和灵活性。 </font>

<br/>color2
<font style="color:rgb(0,0,0);">多模态大模型是一种</font>**<font style="color:rgb(0,0,0);">能够处理和理解多种类型数据（如文本、图像、音频和视频）的人工智能模型</font>**<font style="color:rgb(0,0,0);">，与专注于单一模态（如文本）的传统大语言模型不同，其拥有</font>**<font style="color:rgb(0,0,0);">整合来自视觉、听觉乃至可能涉及触觉等多种感官信息的能力</font>**<font style="color:rgb(0,0,0);">。</font>

<br/>

![](https://cdn.nlark.com/yuque/0/2025/png/2639475/1736751730141-465ad25f-e587-4178-8d90-b8a072f19ac2.png)

<font style="color:rgb(0,0,0);">这种模型的核心优势在于其强大的跨模态融合能力，它能够将不同模态的数据进行深度融合，从而捕捉到数据之间的内在联系和互补信息，以执行更为复杂且表现力更为丰富的任务，如图像识别与生成、文本生成与理解以及音频分析与生成等。构建多模态大模型的目的是增强模型在不同模态间的语义映射能力，更好地理解和连接各个模态之间的关系。构建统一的、跨模态、多任务的多模态模型已经成为人工智能发展的核心方向。 </font>

> <font style="color:rgb(0,0,0);">重点：多模态大模型的架构及其发展，重点介绍 ViT、CLIP、BLIP 和 BLIP2 等标志性模型。这些模型在融合图像、文本和其他模态的信息方面具有显著的性能提升，能够有效地融合和利用不同模态数据的信息，为图像理解、文本嵌入以及多模态任务提供了新的解决模型框架和性能标杆。</font>
>



