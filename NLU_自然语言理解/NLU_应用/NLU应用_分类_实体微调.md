[åˆ†ç±»å’Œå®ä½“çš„æå–](https://www.yuque.com/qiaokate/su87gb/sxlni0st18qafuye)ï¼Œå·²ç»ä»‹ç»äº†å„ç§åˆ†ç±»å’Œå®ä½“æå–çš„ç”¨æ³•ã€‚è¿™é‡Œæ˜¯æ›´å…·ä½“ã€å¸¸è§çš„ä»»åŠ¡ã€‚

<br/>success
é¦–å…ˆæ˜¯ä¸»é¢˜åˆ†ç±»ï¼Œç®€å•æ¥è¯´å°±æ˜¯ç»™å®šæ–‡æœ¬ï¼Œåˆ¤æ–­å±äºå“ªä¸€ç±»ä¸»é¢˜ã€‚

<br/>

### åˆ†ç±»
æ‰¾ä¸€ä¸ªæ–°é—»ä¸»é¢˜åˆ†ç±»çš„æ•°æ®é›†çœ‹çœ‹ï¼Œæ•°æ®é›†å–è‡ªï¼š[CLUEbenchmark/CLUE](https://github.com/CLUEbenchmark/CLUE)ï¼Œå…±15ä¸ªç±»åˆ«ã€‚

[tnews.json](https://www.yuque.com/attachments/yuque/0/2025/json/2639475/1735873919579-b61dec94-6e04-45bf-a4e9-0b23647aa62c.json)

```python
import pnlp
lines = pnlp.read_file_to_list_dict("./dataset/tnews.json")
len(lines)
```

<details class="lake-collapse"><summary id="ud90c2766"><span class="ne-text">outputï¼š</span></summary><pre data-language="json" id="iOXbm" class="ne-codeblock language-json"><code>10000</code></pre></details>


```python
from collections import Counter
ct = Counter([v["label_desc"] for v in lines])
```

```python
ct.most_common()
```

<details class="lake-collapse"><summary id="u3624e5fd"><span class="ne-text">outputï¼š</span></summary><pre data-language="json" id="VdsMm" class="ne-codeblock language-json"><code>[('news_tech', 1089),
 ('news_finance', 956),
 ('news_entertainment', 910),
 ('news_world', 905),
 ('news_car', 791),
 ('news_sports', 767),
 ('news_culture', 736),
 ('news_military', 716),
 ('news_travel', 693),
 ('news_game', 659),
 ('news_edu', 646),
 ('news_agriculture', 494),
 ('news_house', 378),
 ('news_story', 215),
 ('news_stock', 45)]</code></pre></details>


`stock`è¿™ä¸ªç±»åˆ«å¤ªå°‘äº†ï¼Œå…ˆç»™å®ƒå»æ‰ï¼š

```python
lines = [v for v in lines if v["label_desc"] != "news_stock"]
len(lines)
```

<details class="lake-collapse"><summary id="ub1eafb89"><span class="ne-text">outputï¼š</span></summary><pre data-language="json" id="HWE7p" class="ne-codeblock language-json"><code>9955</code></pre></details>


```python
def get_prompt(text):
    prompt = f"""å¯¹ç»™å®šæ–‡æœ¬è¿›è¡Œåˆ†ç±»ï¼Œç±»åˆ«åŒ…æ‹¬ï¼šç§‘æŠ€ã€é‡‘èã€å¨±ä¹ã€ä¸–ç•Œã€æ±½è½¦ã€è¿åŠ¨ã€æ–‡åŒ–ã€å†›äº‹ã€æ—…æ¸¸ã€æ¸¸æˆã€æ•™è‚²ã€å†œä¸šã€æˆ¿äº§ã€ç¤¾ä¼šã€‚

ç»™å®šæ–‡æœ¬ï¼š
{text}
ç±»åˆ«ï¼š
"""
    return prompt
```

```python
lines[59]
```

<details class="lake-collapse"><summary id="u8a465312"><span class="ne-text">outputï¼š</span></summary><pre data-language="json" id="aPTgC" class="ne-codeblock language-json"><code>{'label': '101',
 'label_desc': 'news_culture',
 'sentence': 'ä¸Šè”ï¼šé“¶ç¬›å¹å¼€äº‘å¤©æœˆï¼Œä¸‹è”æ€ä¹ˆå¯¹ï¼Ÿ',
 'keywords': ''}</code></pre></details>


```python
prompt = get_prompt(lines[59]["sentence"])
print(prompt)
```

<details class="lake-collapse"><summary id="u8db02a23"><span class="ne-text">outputï¼š</span></summary><pre data-language="json" id="EfokA" class="ne-codeblock language-json"><code>å¯¹ç»™å®šæ–‡æœ¬è¿›è¡Œåˆ†ç±»ï¼Œç±»åˆ«åŒ…æ‹¬ï¼šç§‘æŠ€ã€é‡‘èã€å¨±ä¹ã€ä¸–ç•Œã€æ±½è½¦ã€è¿åŠ¨ã€æ–‡åŒ–ã€å†›äº‹ã€æ—…æ¸¸ã€æ¸¸æˆã€æ•™è‚²ã€å†œä¸šã€æˆ¿äº§ã€ç¤¾ä¼šã€‚

ç»™å®šæ–‡æœ¬ï¼š
ä¸Šè”ï¼šé“¶ç¬›å¹å¼€äº‘å¤©æœˆï¼Œä¸‹è”æ€ä¹ˆå¯¹ï¼Ÿ
ç±»åˆ«ï¼š</code></pre></details>


```python
from openai import OpenAI
client = OpenAI(api_key="YOUR OPENAI KEY")

from zhipuai import ZhipuAI
zhipu_client = ZhipuAI(api_key="YOUR ZHIPU KEY")
```

```python
def ask(content, model="gpt-3.5-turbo"):
    response = client.chat.completions.create(
        model=model, 
        messages=[{"role": "user", "content": content}],
        temperature=0,
        max_tokens=300,
        top_p=1.0,
        frequency_penalty=0,
        presence_penalty=0,
    )

    ans = response.choices[0].message.content
    return ans

def ask_zhipu(content, model="glm-3-turbo"):
    response = zhipu_client.chat.completions.create(
        model=model, 
        messages=[{"role": "user", "content": content}],
        temperature=0.1,
        max_tokens=300,
        top_p=0.9,
    )

    ans = response.choices[0].message.content
    return ans
```

```python
ask(prompt)
```

<details class="lake-collapse"><summary id="u6b7fc72e"><span class="ne-text">outputï¼š</span></summary><pre data-language="json" id="J6iAk" class="ne-codeblock language-json"><code>'æ–‡åŒ–'</code></pre></details>


```python
ask_zhipu(prompt)
```

<details class="lake-collapse"><summary id="ubc96f6ab"><span class="ne-text">outputï¼š</span></summary><pre data-language="json" id="Vvq1Q" class="ne-codeblock language-json"><code>'ç»™å®šæ–‡æœ¬â€œä¸Šè”ï¼šé“¶ç¬›å¹å¼€äº‘å¤©æœˆï¼Œä¸‹è”æ€ä¹ˆå¯¹ï¼Ÿâ€å±äºæ–‡åŒ–ç±»åˆ«ã€‚è¿™æ˜¯å› ä¸ºæ–‡æœ¬æ¶‰åŠå¯¹è”çš„åˆ›ä½œï¼Œå¯¹è”æ˜¯ä¸­å›½ä¼ ç»Ÿæ–‡åŒ–ä¸­çš„ä¸€ç§æ–‡å­¦å½¢å¼ï¼Œä½“ç°äº†ä¸­å›½çš„è¯­è¨€è‰ºæœ¯å’Œæ–‡å­¦ä¿®å…»ã€‚'</code></pre></details>


å†è¯•å‡ ä¸ªå…¶ä»–ç±»åˆ«çš„ï¼š

```python
lines[1]
```

<details class="lake-collapse"><summary id="u30dc6dbf"><span class="ne-text">outputï¼š</span></summary><pre data-language="json" id="rmCXz" class="ne-codeblock language-json"><code>{'label': '110',
 'label_desc': 'news_military',
 'sentence': 'ä»¥è‰²åˆ—å¤§è§„æ¨¡ç©ºè¢­å¼€å§‹ï¼ä¼Šæœ—å¤šä¸ªå†›äº‹ç›®æ ‡é­é‡æ‰“å‡»ï¼Œèª“è¨€å¯¹ç­‰åå‡»',
 'keywords': 'ä¼Šæœ—,åœ£åŸå†›,å™åˆ©äºš,ä»¥è‰²åˆ—å›½é˜²å†›,ä»¥è‰²åˆ—'}</code></pre></details>


```python
prompt = get_prompt(lines[1]["sentence"])
print(prompt)
```

<details class="lake-collapse"><summary id="u9be334a8"><span class="ne-text">outputï¼š</span></summary><pre data-language="json" id="ii7jq" class="ne-codeblock language-json"><code>å¯¹ç»™å®šæ–‡æœ¬è¿›è¡Œåˆ†ç±»ï¼Œç±»åˆ«åŒ…æ‹¬ï¼šç§‘æŠ€ã€é‡‘èã€å¨±ä¹ã€ä¸–ç•Œã€æ±½è½¦ã€è¿åŠ¨ã€æ–‡åŒ–ã€å†›äº‹ã€æ—…æ¸¸ã€æ¸¸æˆã€æ•™è‚²ã€å†œä¸šã€æˆ¿äº§ã€ç¤¾ä¼šã€‚

ç»™å®šæ–‡æœ¬ï¼š
ä»¥è‰²åˆ—å¤§è§„æ¨¡ç©ºè¢­å¼€å§‹ï¼ä¼Šæœ—å¤šä¸ªå†›äº‹ç›®æ ‡é­é‡æ‰“å‡»ï¼Œèª“è¨€å¯¹ç­‰åå‡»
ç±»åˆ«ï¼š</code></pre></details>


```python
ask(prompt)
```

<details class="lake-collapse"><summary id="u73341abb"><span class="ne-text">outputï¼š</span></summary><pre data-language="json" id="Q6ZGe" class="ne-codeblock language-json"><code>'å†›äº‹'</code></pre></details>


```python
lines[2]
```

<details class="lake-collapse"><summary id="u4a56b451"><span class="ne-text">outputï¼š</span></summary><pre data-language="json" id="tPNVV" class="ne-codeblock language-json"><code>{'label': '104',
 'label_desc': 'news_finance',
 'sentence': 'å‡ºæ ä¸€å¤´çŒªäºæŸ300å…ƒï¼Œç©¶ç«Ÿè°èƒ½ç¬‘åˆ°æœ€åï¼',
 'keywords': 'å•†å“çŒª,å…»çŒª,çŒªä»·,ä»”çŒª,é¥²æ–™'}</code></pre></details>


```python
prompt = get_prompt(lines[2]["sentence"])
print(prompt)
```

<details class="lake-collapse"><summary id="u5bede74b"><span class="ne-text">outputï¼š</span></summary><pre data-language="json" id="WMgyN" class="ne-codeblock language-json"><code>å¯¹ç»™å®šæ–‡æœ¬è¿›è¡Œåˆ†ç±»ï¼Œç±»åˆ«åŒ…æ‹¬ï¼šç§‘æŠ€ã€é‡‘èã€å¨±ä¹ã€ä¸–ç•Œã€æ±½è½¦ã€è¿åŠ¨ã€æ–‡åŒ–ã€å†›äº‹ã€æ—…æ¸¸ã€æ¸¸æˆã€æ•™è‚²ã€å†œä¸šã€æˆ¿äº§ã€ç¤¾ä¼šã€‚

ç»™å®šæ–‡æœ¬ï¼š
å‡ºæ ä¸€å¤´çŒªäºæŸ300å…ƒï¼Œç©¶ç«Ÿè°èƒ½ç¬‘åˆ°æœ€åï¼
ç±»åˆ«ï¼š</code></pre></details>


```python
ask_zhipu(prompt)
```

<details class="lake-collapse"><summary id="uf96be6c4"><span class="ne-text">outputï¼š</span></summary><pre data-language="json" id="R7juX" class="ne-codeblock language-json"><code>'ç»™å®šæ–‡æœ¬â€œå‡ºæ ä¸€å¤´çŒªäºæŸ300å…ƒï¼Œç©¶ç«Ÿè°èƒ½ç¬‘åˆ°æœ€åï¼â€å¯ä»¥è¢«å½’ç±»ä¸ºâ€œå†œä¸šâ€ç±»åˆ«ã€‚è¿™æ˜¯å› ä¸ºæ–‡æœ¬å†…å®¹æ¶‰åŠå†œä¸šç”Ÿäº§å’Œå…»æ®–ä¸šçš„æƒ…å†µï¼ŒäºæŸ300å…ƒåæ˜ çš„æ˜¯å†œä¸šé¢†åŸŸçš„ç»æµç°è±¡ï¼Œè€Œâ€œè°èƒ½ç¬‘åˆ°æœ€åâ€å¯èƒ½æŒ‡çš„æ˜¯åœ¨å½“å‰æƒ…å†µä¸‹ï¼Œå…»æ®–ä¸šè€…å¦‚ä½•é€šè¿‡å„ç§æ–¹å¼è½¬äºä¸ºç›ˆï¼Œä¿æŒç«äº‰åŠ›ã€‚'</code></pre></details>
è¿™ä¸ªæœ‰ç‚¹è¿·ç³Šï¼Œä¸è¿‡ã€Œå†œä¸šã€è¿™ä¸ªç±»åˆ«æ„Ÿè§‰ä¹Ÿæ²¡é—®é¢˜ã€‚å½“ç„¶ï¼Œé‡åˆ°æœ‰é”™è¯¯çš„æƒ…å†µï¼Œèµ·ç è¿˜æœ‰ä¸¤ç§æ‰‹æ®µæ¥è§£å†³ï¼š

+ `Few-Shot`ï¼Œå¯ä»¥æ¯æ¬¡éšæœºä»æ•°æ®é›†é‡ŒæŠ½å‡ æ¡å‡ºæ¥ä½œä¸º`Prompt`çš„ä¸€éƒ¨åˆ†ã€‚
+ `Fine-Tuning`ï¼ŒæŠŠè‡ªå·±çš„æ•°æ®é›†æŒ‰æŒ‡å®šæ ¼å¼å‡†å¤‡å¥½ï¼Œæäº¤ç»™APIï¼Œè®©å®ƒå¸®å¾®è°ƒä¸€ä¸ªå±äºè‡ªå·±çš„æ¨¡å‹ï¼Œå®ƒåœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šå­¦ä¹ è¿‡ã€‚

`Few-Shot`æœ€å…³é”®çš„æ˜¯å¦‚ä½•æ‰¾åˆ°è¿™ä¸ªã€Œ`Few`ã€ï¼Œæ¢å¥è¯è¯´ï¼Œæ‹¿ä»€ä¹ˆCaseç»™æ¨¡å‹å½“åšå‚è€ƒæ ·æœ¬ã€‚å¯¹äºç±»åˆ«æ¯”è¾ƒå¤šçš„å¤šåˆ†ç±»ï¼ˆå®é™…å·¥ä½œä¸­ï¼Œæˆç™¾ä¸Šåƒä¸­Labelæ˜¯å¾ˆå¸¸è§çš„ï¼‰ï¼ŒFew-Shotå³ä½¿æ¯ä¸ªLabelä¸€ä¸ªä¾‹å­ï¼Œè¿™ä¸Šä¸‹æ–‡é•¿åº¦ä¹Ÿä¸å¾—äº†ã€‚ä¸å¤ªç°å®ã€‚è¿™æ—¶å€™å…¶å®Few-Shotæœ‰ç‚¹ä¸å¤ªæ–¹ä¾¿äº†ã€‚

å½“ç„¶ï¼Œå¦‚æœéè¦ç”¨ä¹Ÿä¸æ˜¯ä¸è¡Œï¼Œè¿˜æ˜¯æœ€å¸¸ç”¨çš„ç­–ç•¥ï¼šå…ˆå¬å›å‡ ä¸ªç›¸ä¼¼å¥ï¼Œç„¶åæŠŠç›¸ä¼¼å¥çš„å†…å®¹å’Œç±»åˆ«ä½œä¸ºFew-Shotçš„ä¾‹å­ï¼Œè®©æ¥å£æ¥é¢„æµ‹ç»™å®šå¥å­çš„ç±»åˆ«ã€‚

### å¾®è°ƒï¼ˆFine-Tuningï¼‰
ä¸è¿‡ï¼Œè¿˜å¯ä»¥ä½¿ç”¨å¾®è°ƒï¼ˆFine-Tuningï¼‰æ–¹æ³•åœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œç®€å•æ¥è¯´å°±æ˜¯è®©æ¨¡å‹ã€Œç†Ÿæ‚‰ã€ç‹¬ç‰¹çš„æ•°æ®ï¼Œè¿›è€Œè®©å…¶å…·å¤‡åœ¨ç±»ä¼¼æ•°æ®é›†ä¸Šè¯†åˆ«å‡ºç±»åˆ«çš„èƒ½åŠ›ã€‚

<br/>color2
æ¥ä¸‹æ¥ï¼Œå°±è®©çœ‹çœ‹å…·ä½“æ€ä¹ˆåšï¼Œä¸€èˆ¬åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ­¥éª¤ï¼š

1. **å‡†å¤‡æ•°æ®**ï¼šæŒ‰æ¥å£è¦æ±‚çš„æ ¼å¼æŠŠæ•°æ®å‡†å¤‡å¥½ï¼Œè¿™é‡Œçš„æ•°æ®å°±æ˜¯è‡ªå·±çš„æ•°æ®é›†ï¼Œè‡³å°‘åŒ…å«ä¸€æ®µæ–‡æœ¬å’Œä¸€ä¸ªç±»åˆ«ã€‚
2. **å¾®è°ƒ**ï¼šä½¿ç”¨å¾®è°ƒæ¥å£å°†åˆšåˆšçš„æ•°æ®ä¼ é€’è¿‡å»ï¼Œç”±æœåŠ¡å™¨è‡ªåŠ¨å®Œæˆå¾®è°ƒï¼Œå¾®è°ƒå®Œæˆåå¯ä»¥å¾—åˆ°ä¸€ä¸ªæ–°çš„model_idã€‚æ³¨æ„ï¼Œè¿™ä¸ªmodel_idåªå±äºä½ è‡ªå·±ï¼Œä¸è¦å°†å®ƒå…¬å¼€ç»™å…¶ä»–äººã€‚
3. **ä½¿ç”¨æ–°çš„æ¨¡å‹è¿›è¡Œæ¨ç†**ï¼šå—¯ï¼Œè¿™ä¸ªå¾ˆç®€å•ï¼ŒæŠŠåŸæ¥æ¥å£é‡Œçš„`model`å‚æ•°å†…å®¹æ¢æˆçš„model_idå³å¯ã€‚

<br/>

å’±ä»¬æ¥ä¸‹æ¥å°±æ¥è°ƒè°ƒè¿™ä¸ªå¤šåˆ†ç±»æ¨¡å‹ï¼Œåªå–å100æ¡ä½œä¸ºè®­ç»ƒé›†ã€‚

```python
train_lines = lines[-100:]
```

```python
import pandas as pd
```

```python
train = pd.DataFrame(train_lines)
train.shape
```

<details class="lake-collapse"><summary id="ucb436364"><span class="ne-text" style="color: var(--jp-cell-prompt-not-active-font-color)">outputï¼š</span></summary><pre data-language="json" id="OptVo" class="ne-codeblock language-json"><code>(100, 4)</code></pre></details>


```python
train.head()
```

Output:

![](https://cdn.nlark.com/yuque/0/2024/png/2639475/1735611415716-40b2d0d4-fe73-4c93-aee6-c561f84bc82a.png)



```python
train.label_desc.value_counts()
```

<details class="lake-collapse"><summary id="u6df4c7b4"><span class="ne-text" style="color: var(--jp-cell-prompt-not-active-font-color)">outputï¼š</span></summary><pre data-language="json" id="A2Ily" class="ne-codeblock language-json"><code>label_desc
news_world            12
news_tech             11
news_finance          11
news_game             11
news_sports           10
news_travel            9
news_edu               7
news_entertainment     7
news_agriculture       5
news_military          5
news_house             5
news_car               3
news_story             2
news_culture           2
Name: count, dtype: int64</code></pre></details>


#### Step1ï¼šå‡†å¤‡æ•°æ®
è¦ä¿è¯æœ‰ä¸¤åˆ—ä¸ºï¼š`role`å’Œ`content`ï¼Œæ”¯æŒå¤šè½®ã€‚å…·ä½“å¯ä»¥è¿›ä¸€æ­¥å‚è€ƒï¼š[Fine-tuning - OpenAI API](https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset)

```python
df_train = train[["sentence", "label_desc"]]
df_train.head()
```

Output:

![](https://cdn.nlark.com/yuque/0/2024/png/2639475/1735611454365-7811bb3a-0411-4ea4-aa78-3467302b442e.png)

```python
desc_map = {
    "news_tech": "ç§‘æŠ€",
    "news_finance": "é‡‘è",
    "news_entertainment": "å¨±ä¹",
    "news_game": "æ¸¸æˆ",
    "news_travel": "æ—…æ¸¸",
    "news_sports": "è¿åŠ¨",
    "news_military": "å†›äº‹",
    "news_world": "ä¸–ç•Œ",
    "news_car": "æ±½è½¦",
    "news_culture": "æ–‡åŒ–",
    "news_edu": "æ•™è‚²",
    "news_agriculture": "å†œä¸š",
    "news_house": "æˆ¿äº§",
    "news_story": "ç¤¾ä¼š",
}
```

```python
data_lines = []
for v in df_train.itertuples():
    msg = {
        "messages": [
            {"role": "user", "content": get_prompt(v.sentence)},
            {"role": "assistant", "content": desc_map[v.label_desc]},
        ]
    }
    data_lines.append(msg)
```

```python
out_fn = "./dataset/tnews-finetuning.jsonl"
pnlp.write_list_dict_to_file(out_fn, data_lines)
```

æ¥ä¸‹æ¥å¯ä»¥æ£€æŸ¥æ•°æ®å¹¶ç»Ÿè®¡Tokenï¼Œå¯å‚è€ƒæ–‡æ¡£ã€‚å¦‚æœä½ å¯¹è‡ªå·±çš„æ•°æ®æ¯”è¾ƒç†Ÿæ‚‰ï¼Œä¸åšè¿™äº›ä¹Ÿæ˜¯å¯ä»¥çš„ã€‚å…¶å®å°±æ˜¯å¯¹æ•°æ®åšä¸€ä¸‹åŸºæœ¬ç»Ÿè®¡ï¼Œåšåˆ°å¿ƒä¸­æœ‰æ•°ã€‚

+ [https://cookbook.openai.com/examples/chat_finetuning_data_prep](https://cookbook.openai.com/examples/chat_finetuning_data_prep)

ä»¥ä¸‹ä»£ç å‡æ¥è‡ªæ–‡æ¡£ã€‚

```python
!pip install tiktoken
```

```python
import json
import tiktoken # for token counting
import numpy as np
from collections import defaultdict
```

```python
# Format error checks
format_errors = defaultdict(int)

for ex in data_lines:
    if not isinstance(ex, dict):
        format_errors["data_type"] += 1
        continue
        
    messages = ex.get("messages", None)
    if not messages:
        format_errors["missing_messages_list"] += 1
        continue
        
    for message in messages:
        if "role" not in message or "content" not in message:
            format_errors["message_missing_key"] += 1
        
        if any(k not in ("role", "content", "name", "function_call", "weight") for k in message):
            format_errors["message_unrecognized_key"] += 1
        
        if message.get("role", None) not in ("system", "user", "assistant", "function"):
            format_errors["unrecognized_role"] += 1
            
        content = message.get("content", None)
        function_call = message.get("function_call", None)
        
        if (not content and not function_call) or not isinstance(content, str):
            format_errors["missing_content"] += 1
    
    if not any(message.get("role", None) == "assistant" for message in messages):
        format_errors["example_missing_assistant_message"] += 1

if format_errors:
    print("Found errors:")
    for k, v in format_errors.items():
        print(f"{k}: {v}")
else:
    print("No errors found")
```

<details class="lake-collapse"><summary id="ue840535a"><span class="ne-text" style="color: var(--jp-cell-prompt-not-active-font-color)">outputï¼š</span></summary><pre data-language="json" id="Trl8E" class="ne-codeblock language-json"><code>No errors found</code></pre></details>


```python
encoding = tiktoken.get_encoding("cl100k_base")

def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):
    num_tokens = 0
    for message in messages:
        num_tokens += tokens_per_message
        for key, value in message.items():
            num_tokens += len(encoding.encode(value))
            if key == "name":
                num_tokens += tokens_per_name
    num_tokens += 3
    return num_tokens

def num_assistant_tokens_from_messages(messages):
    num_tokens = 0
    for message in messages:
        if message["role"] == "assistant":
            num_tokens += len(encoding.encode(message["content"]))
    return num_tokens

def print_distribution(values, name):
    print(f"\n#### Distribution of {name}:")
    print(f"min / max: {min(values)}, {max(values)}")
    print(f"mean / median: {np.mean(values)}, {np.median(values)}")
    print(f"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}")
```



```python
# Warnings and tokens counts
n_missing_system = 0
n_missing_user = 0
n_messages = []
convo_lens = []
assistant_message_lens = []

for ex in data_lines:
    messages = ex["messages"]
    if not any(message["role"] == "system" for message in messages):
        n_missing_system += 1
    if not any(message["role"] == "user" for message in messages):
        n_missing_user += 1
    n_messages.append(len(messages))
    convo_lens.append(num_tokens_from_messages(messages))
    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))
    
print("Num examples missing system message:", n_missing_system)
print("Num examples missing user message:", n_missing_user)
print_distribution(n_messages, "num_messages_per_example")
print_distribution(convo_lens, "num_total_tokens_per_example")
print_distribution(assistant_message_lens, "num_assistant_tokens_per_example")
n_too_long = sum(l > 4096 for l in convo_lens)
print(f"\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning")
```

<details class="lake-collapse"><summary id="ud5c2bd99"><span class="ne-text" style="color: var(--jp-cell-prompt-not-active-font-color)">outputï¼š</span></summary><pre data-language="json" id="ZF840" class="ne-codeblock language-json"><code>Num examples missing system message: 100
Num examples missing user message: 0

#### Distribution of num_messages_per_example:
min / max: 2, 2
mean / median: 2.0, 2.0
p5 / p95: 2.0, 2.0

#### Distribution of num_total_tokens_per_example:
min / max: 101, 146
mean / median: 119.9, 120.0
p5 / p95: 107.0, 132.10000000000002

#### Distribution of num_assistant_tokens_per_example:
min / max: 2, 5
mean / median: 3.04, 3.0
p5 / p95: 2.0, 4.0

0 examples may be over the 4096 token limit, they will be truncated during fine-tuning</code></pre></details>


```python
# Pricing and default n_epochs estimate
MAX_TOKENS_PER_EXAMPLE = 4096

TARGET_EPOCHS = 3
MIN_TARGET_EXAMPLES = 100
MAX_TARGET_EXAMPLES = 25000
MIN_DEFAULT_EPOCHS = 1
MAX_DEFAULT_EPOCHS = 25

n_epochs = TARGET_EPOCHS
n_train_examples = len(data_lines)
if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:
    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)
elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:
    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)

n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)
print(f"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training")
print(f"By default, you'll train for {n_epochs} epochs on this dataset")
print(f"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens")
```

<details class="lake-collapse"><summary id="u4ef8f7df"><span class="ne-text" style="color: var(--jp-cell-prompt-not-active-font-color)">outputï¼š</span></summary><pre data-language="json" id="auXGg" class="ne-codeblock language-json"><code>Dataset has ~11990 tokens that will be charged for during training
By default, you'll train for 3 epochs on this dataset
By default, you'll be charged for ~35970 tokens</code></pre></details>


#### Step2ï¼šå¾®è°ƒï¼ˆOpenAIï¼‰
ç›¸å…³æ–‡æ¡£ï¼š

+ [https://platform.openai.com/docs/guides/fine-tuning/create-a-fine-tuned-model](https://platform.openai.com/docs/guides/fine-tuning/create-a-fine-tuned-model)
+ [https://platform.openai.com/docs/api-reference/fine-tuning/create](https://platform.openai.com/docs/api-reference/fine-tuning/create)

æ™ºè°±AIçš„å¾®è°ƒå’ŒOpenAIå·®ä¸å¤šï¼Œå…¶å®æ¯å®¶éƒ½å·®ä¸å¤ªå¤šã€‚è€Œä¸”æœ‰äº›å¹³å°è¿˜æ”¯æŒå›¾å½¢åŒ–ç•Œé¢ï¼Œæ“ä½œèµ·æ¥ä¹Ÿéå¸¸ç®€å•ï¼Œåªè¦ä¸Šä¼ æ•°æ®è¿è¡Œå¾®è°ƒä»»åŠ¡å³å¯ã€‚å¾®è°ƒç»“æŸåä¼šå¾—åˆ°ä¸€ä¸ªæ–°çš„æ¨¡å‹IDï¼Œè°ƒç”¨æ—¶æŠŠæ¨¡å‹IDæ¢ä¸€ä¸‹å³å¯ã€‚

```python
file = client.files.create(
    file=open(out_fn, "rb"),
    purpose="fine-tune"
)
```



```python
job = client.fine_tuning.jobs.create(
    training_file=file.id,
    model="gpt-3.5-turbo",
    hyperparameters={
        "n_epochs": 2
    }
)
```



```python
# åˆ—å‡º10ä¸ªäº‹ä»¶
# client.fine_tuning.jobs.list(limit=10)

# è·å–å¾®è°ƒçš„çŠ¶æ€
# client.fine_tuning.jobs.retrieve("ftjob-abc123")

# å–æ¶ˆä¸€ä¸ªjob
# client.fine_tuning.jobs.cancel("ftjob-abc123")

# åˆ—å‡ºå½“å‰å¾®è°ƒçš„10ä¸ªäº‹ä»¶
# client.fine_tuning.jobs.list_events(fine_tuning_job_id="ftjob-abc123", limit=10)

# åˆ é™¤ä¸€ä¸ªå¾®è°ƒæ¨¡å‹
# client.models.delete("ft:gpt-3.5-turbo:acemeco:suffix:abc123")
```



```python
# å¯ä»¥ç”¨è¿™ä¸ªå‘½ä»¤ä¸å®šæ—¶è·å–çŠ¶æ€ï¼Œä¸ä¹‹å‰ç‰ˆæœ¬ä¸€è‡´ã€‚
# å¯é‡å¤æ‰§è¡Œï¼Œæ³¨æ„è§‚å¯Ÿå…¶ä¸­çš„â€œstatusâ€å­—æ®µ
ft_job = client.fine_tuning.jobs.retrieve(job.id)
ft_job
```

<details class="lake-collapse"><summary id="u4178ed2a"><span class="ne-text" style="color: var(--jp-cell-prompt-not-active-font-color)">outputï¼š</span></summary><pre data-language="json" id="aV56G" class="ne-codeblock language-json"><code>FineTuningJob(id='ftjob-hQpExgNbOsnnOV4DLZn1SDUk', created_at=1716303071, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-3.5-turbo-0125:personal::9RLH3nut', finished_at=1716303660, hyperparameters=Hyperparameters(n_epochs=2, batch_size=1, learning_rate_multiplier=2), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-U35hu1wdD7w3HnkgJ5fdBW8m', result_files=['file-PhKdwGbqzf5IjiecDe1kiU4M'], seed=1873867419, status='succeeded', trained_tokens=23580, training_file='file-XCZvA9drMr1UebtoGN1vGM5D', validation_file=None, estimated_finish=None, integrations=[], user_provided_suffix=None)</code></pre></details>


å¤§æ¦‚ç­‰å¾…10-15åˆ†é’Ÿï¼ˆè§†æ•°æ®é‡å’Œæ’é˜Ÿæƒ…å†µï¼Œå¯èƒ½ä¼šæ›´ä¹…ï¼‰åï¼Œå°±å¾—åˆ°äº†ä¸Šé¢çš„ç»“æœã€‚è¿˜å¯ä»¥é€šè¿‡ä¸‹é¢çš„APIè·å–checkpointã€‚

```python
import requests
```

```python
headers = {"Authorization": f"Bearer {client.api_key}"}
```

```python
resp = requests.get(
    f"https://api.openai.com/v1/fine_tuning/jobs/{job.id}/checkpoints",
    headers=headers
)
```

```python
resp.json()
```

<details class="lake-collapse"><summary id="uf40f2501"><span class="ne-text" style="color: var(--jp-cell-prompt-not-active-font-color)">outputï¼š</span></summary><pre data-language="json" id="bVeNf" class="ne-codeblock language-json"><code>{'object': 'list',
 'data': [{'object': 'fine_tuning.job.checkpoint',
   'id': 'ftckpt_0iu99HCJW9FllVqvrD16hBER',
   'created_at': 1716303657,
   'fine_tuned_model_checkpoint': 'ft:gpt-3.5-turbo-0125:personal::9RLH3nut',
   'fine_tuning_job_id': 'ftjob-hQpExgNbOsnnOV4DLZn1SDUk',
   'metrics': {'step': 200,
    'train_loss': 7.629394644936838e-07,
    'train_mean_token_accuracy': 1.0},
   'step_number': 200},
  {'object': 'fine_tuning.job.checkpoint',
   'id': 'ftckpt_4QVDvsnDl1QdaabczYpi0nuI',
   'created_at': 1716303462,
   'fine_tuned_model_checkpoint': 'ft:gpt-3.5-turbo-0125:personal::9RLH30Ax:ckpt-step-100',
   'fine_tuning_job_id': 'ftjob-hQpExgNbOsnnOV4DLZn1SDUk',
   'metrics': {'step': 100,
    'train_loss': 2.2964580059051514,
    'train_mean_token_accuracy': 0.8333333134651184},
   'step_number': 100}],
 'has_more': False,
 'first_id': 'ftckpt_0iu99HCJW9FllVqvrD16hBER',
 'last_id': 'ftckpt_4QVDvsnDl1QdaabczYpi0nuI'}</code></pre></details>


**ğŸ“¢****æ³¨æ„ï¼šå¦‚æœä½ å¾®è°ƒçš„æ˜¯babbage-002anddavinci-002ï¼Œå¯ä»¥åœ¨GitHubé¦–é¡µåˆ‡æ¢åˆ°v1.0åˆ†æ”¯ï¼Œå‚è€ƒé‚£é‡Œçš„ä»£ç ã€‚**

æ›´å¤šä¿¡æ¯å¯å‚è€ƒæ–‡æ¡£ï¼š[https://platform.openai.com/docs/guides/fine-tuning/use-a-fine-tuned-model](https://platform.openai.com/docs/guides/fine-tuning/use-a-fine-tuned-model)

#### Step3ï¼šä½¿ç”¨
```python
def ask(content, model="gpt-3.5-turbo", max_tokens=5):
    response = client.chat.completions.create(
        model=model, 
        messages=[{"role": "user", "content": content}],
        temperature=0,
        max_tokens=max_tokens,
        top_p=1.0,
        frequency_penalty=0,
        presence_penalty=0,
    )

    ans = response.choices[0].message.content
    return ans

def ask_zhipu(content, model="glm-3-turbo", max_tokens=5):
    response = zhipu_client.chat.completions.create(
        model=model, 
        messages=[{"role": "user", "content": content}],
        temperature=0.1,
        max_tokens=max_tokens,
        top_p=0.9,
    )

    ans = response.choices[0].message.content
    return ans
```

```python
idx = -1
prompt = get_prompt(lines[idx]["sentence"])
print(prompt)
print("\nTrue label:", desc_map[lines[idx]["label_desc"]])
```

<details class="lake-collapse"><summary id="u2db5c8a0"><span class="ne-text" style="color: var(--jp-cell-prompt-not-active-font-color)">outputï¼š</span></summary><pre data-language="json" id="Um8H8" class="ne-codeblock language-json"><code>å¯¹ç»™å®šæ–‡æœ¬è¿›è¡Œåˆ†ç±»ï¼Œç±»åˆ«åŒ…æ‹¬ï¼šç§‘æŠ€ã€é‡‘èã€å¨±ä¹ã€ä¸–ç•Œã€æ±½è½¦ã€è¿åŠ¨ã€æ–‡åŒ–ã€å†›äº‹ã€æ—…æ¸¸ã€æ¸¸æˆã€æ•™è‚²ã€å†œä¸šã€æˆ¿äº§ã€ç¤¾ä¼šã€‚

ç»™å®šæ–‡æœ¬ï¼š
æ—¥æœ¬è™è§†çœˆçœˆâ€œæ­¦åŠ›å¤ºå²›â€, ç¾å†›å‘ä¿„åé™¢å¼€ç«ï¼Œæ™®äº¬ç»ˆä¸å†å¿ï¼
ç±»åˆ«ï¼š


True label: ä¸–ç•Œ</code></pre></details>


```python
# åŸæ¥çš„ï¼Œå¾®è°ƒä¹‹å‰çš„
ask(prompt)
```

<details class="lake-collapse"><summary id="u2bc2d794"><span class="ne-text" style="color: var(--jp-cell-prompt-not-active-font-color)">outputï¼š</span></summary><pre data-language="json" id="Cuyoy" class="ne-codeblock language-json"><code>'å†›äº‹'</code></pre></details>


```python
# å¾®è°ƒä¹‹åçš„
ask(prompt, ft_job.fine_tuned_model)
```

<details class="lake-collapse"><summary id="u7fcf05b5"><span class="ne-text" style="color: var(--jp-cell-prompt-not-active-font-color)">outputï¼š</span></summary><pre data-language="json" id="zCcIy" class="ne-codeblock language-json"><code>'ä¸–ç•Œ'</code></pre></details>


å¯ä»¥çœ‹åˆ°ï¼Œå¾®è°ƒåè¾“å‡ºçš„æ˜¯è®­ç»ƒé›†é‡Œçš„çœŸå®ç±»åˆ«ã€‚

ä¸Šé¢ä»‹ç»äº†ä¸»é¢˜åˆ†ç±»çš„å¾®è°ƒã€‚å®ä½“æŠ½å–çš„å¾®è°ƒä¹Ÿæ˜¯ç±»ä¼¼çš„ï¼Œæ¯”å¦‚å¯ä»¥è¿™æ ·å†™ï¼š

```python
item = {
    "prompt":"è¯·æŠ½å–ç»™å®šTextä¸­çš„å®ä½“ï¼Œå®ä½“åŒ…æ‹¬äººåå’Œåœ°åã€‚\nText:<ä»»æ„æ–‡æœ¬>\n", 
    "completion":" <æƒ³è¦çš„å®ä½“åˆ—è¡¨>"
}
```

```python
[
    {"role": "user", "content": item["prompt"]},
    {"role": "assistant", "content": item["completion"]}
]
```

<details class="lake-collapse"><summary id="uedec4fd6"><span class="ne-text" style="color: var(--jp-cell-prompt-not-active-font-color)">outputï¼š</span></summary><pre data-language="json" id="Eb7z7" class="ne-codeblock language-json"><code>[{'role': 'user', 'content': 'è¯·æŠ½å–ç»™å®šTextä¸­çš„å®ä½“ï¼Œå®ä½“åŒ…æ‹¬äººåå’Œåœ°åã€‚\nText:&lt;ä»»æ„æ–‡æœ¬&gt;\n'},
 {'role': 'assistant', 'content': ' &lt;æƒ³è¦çš„å®ä½“åˆ—è¡¨&gt;'}]</code></pre></details>
å½“ç„¶ï¼Œä¹Ÿå¯ä»¥æŒ‡å®šè¾“å‡ºç±»å‹ã€æ ¼å¼ç­‰ï¼Œå°±åƒå‰é¢æç¤ºè¯æ‰€ç¤ºçš„é‚£æ ·ã€‚ç›¸ä¿¡å¤§å®¶åº”è¯¥å¾ˆå®¹æ˜“ç†è§£ï¼Œä¸å¦¨è‡ªå·±åšä¸€äº›å°è¯•ã€‚å°¤å…¶æ˜¯ç»™ä¸€äº›ä¸“ä¸šé¢†åŸŸçš„å®ä½“è¿›è¡Œå¾®è°ƒï¼Œå¯¹æ¯”ä¸€ä¸‹å¾®è°ƒå‰åçš„æ•ˆæœã€‚å½“ç„¶ï¼Œä½ ä¹Ÿå¯ä»¥æŒ‡å®šè¾“å‡ºæ ¼å¼ç­‰ã€‚

å¦‚æœå¯¹è¿™å—å†…å®¹æ„Ÿå…´è¶£ï¼Œå¯ä»¥è¿›ä¸€æ­¥é˜…è¯»

[Fine-tuning - OpenAI API](https://platform.openai.com/docs/guides/fine-tuning)

[[PUBLIC] Best practices for fine-tuning GPT-3 to classify text - Google Docs](https://docs.google.com/document/d/1rqj7dkuvl7Byd5KQPUJRxc19BJt8wo0yHNwK84KfU3Q/edit)



