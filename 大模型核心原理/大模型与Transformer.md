> **Transformer的优势**：
>
> + **位置编码：**通过位置编码理解词之间的顺序关系，允许并行计算，提高训练效率**。**
> + **自注意力机制：**能够学习输入序列中所有词的相关性，赋予每个词不同的注意力权重，专注于真正重要的部分**。**
>

**大语言模型（LLM, Large Language Model）** 是用于执行自然语言相关任务的深度学习模型。简单来说，给模型输入一些文本内容，它就能返回相应的输出。这些任务可以是续写、分类、总结、改写、翻译等等。

# 大语言模型的“大”体现在哪里？
+ **训练数据巨大**：大语言模型首先需要通过大量文本进行无监督学习。以GPT3为例，它的训练数据来自广泛的互联网文本语料，如电子书、新闻文章、博文、论文、百科、社交媒体帖子等。这些文本数据没有人工标签，模型主要学习单词与上下文之间的关系，以更好地理解文本并生成准确预测。
+ **参数量巨大**：参数在模型中用于刻画从庞大训练数据集中学习到的规律，决定模型如何响应输入数据。随着参数增加，模型能力增强，甚至能创造出全新的内容。例如，GPT系列的参数从GPT1的1.17亿增长到GPT3的1750亿。

# 为什么Transformer是关键？
在Transformer架构出现之前，语言模型主要使用循环神经网络（RNN）。但RNN存在顺序处理、无法并行计算和难以处理长序列的问题。

**RNN的劣势**：

+ 顺序处理：无法并行计算。
+ 难以处理长序列的文本：容易造成遗忘。

<br/>color2
⚠️**Transformer 如何解决的？**

**Transformer**通过自注意力机制和位置编码解决了这些问题，实现了对所有输入词的同时关注和理解，以及长距离依赖性的捕获。

<br/>

**Transformer的优势**：

+ **位置编码**：通过位置编码理解词之间的顺序关系，允许并行计算，提高训练效率。
+ **自注意力机制**：能够学习输入序列中所有词的相关性，赋予每个词不同的注意力权重，专注于真正重要的部分。

