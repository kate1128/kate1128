> 1. 海量文本的无监督学习 -> 得到基座大模型
> 2. 有监督微调->得到可以对话的大模型
> 3. 奖励模型的训练->得到可以评价回答的模型
> 4. 强化学习训练->得到更符合人类价值观的优秀模型
>

# LLM 的发展历程
语言建模的研究可以追溯到`20 世纪 90 年代`，当时的研究主要集中在采用**统计学习方法**来预测词汇，通过分析前面的词汇来预测下一个词汇。但在理解复杂语言规则方面存在一定局限性。

随后，研究人员不断尝试改进，`2003 年`深度学习先驱 **Bengio** 在他的经典论文 `《A Neural Probabilistic Language Model》`中，首次将深度学习的思想融入到语言模型中。强大的**神经网络模型**，相当于为计算机提供了强大的"大脑"来理解语言，让模型可以更好地捕捉和理解语言中的复杂关系。

`2018 年`左右，**Transformer 架构的神经网络模型**开始崭露头角。通过大量文本数据训练这些模型，使它们能够通过阅读大量文本来深入理解语言规则和模式，就像让计算机阅读整个互联网一样，对语言有了更深刻的理解，极大地提升了模型在各种自然语言处理任务上的表现。

与此同时，研究人员发现，随着**语言模型规模的扩大（增加模型大小或使用更多数据）**，模型展现出了一些惊人的能力，在各种任务中的表现均显著提升。这一发现标志着大型语言模型（LLM）时代的开启。

# 过程
## 海量文本的无监督学习➡️得到基座大模型
<br/>color2
+ **无监督学习** ：处理没有标签的数据，让计算机自主发现数据中的模式。[什么是 机器学习](https://www.yuque.com/qiaokate/su87gb/lt6p24ggetlzag85)

<br/>

1. **原料**：首先，我们需要海量的文本数据，这些数据可以来自互联网上的各种语料库，包括书籍、新闻、科学论文、社交媒体帖子等等。这些文本将作为模型的“原料”，供模型学习。
2. **目标**：通过无监督学习，让模型能够基于上下文预测下一个token。这里用到了Transformer技术，它可以根据上下文预测被掩码的token。
3. **技术过程**：无监督学习让模型在没有标签的数据上进行训练，通过比较正确答案和模型的预测结果，计算交叉熵损失，并使用优化算法更新模型的权重。随着见过的文本越来越多，模型生成的能力也会越来越好。

## 有监督微调➡️得到可以对话的大模型
<br/>color2
+ **监督学习** ：学习带有标签的原始数据。目标是发现原始数据与标签之间的映射关系，从而预测新数据。[什么是 机器学习](https://www.yuque.com/qiaokate/su87gb/lt6p24ggetlzag85)

<br/>

1. **原料**：虽然基座模型已经能够根据上下文生成文本，但它并不擅长对话。为了解决这个问题，我们需要使用人类撰写的高质量对话数据对基座模型进行有监督微调。
2. **目标**：通过微调，让模型更加适应对话任务，具备更好的对话能力。
3. **过程**：微调的成本相对较低，因为需要的训练数据规模更小，训练时长更短。在这一阶段，模型从人类高质量的对话中学习如何回答问题，这个过程被称为监督微调（supervised fine tuning）。


🤔思考：为啥是“微调”而不叫“中调”或者“大调”？

<font style="color:rgb(86, 87, 114);">因为相比于基座模型所用到的巨量数据，这个过程里的大部分有监督学习，其所用到的数据，都像是冰山之一角，九牛之一毛！</font>

<br/>

## 奖励模型的训练➡️得到可以评价回答的模型
1. **原料**：为了让模型的回答更加优质且符合人类道德与价值观，我们需要让模型一次性给出多个回答结果，并由人工对这些回答结果进行打分和排序。
2. **目标**：基于这些以评分作为标签的训练数据，训练出一个能对回答进行评分预测的奖励模型。
3. **过程**：奖励模型能够对模型的回答进行评分，从而引导模型生成更符合人类期望的回答。这个过程也常被称为对齐（alignment）。

## 强化学习训练➡️得到更符合人类价值观的优秀模型
<br/>color2
+ **强化学习** ：让模型在环境中采取行动，并根据奖励或惩罚来调整策略，以找到最佳行动方案。[什么是 机器学习](https://www.yuque.com/qiaokate/su87gb/lt6p24ggetlzag85)

<br/>

1. **原料**：使用第二步得到的模型和第三步的奖励模型进行强化学习训练。
2. **目标**：让模型的回答不断被奖励模型评价，并通过优化策略获取更高的评分，从而改进自身的结构。
3. **过程**：强化学习训练利用奖励模型的评分作为反馈信号，引导模型生成更高质量的回答。同时，C端用户的点赞或倒赞也为模型的升级提供了宝贵的评价数据。


🤔思考：能不能让普通用户也为模型打分？

看看文心一言的输出结果里，右下就有一个点赞或者倒赞！有更多用户对结果的反馈，大模型能力就会持续增强哦！模型能力增强➡️用户变多➡️反馈变多➡️模型能力增强➡️……这，就是数据飞轮的奇妙！

<br/>

