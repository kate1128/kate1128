## 简介
提供一套系统化的方法和工具，帮助您有效地跟踪和调试生成式 AI 模型。

在构建机器学习系统的过程中，管理和跟踪所有的数据、模型和超参数可能会变得复杂。随着团队规模的增大，这种复杂性可能会进一步加剧。生成式 AI 模型相比于监督学习模型增加了一层复杂性，因为它们的输出是复杂的，所以它们可能更难评估。

将使用 Weights & Biases 的工具，这是一套易用且灵活的工具集，已经成为机器学习实验跟踪的行业标准。涵盖用于文本生成的大型语言模型和用于图像生成的扩散模型。

## 内容
专注于以下内容：

1. 如何跟踪和可视化实验
2. 如何监控扩散模型
3. 如何评估和微调大型语言模型（LLMs）

一系列的调试和评估工具，包括：

+ Experiments：用于跟踪机器学习实验
+ Artifacts：用于版本控制和存储数据集和模型
+ Tables：用于可视化和检查模型做出的预测
+ Reports：用于协作和分享实验结果
+ Model Registry：用于管理模型的生命周期
+ Prompts：用于评估大型语言模型生成

这些工具可以与 Python、TensorFlow 或 PyTorch 等主流的框架和计算平台一起工作。

