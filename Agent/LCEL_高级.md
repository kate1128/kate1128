> LangChain Expression Languageï¼ˆæˆ–ç§°ä¸º LCELï¼‰ï¼Œç§°ä¹‹ä¸º Langchain çš„è¡¨è¾¾å¼è¯­è¨€
>

[ä»€ä¹ˆæ˜¯ LCEL](https://www.yuque.com/qiaokate/su87gb/fpwp7u1tggmac2yb)

# ç®€å•é“¾ Simple Chain
æ¥ä¸‹æ¥ä¾æ—§ä¼šä½¿ç”¨ OpenAI çš„ APIï¼Œæ‰€ä»¥é¦–å…ˆè¦åˆå§‹åŒ–çš„ API_Keyï¼Œè¿™ä¸ªæ–¹æ³•å’Œä¸Šä¸€ç« çš„æ–¹å¼æ˜¯ä¸€æ ·çš„ã€‚

```python
# !pip install langchain
# !pip install openai==0.28
# !pip install "langchain[docarray]"
# !pip install tiktoken
```

```python
import os
import openai
from dotenv import load_dotenv, find_dotenv

# è¯»å–æœ¬åœ°/é¡¹ç›®çš„ç¯å¢ƒå˜é‡ã€‚

# find_dotenv()å¯»æ‰¾å¹¶å®šä½.envæ–‡ä»¶çš„è·¯å¾„
# load_dotenv()è¯»å–è¯¥.envæ–‡ä»¶ï¼Œå¹¶å°†å…¶ä¸­çš„ç¯å¢ƒå˜é‡åŠ è½½åˆ°å½“å‰çš„è¿è¡Œç¯å¢ƒä¸­  
# å¦‚æœä½ è®¾ç½®çš„æ˜¯å…¨å±€çš„ç¯å¢ƒå˜é‡ï¼Œè¿™è¡Œä»£ç åˆ™æ²¡æœ‰ä»»ä½•ä½œç”¨ã€‚
_ = load_dotenv(find_dotenv())

# è·å–ç¯å¢ƒå˜é‡ OPENAI_API_KEY
openai_api_key = os.environ['OPENAI_API_KEY']
```

æ¥ä¸‹æ¥é¦–å…ˆå¯¼å…¥ LangChain çš„åº“ï¼Œå¹¶ä¸”å®šä¹‰ä¸€ä¸ªç®€å•çš„é“¾ï¼Œè¿™ä¸ªé“¾åŒ…æ‹¬æç¤ºæ¨¡æ¿ï¼Œå¤§è¯­è¨€æ¨¡å‹å’Œä¸€ä¸ªè¾“å‡ºè§£æå™¨ã€‚å¯ä»¥çœ‹åˆ°ï¼ŒæˆåŠŸè¾“å‡ºäº†å¤§è¯­è¨€æ¨¡å‹çš„ç»“æœï¼Œå®Œæˆäº†ä¸€ä¸ªç®€å•çš„é“¾ã€‚

```python
# å¯¼å…¥LangChainæ‰€éœ€çš„æ¨¡å—
from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.schema.output_parser import StrOutputParser

# ä½¿ç”¨ ChatPromptTemplate ä»æ¨¡æ¿åˆ›å»ºä¸€ä¸ªæç¤ºï¼Œæ¨¡æ¿ä¸­çš„ {topic} å°†åœ¨åç»­ä»£ç ä¸­æ›¿æ¢ä¸ºå®é™…çš„è¯é¢˜
prompt = ChatPromptTemplate.from_template(
    "å‘Šè¯‰æˆ‘ä¸€ä¸ªå…³äº{topic}çš„çŸ­ç¬‘è¯"
)

# åˆ›å»ºä¸€ä¸ª ChatOpenAI æ¨¡å‹å®ä¾‹ï¼Œé»˜è®¤ä½¿ç”¨ gpt-3.5-turbo æ¨¡å‹
model = ChatOpenAI(openai_api_base="https://xiaoai.plus/v1")

# åˆ›å»ºä¸€ä¸ªStrOutputParserå®ä¾‹ï¼Œç”¨äºè§£æè¾“å‡º
output_parser = StrOutputParser()

# åˆ›å»ºä¸€ä¸ªé“¾å¼è°ƒç”¨ï¼Œå°† promptã€model å’Œoutput_parser è¿æ¥åœ¨ä¸€èµ·
chain = prompt | model | output_parser

# è°ƒç”¨é“¾å¼è°ƒç”¨ï¼Œå¹¶ä¼ å…¥å‚æ•°
chain.invoke({"topic": "ç†Š"})
```

<details class="lake-collapse"><summary id="uc2ab19cd"><span class="ne-text">outputï¼š</span></summary><pre data-language="json" id="GKKcB" class="ne-codeblock language-json"><code>'ä¸ºä»€ä¹ˆç†Šä¸å–œæ¬¢ç©æ‰‘å…‹ç‰Œï¼Ÿå› ä¸ºä»–æ€»æ˜¯æŠŠä¸¤ä¸ªç†ŠæŒéƒ½éœ²å‡ºæ¥ï¼'</code></pre></details>
å¦‚æœå»æŸ¥çœ‹`Chain`çš„è¾“å‡ºï¼Œä¼šå‘ç°ï¼Œä»–è·Ÿå®šä¹‰çš„æ˜¯ä¸€æ ·çš„ï¼Œä¸€å…±æœ‰ä¸‰éƒ¨åˆ†è¿›è¡Œç»„æˆï¼Œä¹Ÿå°±æ˜¯`Chain = prompt | LLM |OutputParser `ã€‚`|`ç¬¦å·ç±»ä¼¼äº unix ç®¡é“æ“ä½œç¬¦ï¼Œå®ƒå°†ä¸åŒçš„ç»„ä»¶é“¾æ¥åœ¨ä¸€èµ·ï¼Œå°†ä¸€ä¸ªç»„ä»¶çš„è¾“å‡ºä½œä¸ºè¾“å…¥æä¾›ç»™ä¸‹ä¸€ä¸ªç»„ä»¶ã€‚åœ¨è¿™ä¸ªé“¾ä¸­ï¼Œç”¨æˆ·è¾“å…¥è¢«ä¼ é€’ç»™æç¤ºæ¨¡æ¿ï¼Œç„¶åæç¤ºæ¨¡æ¿è¾“å‡ºè¢«ä¼ é€’ç»™æ¨¡å‹ï¼Œç„¶åæ¨¡å‹è¾“å‡ºè¢«ä¼ é€’åˆ°è¾“å‡ºè§£æå™¨ã€‚

```python
# æŸ¥çœ‹Chainçš„å€¼
chain
```

<details class="lake-collapse"><summary id="u3cee97de"><span class="ne-text">outputï¼š</span></summary><pre data-language="json" id="b2xq3" class="ne-codeblock language-json"><code>ChatPromptTemplate(input_variables=['topic'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], template='å‘Šè¯‰æˆ‘ä¸€ä¸ªå…³äº{topic}çš„çŸ­ç¬‘è¯'))])
| ChatOpenAI(client=&lt;class 'openai.api_resources.chat_completion.ChatCompletion'&gt;, openai_api_key='xxx', openai_proxy='')
| StrOutputParser()</code></pre></details>
#  æ›´å¤æ‚çš„é“¾ More complex chain
æ¥ä¸‹æ¥ï¼Œä¼šåˆ›å»ºä¸€ä¸ªæ›´å¤æ‚çš„é“¾æ¡ï¼Œåœ¨ä¹‹å‰çš„è¯¾ç¨‹ä¸­ï¼Œæ¥è§¦è¿‡å¦‚ä½•è¿›è¡Œæ£€ç´¢å¢å¼ºç”Ÿæˆã€‚æ‰€ä»¥æ¥ä¸‹æ¥ä½¿ç”¨ LCEL æ¥é‡å¤ä¹‹å‰çš„è¿‡ç¨‹ï¼Œå°†ç”¨æˆ·çš„é—®é¢˜å’Œå‘é‡æ•°æ®åº“æ£€ç´¢ç»“æœç»“åˆèµ·æ¥ï¼Œä½¿ç”¨ `RunnableMap` æ¥æ„å»ºä¸€ä¸ªæ›´å¤æ‚çš„é“¾ã€‚

## æ„å»ºç®€å•å‘é‡æ•°æ®åº“
é¦–å…ˆæ„å»ºä¸€ä¸ªå‘é‡æ•°æ®åº“ï¼Œè¿™ä¸ªç®€å•çš„å‘é‡æ•°æ®åº“åªåŒ…å«ä¸¤å¥è¯ï¼Œä½¿ç”¨ OpenAI çš„ Embedding ä½œä¸ºåµŒå…¥æ¨¡å‹ï¼Œç„¶åé€šè¿‡ `vector store.as_retriever `æ¥åˆ›å»ºä¸€ä¸ªæ£€ç´¢å™¨ã€‚

```python
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import DocArrayInMemorySearch

# åˆ›å»ºä¸€ä¸ªDocArrayInMemorySearchå¯¹è±¡ï¼Œç”¨äºå­˜å‚¨å’Œæœç´¢æ–‡æ¡£å‘é‡
vectorstore = DocArrayInMemorySearch.from_texts(
    ["å“ˆé‡Œæ£®åœ¨è‚¯è‚–å·¥ä½œ", "ç†Šå–œæ¬¢åƒèœ‚èœœ"],
    embedding=OpenAIEmbeddings() # ä½¿ç”¨OpenAIçš„Embedding
)

# åˆ›å»ºä¸€ä¸ªæ£€ç´¢å™¨
retriever = vectorstore.as_retriever()
```

é€šè¿‡ä¹‹å‰çš„å­¦ä¹ ï¼Œå¦‚æœè°ƒç”¨`retriever.get_relevant_documents`ï¼Œä¼šå¾—åˆ°ç›¸å…³çš„æ£€ç´¢æ–‡æ¡£ï¼Œé¦–å…ˆé—®â€œå“ˆé‡Œæ£®åœ¨å“ªé‡Œå·¥ä½œï¼Ÿâ€ï¼Œä¼šå‘ç°è¿”å›äº†ä¸€ä¸ªæ–‡æ¡£åˆ—è¡¨ï¼Œä»–ä¼šæ ¹æ®ç›¸ä¼¼åº¦æ’åºè¿”å›æ–‡æ¡£åˆ—è¡¨ï¼Œæ‰€ä»¥å…¶ä¸­æœ€ç›¸å…³çš„æ”¾åœ¨äº†ç¬¬ä¸€ä¸ªã€‚

```python
# è·å–ä¸é—®é¢˜â€œå“ˆé‡Œæ£®åœ¨å“ªé‡Œå·¥ä½œï¼Ÿâ€ç›¸å…³çš„æ–‡æ¡£
retriever.get_relevant_documents("å“ˆé‡Œæ£®åœ¨å“ªé‡Œå·¥ä½œï¼Ÿ")
```

<details class="lake-collapse"><summary id="u1095f9c1"><span class="ne-text">outputï¼š</span></summary><pre data-language="json" id="T7r3Z" class="ne-codeblock language-json"><code>[Document(page_content='å“ˆé‡Œæ£®åœ¨è‚¯è‚–å·¥ä½œ'), Document(page_content='ç†Šå–œæ¬¢åƒèœ‚èœœ')]</code></pre></details>
å¦‚æœæ¢ä¸€ä¸ªé—®é¢˜ï¼Œæ¯”å¦‚"ç†Šå–œæ¬¢åƒä»€ä¹ˆ"ï¼Œå¯ä»¥çœ‹åˆ°é—®é¢˜çš„é¡ºåºå°±å‘ç”Ÿäº†å˜åŒ–ã€‚

```python
# è·å–ä¸é—®é¢˜â€œç†Šå–œæ¬¢åƒä»€ä¹ˆâ€ç›¸å…³çš„æ–‡æ¡£
retriever.get_relevant_documents("ç†Šå–œæ¬¢åƒä»€ä¹ˆ")
```

<details class="lake-collapse"><summary id="ud90c2766"><span class="ne-text" style="color: rgba(0, 0, 0, 0.87)">outputï¼š</span></summary><pre data-language="python" id="iOXbm" class="ne-codeblock language-python"><code>[Document(page_content='ç†Šå–œæ¬¢åƒèœ‚èœœ'), Document(page_content='å“ˆé‡Œæ£®åœ¨è‚¯è‚–å·¥ä½œ')]</code></pre></details>
## ä½¿ç”¨RunnableMap
ä¸Šè¿°ä¾‹å­è¿”å›ä¸¤ä¸ªç»“æœæ˜¯å› ä¸ºåªæœ‰ä¸¤ä¸ªæ–‡æ¡£åˆ—è¡¨ï¼Œè¿™å®Œå…¨é€‚ç”¨äºæ›´å¤šæ–‡æ¡£æƒ…å†µã€‚æ¥ä¸‹æ¥ä¼šåŠ å…¥`RunnableMap`ï¼Œåœ¨è¿™ä¸ª`RunnableMap`ä¸­ï¼Œä¸ä»…ä»…æœ‰ç”¨æˆ·çš„é—®é¢˜ï¼Œä»¥åŠæœ‰å¯¹åº”çš„é—®é¢˜çš„æ–‡æ¡£åˆ—è¡¨ï¼Œç›¸å½“äºè¿™ä¹Ÿä¸ºå¤§æ¨¡å‹çš„æ–‡æ¡£å¢åŠ äº†ä¸Šä¸‹æ–‡ï¼Œè¿™æ ·å°±èƒ½å®Œæˆæ£€ç´¢å¢å¼ºçš„äº‹æƒ…ã€‚å¦‚æœæ­£å¸¸é—®ä¸€ä¸ªé—®é¢˜ï¼Œå¯ä»¥çœ‹åˆ°ï¼Œå¤§æ¨¡å‹æ­£ç¡®çš„è¿”å›äº†æ–‡æ¡£é‡Œé¢çš„ç»“æœï¼Œå¾—åˆ°äº†æ­£ç¡®çš„è¾“å‡ºã€‚

```python
from langchain.schema.runnable import RunnableMap

# å®šä¹‰ä¸€ä¸ªæ¨¡æ¿å­—ç¬¦ä¸²template
template = """ä»…æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š
{context}

é—®é¢˜ï¼š{question}
"""

# ä½¿ç”¨ template ä½œä¸ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_template(template)

# åˆ›å»ºä¸€ä¸ªå¤„ç†é“¾ chain ï¼ŒåŒ…å«äº† RunnableMapã€promptã€model å’Œ output_parser ç»„ä»¶
chain = RunnableMap({
    "context": lambda x: retriever.get_relevant_documents(x["question"]),
    "question": lambda x: x["question"]
}) | prompt | model | output_parser

# è°ƒç”¨chainçš„invokeæ–¹æ³•
chain.invoke({"question": "å“ˆé‡Œæ£®åœ¨å“ªé‡Œå·¥ä½œ?"})
```

<details class="lake-collapse"><summary id="u466505d7"><span class="ne-text">outputï¼š</span></summary><pre data-language="python" id="X5X3l" class="ne-codeblock language-python"><code>'è‚¯è‚–'</code></pre></details>
<font style="color:rgba(0, 0, 0, 0.87);">å¦‚æœæƒ³æ›´æ·±å…¥æŒ–æ˜ä¸€ä¸‹èƒŒåçš„å·¥ä½œæœºç†ï¼Œå¯ä»¥çœ‹ä¸€ä¸‹</font>`<font style="color:rgba(0, 0, 0, 0.87);">RunnableMap</font>`<font style="color:rgba(0, 0, 0, 0.87);">ï¼ŒæŠŠå…¶åˆ›å»ºä¸ºä¸€ä¸ªè¾“å…¥ï¼Œç”¨ä¸€æ ·çš„æ–¹å¼è¿›è¡Œæ“ä½œã€‚</font>

<font style="color:rgba(0, 0, 0, 0.87);">å¯ä»¥çœ‹åˆ°ï¼Œåœ¨è¿™ä¹‹ä¸­ï¼Œ</font>`<font style="color:rgba(0, 0, 0, 0.87);">RunnableMap</font>`<font style="color:rgba(0, 0, 0, 0.87);">æä¾›äº†</font>`<font style="color:rgba(0, 0, 0, 0.87);">context</font>`<font style="color:rgba(0, 0, 0, 0.87);">å’Œ</font>`<font style="color:rgba(0, 0, 0, 0.87);">question</font>`<font style="color:rgba(0, 0, 0, 0.87);">ä¸¤ä¸ªå˜é‡ï¼Œä¸€ä¸ªæ˜¯æŸ¥è¯¢çš„æ–‡æ¡£åˆ—è¡¨ï¼Œå¦ä¸€ä¸ªæ˜¯å¯¹åº”çš„é—®é¢˜ï¼Œè¿™ä¸ªå¤§æ¨¡å‹å°±å¯ä»¥æ ¹æ®æä¾›æ–‡æ¡£æ¥æ€»ç»“å›ç­”å¯¹åº”çš„é—®é¢˜äº†ã€‚</font>

```python
# åˆ›å»ºä¸€ä¸ªRunnableMapå¯¹è±¡ï¼Œå…¶ä¸­åŒ…å«ä¸¤ä¸ªé”®å€¼å¯¹
# é”® "context" å¯¹åº”ä¸€ä¸ªlambdaå‡½æ•°ï¼Œç”¨äºè·å–ç›¸å…³æ–‡æ¡£ï¼Œå‡½æ•°è¾“å…¥å‚æ•°ä¸ºxï¼Œå³è¾“å…¥çš„å­—å…¸ï¼Œå‡½æ•°è¿”å›å€¼ä¸ºretriever.get_relevant_documents(x["question"])
# é”® "question" å¯¹åº”ä¸€ä¸ªlambdaå‡½æ•°ï¼Œç”¨äºè·å–é—®é¢˜ï¼Œå‡½æ•°è¾“å…¥å‚æ•°ä¸ºxï¼Œå³è¾“å…¥çš„å­—å…¸ï¼Œå‡½æ•°è¿”å›å€¼ä¸ºx["question"]
inputs = RunnableMap({
    "context": lambda x: retriever.get_relevant_documents(x["question"]),
    "question": lambda x: x["question"]
})

# è°ƒç”¨ inputs çš„ invoke æ–¹æ³•ï¼Œå¹¶ä¼ é€’ä¸€ä¸ªå­—å…¸ä½œä¸ºå‚æ•°ï¼Œå­—å…¸ä¸­åŒ…å«ä¸€ä¸ªé”®å€¼å¯¹ï¼Œé”®ä¸º"question"ï¼Œå€¼ä¸º"å“ˆé‡Œæ£®åœ¨å“ªé‡Œå·¥ä½œ?"
inputs.invoke({"question": "å“ˆé‡Œæ£®åœ¨å“ªé‡Œå·¥ä½œ?"})

```

<details class="lake-collapse"><summary id="uead42282"><span class="ne-text">outputï¼š</span></summary><pre data-language="python" id="S0hbc" class="ne-codeblock language-python"><code>{'context': [Document(page_content='å“ˆé‡Œæ£®åœ¨è‚¯è‚–å·¥ä½œ'),
             Document(page_content='ç†Šå–œæ¬¢åƒèœ‚èœœ')],
 'question': 'å“ˆé‡Œæ£®åœ¨å“ªé‡Œå·¥ä½œ?'}</code></pre></details>
#  ç»‘å®š Bind
[Function Calling](https://www.yuque.com/qiaokate/su87gb/cu0lppkawdfrm12r)ä»‹ç»äº†OpenAIå‡½æ•°çš„è°ƒç”¨ï¼Œæ–°çš„`function`å‚æ•°å¯ä»¥è‡ªåŠ¨åˆ¤æ–­æ˜¯å¦è¦ä½¿ç”¨å·¥å…·å‡½æ•°ï¼Œå¦‚æœéœ€è¦å°±ä¼šè¿”å›éœ€è¦ä½¿ç”¨çš„å‚æ•°ã€‚æ¥ä¸‹æ¥ä¹Ÿä½¿ç”¨LangChainå®ç°OpenAIå‡½æ•°è°ƒç”¨çš„æ–°åŠŸèƒ½ï¼Œé¦–å…ˆéœ€è¦ä¸€ä¸ªå‡½æ•°çš„æè¿°ä¿¡æ¯ï¼Œä»¥åŠå®šä¹‰å‡½æ•°ï¼Œè¿™é‡Œçš„å‡½æ•°è¿˜æ˜¯ä½¿ç”¨[Function Calling](https://www.yuque.com/qiaokate/su87gb/cu0lppkawdfrm12r)çš„`get_current_weather`å‡½æ•°ã€‚

```python
# å®šä¹‰ä¸€ä¸ªå‡½æ•°
functions = [
    {
        "name": "get_current_weather",
        "description": "è·å–æŒ‡å®šä½ç½®çš„å½“å‰å¤©æ°”æƒ…å†µ",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": "åŸå¸‚å’Œçœä»½ï¼Œä¾‹å¦‚ï¼šåŒ—äº¬ï¼ŒåŒ—äº¬å¸‚",
                },
                "unit": {"type": "string", "enum": ["æ‘„æ°åº¦", "åæ°åº¦"]},
            },
            "required": ["location"],
        },
    }
]
```

## å•å‡½æ•°ç»‘å®š
æ¥ä¸‹æ¥ä½¿ç”¨`bind`çš„æ–¹æ³•æŠŠå·¥å…·å‡½æ•°ç»‘å®šåˆ°å¤§æ¨¡å‹ä¸Šï¼Œå¹¶æ„å»ºä¸€ä¸ªç®€å•çš„é“¾ã€‚è¿›è¡Œè°ƒç”¨ä»¥åï¼Œå¯ä»¥çœ‹åˆ°è¿”å›äº†ä¸€ä¸ª`AIMessage`ï¼Œ å…¶ä¸­è¿”å›çš„`content`ä¸ºç©ºï¼Œä½†æ˜¯è¿”å›äº†éœ€è¦è°ƒç”¨å·¥å…·å‡½æ•°çš„å‚æ•°ã€‚

```python
# ä½¿ç”¨ChatPromptTemplate.from_messagesæ–¹æ³•åˆ›å»ºä¸€ä¸ªChatPromptTemplateå¯¹è±¡
prompt = ChatPromptTemplate.from_messages(
    [
        ("human", "{input}")
    ]
)

# ä½¿ç”¨bindæ–¹æ³•ç»‘å®šfunctionså‚æ•°
model = ChatOpenAI(temperature=0).bind(functions=functions)

runnable = prompt | model

# è°ƒç”¨invokeæ–¹æ³•
runnable.invoke({"input": "åŒ—äº¬å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"})
```

<details class="lake-collapse"><summary id="uf1e63604"><span class="ne-text" style="color: rgba(0, 0, 0, 0.87)">outputï¼š</span></summary><pre data-language="python" id="HJaEx" class="ne-codeblock language-python"><code>AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\n  &quot;location&quot;: &quot;åŒ—äº¬ï¼ŒåŒ—äº¬å¸‚&quot;\n}'}})</code></pre></details>
##  å¤šä¸ªå‡½æ•°ç»‘å®š
åŒæ—¶ä¹Ÿå¯ä»¥å®šä¹‰å¤šä¸ª`function`ï¼Œå¤§æ¨¡å‹åœ¨å¯¹è¯çš„æ—¶å€™å¯ä»¥è‡ªåŠ¨åˆ¤æ–­ä½¿ç”¨å“ªä¸€ä¸ªå‡½æ•°ã€‚è¿™é‡Œé¢å®šä¹‰æœ‰ä¸¤ä¸ªå‡½æ•°ï¼Œç¬¬ä¸€ä¸ªå‡½æ•°æ˜¯ç±»ä¼¼äºå‰é¢çš„`weather_search`ï¼Œæœç´¢ç»™å®šæœºåœºçš„å¤©æ°”ï¼Œç„¶åè¿˜å®šä¹‰äº†ä¸€ä¸ªèµ›äº‹ä½“è‚²æ–°é—»æœç´¢çš„`sports_search`ï¼ŒæŸ¥è¯¢å¤©æ°”çš„å‡½æ•°`weather_search`æ¥å—çš„å‚æ•°ä¸º`airport_code`å³æœºåœºä»£ç ï¼Œä½“è‚²æ–°é—»æœç´¢å‡½æ•°`sports_search`æ¥å—çš„å‚æ•°ä¸º`team_name`å³ä½“è‚²é˜Ÿåã€‚ç”±äºè¿™é‡Œä¸éœ€è¦è¿è¡Œè¿™äº›å‡½æ•°ï¼Œå› ä¸ºå¤§æ¨¡å‹æ˜¯é€šè¿‡é—®çš„é—®é¢˜æ¥è‡ªåŠ¨åˆ¤æ–­æ˜¯å¦è°ƒç”¨è¿™äº›å‡½æ•°ï¼Œå¹¶ä¸”è¿”å›å‚æ•°ï¼Œå¹¶ä¸ä¼šç›´æ¥å¸®è°ƒç”¨ã€‚

```python
functions = [
    {
        "name": "weather_search",
        "description": "æœç´¢ç»™å®šæœºåœºä»£ç çš„å¤©æ°”",
        "parameters": {
            "type": "object",
            "properties": {
                "airport_code": {
                    "type": "string",
                    "description": "è¦è·å–å¤©æ°”çš„æœºåœºä»£ç "
                },
            },
            "required": ["airport_code"]
        }
    },
    {
        "name": "sports_search",
        "description": "æœç´¢æœ€è¿‘ä½“è‚²èµ›äº‹çš„æ–°é—»",
        "parameters": {
            "type": "object",
            "properties": {
                "team_name": {
                    "type": "string",
                    "description": "è¦æœç´¢çš„ä½“è‚²é˜Ÿå"
                },
            },
            "required": ["team_name"]
        }
    }
]
```

æ¥ç€å°±å¯ä»¥ä½¿ç”¨å‡½æ•°ç»‘å®šå¤§æ¨¡å‹ï¼Œå®šä¹‰ä¸€ä¸ªç®€å•çš„é“¾ï¼Œå¯ä»¥çœ‹åˆ°ï¼Œå½“é—®äº†ç›¸å…³çš„é—®é¢˜ä»¥åï¼Œå¤§æ¨¡å‹èƒ½å¤Ÿè‡ªåŠ¨åˆ¤æ–­å¹¶ä¸”æ­£ç¡®è¿”å›å‚æ•°ï¼ŒçŸ¥é“éœ€è¦è°ƒç”¨å‡½æ•°äº†ã€‚

```python
# ç»‘å®šå¤§æ¨¡å‹
model = model.bind(functions=functions)
runnable = prompt | model

runnable.invoke({"input": "çˆ±å›½è€…é˜Ÿæ˜¨å¤©è¡¨ç°çš„æ€ä¹ˆæ ·?"})
```

<details class="lake-collapse"><summary id="u5ab7c42f"><span class="ne-text" style="color: rgba(0, 0, 0, 0.87)">outputï¼š</span></summary><pre data-language="python" id="FztYh" class="ne-codeblock language-python"><code>AIMessage(content='', additional_kwargs={'function_call': {'name': 'sports_search', 'arguments': '{\n  &quot;team_name&quot;: &quot;çˆ±å›½è€…é˜Ÿ&quot;\n}'}})</code></pre></details>
# åå¤‡æªæ–½ `Fallbacks`
åœ¨ä½¿ç”¨æ—©æœŸçš„OpenAIæ¨¡å‹å¦‚"`text-davinci-001`"ï¼Œè¿™äº›æ¨¡å‹åœ¨å¯¹è¯è¿‡ç¨‹ä¸­ï¼Œä¸æ”¯æŒæ ¼å¼åŒ–è¾“å‡ºç»“æœå³å®ƒä»¬éƒ½æ˜¯ä»¥å­—ç¬¦ä¸²çš„å½¢å¼è¾“å‡ºç»“æœï¼Œè¿™å¯¹æœ‰æ—¶å€™éœ€è¦è§£æ LLM çš„è¾“å‡ºå¸¦æ¥ä¸€äº›éº»çƒ¦ï¼Œæ¯”å¦‚ä¸‹é¢è¿™ä¸ªä¾‹å­ï¼Œå°±æ˜¯åˆ©ç”¨æ—©æœŸæ¨¡å‹"`text-davinci-001`"æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œå¸Œæœ› llm èƒ½ä»¥ json æ ¼å¼è¾“å‡ºç»“æœã€‚

å®šä¹‰äº† OpenAI çš„æ¨¡å‹ä»¥åŠåˆ›å»ºäº†ä¸€ä¸ªç®€å•çš„é“¾ï¼Œä»¥æ­¤åŠ å…¥ json å¸Œæœ›èƒ½ä»¥ json æ ¼å¼è¾“å‡ºç»“æœï¼Œè®© `simple_model` å†™ä¸‰é¦–è¯—ï¼Œå¹¶ä»¥ josn æ ¼å¼è¾“å‡ºï¼Œæ¯é¦–è¯—å¿…é¡»åŒ…å«:`æ ‡é¢˜ï¼Œä½œè€…å’Œè¯—çš„ç¬¬ä¸€å¥`ã€‚ä¼šå‘ç°ç»“æœåªæœ‰å­—ç¬¦ä¸²ï¼Œæ— æ³•è¾“å‡ºæŒ‡å®šæ ¼å¼çš„å†…å®¹ï¼Œè™½ç„¶é‡Œé¢æœ‰ä¸€äº›`[`ï¼Œä½†æ˜¯æœ¬è´¨ä¸Šè¿˜æ˜¯ä¸€ä¸ªå¤§çš„å­—ç¬¦ä¸²ï¼Œè¿™å°±æ— æ³•è®©è§£æè¾“å‡ºã€‚

ç”±äºOpenAIäº2024å¹´1æœˆ4æ—¥åœç”¨äº†æ¨¡å‹`text-davinci-001`ï¼Œä½ å°†ä½¿ç”¨OpenAIæ¨èçš„æ›¿ä»£æ¨¡å‹`gpt-3.5-turbo-instruct`ã€‚

åœ¨ä½¿ç”¨è¯­è¨€æ¨¡å‹æ—¶ï¼Œä½ å¯èƒ½ç»å¸¸ä¼šé‡åˆ°æ¥è‡ªåº•å±‚ API çš„é—®é¢˜ï¼Œæ— è®ºè¿™äº›é—®é¢˜æ˜¯é€Ÿç‡é™åˆ¶è¿˜æ˜¯åœæœºæ—¶é—´ã€‚å› æ­¤ï¼Œå½“ä½ å°† LLM åº”ç”¨ç¨‹åºè½¬ç§»åˆ°å®é™…ç”Ÿäº§ç¯å¢ƒä¸­æ—¶ï¼Œé˜²èŒƒè¿™äº›é—®é¢˜å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå¼•å…¥äº†`å›é€€ï¼ˆFallbacksï¼‰`çš„æ¦‚å¿µã€‚

## ä½¿ç”¨æ—©æœŸæ¨¡å‹æ ¼å¼åŒ–è¾“å‡º
```python
from langchain.llms import OpenAI
import json

# ä½¿ç”¨æ—©æœŸçš„OpenAIæ¨¡å‹
simple_model = OpenAI(
    temperature=0,
    max_tokens=1000,
    model="gpt-3.5-turbo-instruct"
)
simple_chain = simple_model | json.loads

challenge = "å†™ä¸‰é¦–è¯—ï¼Œå¹¶ä»¥josnæ ¼å¼è¾“å‡ºï¼Œæ¯é¦–è¯—å¿…é¡»åŒ…å«:æ ‡é¢˜ï¼Œä½œè€…å’Œè¯—çš„ç¬¬ä¸€å¥ã€‚"

simple_model.invoke(challenge)
```

<details class="lake-collapse"><summary id="u0c015fbe"><span class="ne-text">outputï¼š</span></summary><pre data-language="python" id="ng7yt" class="ne-codeblock language-python"><code>'\n\n{\n  &quot;title&quot;: &quot;æ˜¥é£&quot;,\n  &quot;author&quot;: &quot;æç™½&quot;,\n  &quot;first_line&quot;: &quot;æ˜¥é£åˆç»¿æ±Ÿå—å²¸&quot;,\n  &quot;content&quot;: [\n    &quot;æ˜¥é£åˆç»¿æ±Ÿå—å²¸&quot;,\n    &quot;èŠ±å¼€æ»¡æ ‘æŸ³å¦‚ä¸&quot;,\n    &quot;é¸Ÿå„¿æ¬¢å”±å¤©åœ°å®½&quot;,\n    &quot;äººé—´æ˜¥è‰²æœ€å®œäºº&quot;\n  ]\n}\n\n{\n  &quot;title&quot;: &quot;å¤œé›¨&quot;,\n  &quot;author&quot;: &quot;æœç”«&quot;,\n  &quot;first_line&quot;: &quot;å¤œé›¨æ½‡æ½‡&quot;,\n  &quot;content&quot;: [\n    &quot;å¤œé›¨æ½‡æ½‡&quot;,\n    &quot;å­¤ç¯ç…§æ—§&quot;,\n    &quot;æ€å¿µå¦‚æ½®&quot;,\n    &quot;æ³›æ»¥å¿ƒå¤´&quot;\n  ]\n}\n\n{\n  &quot;title&quot;: &quot;å±±è¡Œ&quot;,\n  &quot;author&quot;: &quot;ç‹ç»´&quot;,\n  &quot;first_line&quot;: &quot;è¿œä¸Šå¯’å±±çŸ³å¾„æ–œ&quot;,\n  &quot;content&quot;: [\n    &quot;è¿œä¸Šå¯’å±±çŸ³å¾„æ–œ&quot;,\n    &quot;ç™½äº‘ç”Ÿå¤„æœ‰äººå®¶&quot;,\n    &quot;åœè½¦åçˆ±æ«æ—æ™š&quot;,\n    &quot;éœœå¶çº¢äºäºŒæœˆèŠ±&quot;\n  ]\n}'</code></pre></details>
å¦‚æœä½¿ç”¨`simple_chain`æ¥è¿è¡Œï¼Œå°±ä¼šå‘ç°å‡ºç°äº† json è§£ç é”™è¯¯çš„é—®é¢˜ï¼Œå› ä¸ºè¿”å›çš„ç»“æœå°±æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œæ— æ³•è§£æï¼Œæ‰€ä»¥ä¸‹é¢ä»£ç å°±ä¼šæŠ¥é”™ã€‚

```python
simple_chain.invoke(challenge)      
```

<details class="lake-collapse"><summary id="u1fc7e60a"><span class="ne-text">outputï¼š</span></summary><pre data-language="python" id="cTSFo" class="ne-codeblock language-python"><code>---------------------------------------------------------------------------
JSONDecodeError                           Traceback (most recent call last)
&lt;ipython-input-17-7b2363c45b31&gt; in &lt;cell line: 1&gt;()
----&gt; 1 simple_chain.invoke(challenge)

/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py in invoke(self, input, config)
2051         try:
    2052             for i, step in enumerate(self.steps):
        -&gt; 2053                 input = step.invoke(
            2054                     input,
            2055                     # mark each step as a child run

            /usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py in invoke(self, input, config, **kwargs)
            3505         &quot;&quot;&quot;Invoke this runnable synchronously.&quot;&quot;&quot;
3506         if hasattr(self, &quot;func&quot;):
-&gt; 3507             return self._call_with_config(
    3508                 self._invoke,
    3509                 input,

    /usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py in _call_with_config(self, func, input, config, run_type, **kwargs)
1244             output = cast(
    1245                 Output,
    -&gt; 1246                 context.run(
        1247                     call_func_with_variable_args,
        1248                     func,  # type: ignore[arg-type]

        /usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py in call_func_with_variable_args(func, input, config, run_manager, **kwargs)
    324     if run_manager is not None and accepts_run_manager(func):
325         kwargs[&quot;run_manager&quot;] = run_manager
--&gt; 326     return func(input, **kwargs)  # type: ignore[call-arg]
327 
328 

/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py in _invoke(self, input, run_manager, config, **kwargs)
3381                         output = chunk
3382         else:
-&gt; 3383             output = call_func_with_variable_args(
    3384                 self.func, input, config, run_manager, **kwargs
    3385             )

/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py in call_func_with_variable_args(func, input, config, run_manager, **kwargs)
324     if run_manager is not None and accepts_run_manager(func):
325         kwargs[&quot;run_manager&quot;] = run_manager
--&gt; 326     return func(input, **kwargs)  # type: ignore[call-arg]
327 
328 

/usr/lib/python3.10/json/__init__.py in loads(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)
344             parse_int is None and parse_float is None and
345             parse_constant is None and object_pairs_hook is None and not kw):
--&gt; 346         return _default_decoder.decode(s)
347     if cls is None:
348         cls = JSONDecoder

/usr/lib/python3.10/json/decoder.py in decode(self, s, _w)
338         end = _w(s, end).end()
339         if end != len(s):
--&gt; 340             raise JSONDecodeError(&quot;Extra data&quot;, s, end)
341         return obj
342 

JSONDecodeError: Extra data: line 15 column 1 (char 147)</code></pre></details>
## ä½¿ç”¨æ–°æ¨¡å‹æ ¼å¼åŒ–è¾“å‡º
æ‰€ä»¥ä¼šå‘ç°æ—©æœŸç‰ˆæœ¬çš„ OpenAI æ¨¡å‹ä¸æ”¯æŒæ ¼å¼åŒ–çš„è¾“å‡ºï¼Œæ‰€ä»¥å³ä½¿ä½¿ç”¨ LangChain å¹¶ä¸”åŠ ä¸Šäº†`json.load`ä½†æ˜¯è¿˜æ˜¯ä¼šå‡ºç°é”™è¯¯ï¼Œä½†æ˜¯å¦‚æœä½¿ç”¨æ–°çš„`gpt-3.5-turbo`æ¨¡å‹å°±ä¸ä¼šå‡ºç°è¿™ä¸ªé—®é¢˜ã€‚

```python
# é»˜è®¤ä½¿ç”¨æ–°çš„æ¨¡å‹
model = ChatOpenAI(temperature=0)
chain = model | StrOutputParser() | json.loads

chain.invoke(challenge)
```

<details class="lake-collapse"><summary id="u829311a2"><span class="ne-text">outputï¼š</span></summary><pre data-language="python" id="kU0M8" class="ne-codeblock language-python"><code>{'poem1': {'title': 'æ˜¥é£',
           'author': 'æç™½',
           'first_line': 'æ˜¥é£åˆç»¿æ±Ÿå—å²¸ã€‚',
           'content': 'æ˜¥é£åˆç»¿æ±Ÿå—å²¸ï¼Œæ˜æœˆä½•æ—¶ç…§æˆ‘è¿˜ã€‚'},
 'poem2': {'title': 'é™å¤œæ€',
           'author': 'æœç”«',
           'first_line': 'åºŠå‰æ˜æœˆå…‰ï¼Œ',
           'content': 'åºŠå‰æ˜æœˆå…‰ï¼Œç–‘æ˜¯åœ°ä¸Šéœœã€‚'},
 'poem3': {'title': 'ç™»é¹³é›€æ¥¼',
           'author': 'ç‹ä¹‹æ¶£',
           'first_line': 'ç™½æ—¥ä¾å±±å°½ï¼Œ',
           'content': 'ç™½æ—¥ä¾å±±å°½ï¼Œé»„æ²³å…¥æµ·æµã€‚'}}</code></pre></details>
## `fallbacks`æ–¹æ³•
é‚£è¿™ä¸ªæ—¶å€™å¯èƒ½å°±ä¼šæ€è€ƒï¼Œæœ‰æ²¡æœ‰ä»€ä¹ˆæ–¹æ³•ï¼Œåœ¨ä¸ç”¨æ”¹å˜å¤ªå¤šä»£ç çš„æƒ…å†µä¸‹ï¼Œè®©æ—©æœŸçš„æ¨¡å‹ä¹Ÿèƒ½è¾¾åˆ°æ ¼å¼åŒ–è¾“å‡ºçš„æ•ˆæœï¼Œè€Œä¸æ˜¯å†™å¤æ‚çš„æ ¼å¼åŒ–è¾“å‡ºçš„ä»£ç å»å¯¹ç»“æœè¿›è¡Œæ“ä½œã€‚è¿™æ—¶å€™å°±å¯ä»¥ä½¿ç”¨`fallbacks`çš„æ–¹å¼èµ‹äºˆæ—©æœŸæ¨¡å‹è¿™æ ·æ ¼å¼åŒ–çš„èƒ½åŠ›ï¼Œä»ç»“æœä¹Ÿå¯ä»¥çœ‹å‡ºï¼ŒæˆåŠŸä½¿ç”¨`fallbacks`èµ‹äºˆäº†ç®€å•æ¨¡å‹æ ¼å¼åŒ–çš„èƒ½åŠ›ã€‚

```python
# ä½¿ç”¨with_fallbacksæœºåˆ¶
final_chain = simple_chain.with_fallbacks([chain])

# è°ƒç”¨final_chainçš„invokeæ–¹æ³•ï¼Œå¹¶ä¼ é€’challengeå‚æ•°
final_chain.invoke(challenge)
```

<details class="lake-collapse"><summary id="ua99f050f"><span class="ne-text">outputï¼š</span></summary><pre data-language="python" id="iLN3G" class="ne-codeblock language-python"><code>{'poem1': {'title': 'æ˜¥é£',
           'author': 'æç™½',
           'first_line': 'æ˜¥é£åˆç»¿æ±Ÿå—å²¸ã€‚',
           'content': 'æ˜¥é£åˆç»¿æ±Ÿå—å²¸ï¼Œæ˜æœˆä½•æ—¶ç…§æˆ‘è¿˜ã€‚'},
 'poem2': {'title': 'é™å¤œæ€',
           'author': 'æœç”«',
           'first_line': 'åºŠå‰æ˜æœˆå…‰ï¼Œ',
           'content': 'åºŠå‰æ˜æœˆå…‰ï¼Œç–‘æ˜¯åœ°ä¸Šéœœã€‚'},
 'poem3': {'title': 'ç™»é¹³é›€æ¥¼',
           'author': 'ç‹ä¹‹æ¶£',
           'first_line': 'ç™½æ—¥ä¾å±±å°½ï¼Œ',
           'content': 'ç™½æ—¥ä¾å±±å°½ï¼Œé»„æ²³å…¥æµ·æµã€‚'}}</code></pre></details>
## fallbacks æ˜¯å¦‚ä½•å®ç°çš„ï¼Ÿ
å½“è°ƒç”¨ LLM æ—¶ï¼Œç»å¸¸ä¼šå‡ºç°ç”±äºåº•å±‚ API é—®é¢˜ã€é€Ÿç‡é—®é¢˜æˆ–è€…ç½‘ç»œé—®é¢˜ç­‰åŸå› ï¼Œå¯¼è‡´ä¸èƒ½æˆåŠŸè¿è¡Œ LLM ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå°±å¯ä»¥ä½¿ç”¨å›é€€è¿™ç§æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå…·ä½“æ¥è¯´ï¼Œä»–æ˜¯é€šè¿‡ä½¿ç”¨å¦ä¸€ç§ LLM æ¥ä»£æ›¿åŸå…ˆçš„ä¸å¯è¿è¡Œçš„ LLM äº§ç”Ÿç»“æœï¼Œè¯·çœ‹ä¸‹é¢ä¾‹å­ï¼š

```python
from langchain_core.chat_models.openai import ChatOpenAI
from langchain_core.chat_models.anthropic import ChatAnthropic

model = ChatAnthropic().with_fallbacks([ChatOpenAI()])
model.invoke('hello')
```

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé€šå¸¸ä¼šä¼˜å…ˆä½¿ç”¨ ChatAnthropic è¿›è¡Œå›ç­”ï¼Œä½†æ˜¯å¦‚æœè°ƒç”¨ ChatAnthropic å¤±è´¥äº†ï¼Œä¼šå›é€€åˆ°ä½¿ç”¨ ChatOpenAI æ¨¡å‹æ¥ç”Ÿæˆå“åº”ã€‚å¦‚æœä¸¤ç§ LLM éƒ½å¤±è´¥äº†ï¼Œå°†ä¼šå›é€€åˆ°ä¸€ç§ç¡¬ç¼–ç å“åº”ã€‚ç¡¬ç¼–ç çš„é»˜è®¤å“åº”ç”¨äºå¤„ç†å¼‚å¸¸æƒ…å†µæˆ–è€…åœ¨æ— æ³•ä»å¤–éƒ¨èµ„æºè·å–æ‰€éœ€ä¿¡æ¯æ—¶æä¾›ä¸€ä¸ªå¤‡ç”¨é€‰é¡¹ï¼Œä¾‹å¦‚ "Looks like our LLM providers are down. Here's a nice ğŸ¦œï¸ emoji for you instead."ï¼ˆçœ‹èµ·æ¥çš„ LLM æä¾›å•†å‡ºäº†é—®é¢˜ï¼Œé‚£ä¹ˆï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªå¯çˆ±çš„ ğŸ¦œï¸ è¡¨æƒ…ç¬¦å·ç»™ä½ ã€‚ï¼‰

å¦‚æœæƒ³äº†è§£æ›´å¤šå…³äº fallbacks çš„å†…å®¹ï¼Œè¯·å‚è€ƒ[Fallbacks | ğŸ¦œï¸ğŸ”— LangChain](https://python.langchain.com/v0.1/docs/guides/productionization/fallbacks/)

# æ¥å£ Interface
åœ¨ä½¿ç”¨LangChainä¸­ï¼Œå­˜åœ¨è®¸å¤šæ¥å£ï¼Œå…¶ä¸­å…¬å¼€çš„æ ‡å‡†æ¥å£åŒ…æ‹¬ï¼š

+ streamï¼šæµå¼è¿”å›è¾“å‡ºå†…å®¹
+ invokeï¼šè¾“å…¥è°ƒç”¨chain
+ batchï¼šåœ¨è¾“å…¥åˆ—è¡¨ä¸­å¹¶è¡Œè°ƒç”¨chain

è¿™äº›ä¹Ÿæœ‰ç›¸åº”çš„å¼‚æ­¥æ–¹æ³•ï¼š

+ astreamï¼šå¼‚æ­¥æµå¼è¿”å›è¾“å‡ºå†…å®¹
+ ainvokeï¼šåœ¨è¾“å…¥ä¸Šå¼‚æ­¥è°ƒç”¨chain
+ abatchï¼šåœ¨è¾“å…¥åˆ—è¡¨ä¸­å¹¶è¡Œå¼‚æ­¥è°ƒç”¨chain

é¦–å…ˆå®šä¹‰ç»™ä¸€ä¸ªç®€å•æç¤ºæ¨¡æ¿ï¼Œä¹Ÿå°±æ˜¯"ç»™æˆ‘è®²ä¸€ä¸ªå…³äº{ä¸»é¢˜}çš„çŸ­ç¬‘è¯"ï¼Œç„¶åå®šä¹‰äº†ä¸€ä¸ªç®€å•çš„é“¾`Chain = prompt | LLM | OutputParser`ã€‚

```python
# åˆ›å»ºä¸€ä¸ªChatPromptTemplateå¯¹è±¡ï¼Œä½¿ç”¨æ¨¡æ¿"ç»™æˆ‘è®²ä¸€ä¸ªå…³äº{topic}çš„çŸ­ç¬‘è¯"
prompt = ChatPromptTemplate.from_template(
    "ç»™æˆ‘è®²ä¸€ä¸ªå…³äº{topic}çš„çŸ­ç¬‘è¯"
)

# åˆ›å»ºä¸€ä¸ªChatOpenAIæ¨¡å‹
model = ChatOpenAI()

# åˆ›å»ºä¸€ä¸ªStrOutputParserå¯¹è±¡
output_parser = StrOutputParser()

# åˆ›å»ºä¸€ä¸ªchainï¼Œå°†promptã€modelå’Œoutput_parserè¿æ¥èµ·æ¥
chain = prompt | model | output_parser
```

##  invokeæ¥å£
æ¥ä¸‹æ¥åˆ†åˆ«ä½¿ç”¨å¯¹åº”çš„æ¥å£ï¼Œæ¯”å¦‚é¦–å…ˆä½¿ç”¨å¸¸è§„çš„`invoke`çš„è°ƒç”¨ï¼Œè¿™ä¸ªä¹Ÿæ˜¯å‰é¢å±•ç°çš„æ–¹æ³•ï¼Œå¾—åˆ°äº†å¯¹åº”ç»“æœã€‚

```python
chain.invoke({"topic": "ç†Š"})
```

<details class="lake-collapse"><summary id="u1a621b29"><span class="ne-text">outputï¼š</span></summary><pre data-language="python" id="rNEgi" class="ne-codeblock language-python"><code>'å½“ç†Šåœ¨æ£®æ—é‡Œé‡åˆ°ä¸€åªå…”å­æ—¶ï¼Œä»–é—®ï¼šâ€œå…”å­å…ˆç”Ÿï¼Œä½ æœ‰æ²¡æœ‰é—®é¢˜ï¼Ÿâ€å…”å­å›ç­”é“ï¼šâ€œå½“ç„¶ï¼Œå…ˆç”Ÿç†Šï¼Œæˆ‘æœ‰ä¸€ä¸ªé—®é¢˜ã€‚ä½ æ€ä¹ˆä¼šæ‹‰è¿™ä¹ˆé•¿çš„å°¾å·´ï¼Ÿâ€ç†Šå¬åç¬‘äº†èµ·æ¥ï¼šâ€œå…”å­å…ˆç”Ÿï¼Œè¿™ä¸æ˜¯å°¾å·´ï¼Œè¿™æ˜¯æˆ‘çš„é¢†å¸¦ï¼â€'</code></pre></details>
## batchæ¥å£
å†å°è¯•ä½¿ç”¨`batch`çš„æ¥å£ï¼Œä¼šå‘ç°å¤§æ¨¡å‹å¯ä»¥è¿”å›ä¸¤ä¸ªé—®é¢˜çš„ç­”æ¡ˆï¼Œä¼šç»™chainä¸€ä¸ªè¾“å…¥çš„åˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­å¯ä»¥åŒ…å«å¤šä¸ªé—®é¢˜ï¼Œæœ€åè¿”å›å¤šä¸ªé—®é¢˜çš„ç­”æ¡ˆã€‚

```python
chain.batch([{"topic": "ç†Š"}, {"topic": "ç‹ç‹¸"}]) 
```

<details class="lake-collapse"><summary id="udcf4e609"><span class="ne-text">outputï¼š</span></summary><pre data-language="python" id="Z5TrO" class="ne-codeblock language-python"><code>['å¥½çš„ï¼Œè¿™æ˜¯ä¸€ä¸ªå…³äºç†Šçš„çŸ­ç¬‘è¯ï¼š\n\næœ‰ä¸€å¤©ï¼Œä¸€åªç†Šèµ°è¿›äº†ä¸€å®¶é…’å§ã€‚ä»–èµ°åˆ°å§å°å‰ï¼Œå¯¹é…’ä¿è¯´ï¼šâ€œè¯·ç»™æˆ‘ä¸€æ¯â€¦â€¦èœ‚èœœå•¤é…’ã€‚â€\n\né…’ä¿ç–‘æƒ‘åœ°çœ‹ç€ç†Šï¼Œè¯´ï¼šâ€œå¯¹ä¸èµ·ï¼Œè¿™é‡Œæ²¡æœ‰èœ‚èœœå•¤é…’ã€‚â€\n\nç†Šæœ‰äº›å¤±æœ›åœ°å¹äº†å£æ°”ï¼Œç„¶åè¯´ï¼šâ€œå¥½å§ï¼Œé‚£å°±ç»™æˆ‘æ¥ä¸€æ¯â€¦â€¦è‰è“é…’å§ã€‚â€\n\né…’ä¿æ‘‡æ‘‡å¤´ï¼Œè¯´ï¼šâ€œæŠ±æ­‰ï¼Œä¹Ÿæ²¡æœ‰è‰è“é…’ã€‚â€\n\nç†Šåˆå¹äº†å£æ°”ï¼Œç„¶åè¯´ï¼šâ€œé‚£è¯·ç»™æˆ‘æ¥ä¸€æ¯â€¦â€¦èœœç³–çº¢é…’å§ã€‚â€\n\né…’ä¿å®åœ¨æ— æ³•å¿å—äº†ï¼Œä»–å¯¹ç†Šè¯´ï¼šâ€œå¯¹ä¸èµ·ï¼Œè¿™é‡Œæ²¡æœ‰è¿™äº›å¥‡æ€ªçš„é…’ï¼Œä½ æ˜¯ç†Šï¼Œä½ åº”è¯¥çŸ¥é“ç†Šåªèƒ½å–èœ‚èœœã€‚â€\n\nç†Šå¬åä¸€æ„£ï¼Œç„¶åè„¸è‰²ä¸€å˜ï¼Œè¯´ï¼šâ€œåŸæ¥ä½ ä»¬è¿™é‡Œæ²¡æœ‰èœ‚èœœå•¤é…’ï¼Œè‰è“é…’å’Œèœœç³–çº¢é…’ï¼Ÿé‚£è¯·ç»™æˆ‘æ¥ä¸€æ¯â€¦â€¦ç™½å¼€æ°´å§ã€‚â€',
 'æœ‰ä¸€å¤©ï¼Œä¸€åªç‹ç‹¸åœ¨æ£®æ—é‡Œé‡åˆ°äº†ä¸€åªå…”å­ã€‚ç‹ç‹¸ç¬‘å˜»å˜»åœ°å¯¹å…”å­è¯´ï¼šâ€œå–‚ï¼Œå…”å­ï¼Œæˆ‘æœ‰ä¸€ä¸ªå¥½æ¶ˆæ¯å’Œä¸€ä¸ªåæ¶ˆæ¯ï¼Œä½ æƒ³å…ˆå¬å“ªä¸ªï¼Ÿâ€å…”å­æœ‰äº›å¥½å¥‡åœ°é—®ï¼šâ€œé‚£å°±å…ˆå‘Šè¯‰æˆ‘å¥½æ¶ˆæ¯å§ã€‚â€ç‹ç‹¸çœ¯èµ·çœ¼ç›è¯´ï¼šâ€œå¥½æ¶ˆæ¯æ˜¯ï¼Œä½ çš„æ™ºå•†æ¯”æˆ‘é«˜ã€‚â€å…”å­é«˜å…´åœ°è·³äº†èµ·æ¥ï¼šâ€œå¤ªå¥½äº†ï¼Œé‚£åæ¶ˆæ¯æ˜¯ä»€ä¹ˆï¼Ÿâ€ç‹ç‹¸ä¸€è„¸è½»æ¾åœ°è¯´ï¼šâ€œåæ¶ˆæ¯æ˜¯ï¼Œä½ çš„æ™ºå•†è¿˜æ¯”ä¸è¿‡èƒ¡èåœã€‚â€']</code></pre></details>
## streamæ¥å£
æ¥ä¸‹æ¥åœ¨çœ‹çœ‹`stream`æ¥å£ï¼Œä¹Ÿå°±æ˜¯æµå¼è¾“å‡ºå†…å®¹ï¼Œè¿™æ ·çš„åŠŸèƒ½å¾ˆæœ‰å¿…è¦ï¼Œæœ‰æ—¶å€™å¯ä»¥å…å»ç”¨æˆ·ç­‰å¾…çš„çƒ¦æ¼ï¼Œè®©ç”¨æˆ·çœ‹åˆ°ä¸€ä¸ªä¸€ä¸ªè¯è¹¦å‡ºæ¥è€Œä¸æ˜¯ä¸€ä¸ªç©ºçš„å±å¹•ï¼Œè¿™æ ·ä¼šå¸¦æ¥æ›´å¥½çš„ç”¨æˆ·ä½“éªŒã€‚

```python
for t in chain.stream({"topic": "ç†Š"}):
    print(t)
```

<details class="lake-collapse"><summary id="u2f401b5e"><span class="ne-text">outputï¼š</span></summary><pre data-language="python" id="FQtUw" class="ne-codeblock language-python"><code>å¥½
çš„
ï¼Œ
è¿™
æ˜¯
ä¸€ä¸ª
å…³
äº
ç†Š
çš„
çŸ­
ç¬‘
è¯
ï¼š


æœ‰
ä¸€
å¤©
ï¼Œ
ä¸€
åª
å°
ç†Š
èµ°
è¿›
äº†
ä¸€
å®¶
é…’
å§
ã€‚
ä»–
èµ°
åˆ°
å§
å°
å‰
ï¼Œ
å¯¹
é…’
ä¿
è¯´
ï¼šâ€œ
é…’
ä¿
ï¼Œ
ç»™
æˆ‘
ä¸€
æ¯
ç‰›
å¥¶
ã€‚â€


é…’
ä¿
æƒŠ
è®¶
åœ°
é—®
é“
ï¼šâ€œ
å°
ç†Š
ï¼Œ
ä½ 
æ€
ä¹ˆ
ä¼š
æ¥
è¿™
é‡Œ
å–
ç‰›
å¥¶
ï¼Ÿ
â€


å°
ç†Š
æ·±
æƒ…
åœ°
å›
ç­”
ï¼šâ€œ
å› 
ä¸º
æˆ‘çš„
å¦ˆ
å¦ˆ
è¯´
ï¼Œ
æ¯
å½“
æˆ‘
å–
é…’
çš„
æ—¶
å€™
ï¼Œ
æˆ‘
éƒ½
ä¼š
å˜
å¾—
ç†Š
æ ·
ï¼â€</code></pre></details>
## å¼‚æ­¥æ¥å£
è¿˜å¯ä»¥å°è¯•å¼‚æ­¥æ¥è°ƒç”¨ï¼Œä½¿ç”¨`ainvoke`æ¥è°ƒç”¨ã€‚

```python
response = await chain.ainvoke({"topic": "ç†Š"})
response
```

<details class="lake-collapse"><summary id="u3ffaadaf"><span class="ne-text" style="color: rgba(0, 0, 0, 0.87)">outputï¼š</span></summary><pre data-language="python" id="Lhj50" class="ne-codeblock language-python"><code>'å¥½çš„ï¼Œä»¥ä¸‹æ˜¯ä¸€ä¸ªå…³äºç†Šçš„çŸ­ç¬‘è¯ï¼š\n\næœ‰ä¸€åªç†Šèµ°è¿›äº†ä¸€å®¶é¤å…ï¼Œä»–èµ°åˆ°æŸœå°å‰ï¼Œå¯¹ç€æœåŠ¡å‘˜è¯´ï¼šâ€œæˆ‘æƒ³è¦ä¸€æ¯å’–å•¡å’Œ......å—¯ï¼Œä¸€å—...ç‰›è‚‰ä¸‰æ˜æ²»ã€‚â€\næœåŠ¡å‘˜ç–‘æƒ‘åœ°çœ‹ç€ç†Šï¼Œç„¶åé—®é“ï¼šâ€œå¯¹ä¸èµ·ï¼Œå…ˆç”Ÿï¼Œä½ æ˜¯çœŸçš„æƒ³è¦ä¸€å—ç‰›è‚‰ä¸‰æ˜æ²»å—ï¼Ÿâ€\nç†Šç‚¹äº†ç‚¹å¤´ã€‚\næœåŠ¡å‘˜åˆé—®ï¼šâ€œé‚£è¯·é—®ä¸ºä»€ä¹ˆä½ è¦æ¥è¿™é‡Œç‚¹é¤å‘¢ï¼Ÿâ€\nç†Šå›ç­”é“ï¼šâ€œå› ä¸ºæˆ‘æ˜¯ä¸ªç†Šå•Šï¼â€'</code></pre></details>
#  è‹±æ–‡æç¤ºè¯
## æ„å»ºç®€å•é“¾
```python
prompt = ChatPromptTemplate.from_template(
    "tell me a short joke about {topic}"
)
model = ChatOpenAI()
output_parser = StrOutputParser()

chain = prompt | model | output_parser

chain.invoke({"topic": "bears"})
```

<details class="lake-collapse"><summary id="ub4bd0604"><span class="ne-text" style="color: var(--jp-cell-prompt-not-active-font-color)">outputï¼š</span></summary><pre data-language="python" id="gEmPm" class="ne-codeblock language-python"><code>'Why did the bear bring a flashlight to the party?\n\nBecause he wanted to be the &quot;light&quot; of the bearbecue!'</code></pre></details>
## æ„å»ºç®€å•æ–‡æ¡£æ•°æ®åº“
```python
vectorstore = DocArrayInMemorySearch.from_texts(
    ["harrison worked at kensho", "bears like to eat honey"],
    embedding=OpenAIEmbeddings()
)
retriever = vectorstore.as_retriever()
```

```python
retriever.get_relevant_documents("where did harrison work?")
```

<details class="lake-collapse"><summary id="u65178964"><span class="ne-text">outputï¼š</span></summary><pre data-language="python" id="L3vJt" class="ne-codeblock language-python"><code>[Document(page_content='harrison worked at kensho'),
 Document(page_content='bears like to eat honey')]</code></pre></details>
```python
retriever.get_relevant_documents("what do bears like to eat")
```

<details class="lake-collapse"><summary id="u81ae1e12"><span class="ne-text">outputï¼š</span></summary><pre data-language="python" id="gVkr8" class="ne-codeblock language-python"><code>[Document(page_content='bears like to eat honey'),
 Document(page_content='harrison worked at kensho')]</code></pre></details>
```python
template = """Answer the question based only on the following context:
{context}

Question: {question}
"""
prompt = ChatPromptTemplate.from_template(template)

chain = RunnableMap({
    "context": lambda x: retriever.get_relevant_documents(x["question"]),
    "question": lambda x: x["question"]
}) | prompt | model | output_parser

chain.invoke({"question": "where did harrison work?"})
```

<details class="lake-collapse"><summary id="u0225fd20"><span class="ne-text">outputï¼š</span></summary><pre data-language="python" id="JqOY1" class="ne-codeblock language-python"><code>'Harrison worked at Kensho.'</code></pre></details>
## ä½¿ç”¨RunnableMap
```python
inputs = RunnableMap({
    "context": lambda x: retriever.get_relevant_documents(x["question"]),
    "question": lambda x: x["question"]
})

inputs.invoke({"question": "where did harrison work?"})
```

<details class="lake-collapse"><summary id="uabb03fcb"><span class="ne-text">outputï¼š</span></summary><pre data-language="python" id="aYPZR" class="ne-codeblock language-python"><code>{'context': [Document(page_content='harrison worked at kensho'),
             Document(page_content='bears like to eat honey')],
 'question': 'where did harrison work?'}</code></pre></details>
## å•å‡½æ•°ç»‘å®š
```python
functions = [
    {
      "name": "weather_search",
      "description": "Search for weather given an airport code",
      "parameters": {
        "type": "object",
        "properties": {
          "airport_code": {
            "type": "string",
            "description": "The airport code to get the weather for"
          },
        },
        "required": ["airport_code"]
      }
    }
  ]
```

```python
prompt = ChatPromptTemplate.from_messages(
    [
        ("human", "{input}")
    ]
)
model = ChatOpenAI(temperature=0).bind(functions=functions)

runnable = prompt | model

runnable.invoke({"input": "what is the weather in sf"})
```

<details class="lake-collapse"><summary id="uf5f5159b"><span class="ne-text" style="color: rgba(0, 0, 0, 0.87)">outputï¼š</span></summary><pre data-language="python" id="MG5IH" class="ne-codeblock language-python"><code>AIMessage(content='', additional_kwargs={'function_call': {'name': 'weather_search', 'arguments': '{\n  &quot;airport_code&quot;: &quot;SFO&quot;\n}'}})</code></pre></details>
## å¤šä¸ªå‡½æ•°ç»‘å®š
```python
functions = [
    {
      "name": "weather_search",
      "description": "Search for weather given an airport code",
      "parameters": {
        "type": "object",
        "properties": {
          "airport_code": {
            "type": "string",
            "description": "The airport code to get the weather for"
          },
        },
        "required": ["airport_code"]
      }
    },
        {
      "name": "sports_search",
      "description": "Search for news of recent sport events",
      "parameters": {
        "type": "object",
        "properties": {
          "team_name": {
            "type": "string",
            "description": "The sports team to search for"
          },
        },
        "required": ["team_name"]
      }
    }
  ]
```

```python
model = model.bind(functions=functions)

runnable = prompt | model

runnable.invoke({"input": "how did the patriots do yesterday?"})
```

<details class="lake-collapse"><summary id="u0f8abbc7"><span class="ne-text" style="color: rgba(0, 0, 0, 0.87)">outputï¼š</span></summary><pre data-language="python" id="jaJQQ" class="ne-codeblock language-python"><code>AIMessage(content='', additional_kwargs={'function_call': {'name': 'sports_search', 'arguments': '{\n  &quot;team_name&quot;: &quot;patriots&quot;\n}'}})</code></pre></details>
## ä½¿ç”¨æ—©æœŸæ¨¡å‹æ ¼å¼åŒ–è¾“å‡º
```python
simple_model = OpenAI(
    temperature=0,
    max_tokens=1000,
    model="gpt-3.5-turbo-instruct"
)
simple_chain = simple_model | json.loads

challenge = "write three poems in a json blob, where each poem is a json blob of a title, author, and first line"

simple_model.invoke(challenge)
```

<details class="lake-collapse"><summary id="u4cbc8a95"><span class="ne-text" style="color: rgba(0, 0, 0, 0.87)">outputï¼š</span></summary><pre data-language="python" id="kDTUI" class="ne-codeblock language-python"><code>'\n\n{\n    &quot;title&quot;: &quot;Autumn Leaves&quot;,\n    &quot;author&quot;: &quot;Emily Dickinson&quot;,\n    &quot;first_line&quot;: &quot;The leaves are falling, one by one&quot;\n}\n\n{\n    &quot;title&quot;: &quot;The Ocean\'s Song&quot;,\n    &quot;author&quot;: &quot;Pablo Neruda&quot;,\n    &quot;first_line&quot;: &quot;I hear the ocean\'s song, a symphony of waves&quot;\n}\n\n{\n    &quot;title&quot;: &quot;A Winter\'s Night&quot;,\n    &quot;author&quot;: &quot;Robert Frost&quot;,\n    &quot;first_line&quot;: &quot;The snow falls softly, covering the ground&quot;\n}'</code></pre></details>
**æ—©æœŸæ¨¡å‹ä¸æ”¯æŒï¼Œä¼šå‡ºç°è§£ç é”™è¯¯**

```python
 simple_chain.invoke(challenge)      
```

<details class="lake-collapse"><summary id="ua1b0ea57"><span class="ne-text">outputï¼š</span></summary><pre data-language="python" id="C8Kd2" class="ne-codeblock language-python"><code>---------------------------------------------------------------------------
JSONDecodeError                           Traceback (most recent call last)
&lt;ipython-input-39-7b2363c45b31&gt; in &lt;cell line: 1&gt;()
----&gt; 1 simple_chain.invoke(challenge)

/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py in invoke(self, input, config)
2051         try:
    2052             for i, step in enumerate(self.steps):
        -&gt; 2053                 input = step.invoke(
            2054                     input,
            2055                     # mark each step as a child run

            /usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py in invoke(self, input, config, **kwargs)
            3505         &quot;&quot;&quot;Invoke this runnable synchronously.&quot;&quot;&quot;
3506         if hasattr(self, &quot;func&quot;):
-&gt; 3507             return self._call_with_config(
    3508                 self._invoke,
    3509                 input,

    /usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py in _call_with_config(self, func, input, config, run_type, **kwargs)
1244             output = cast(
    1245                 Output,
    -&gt; 1246                 context.run(
        1247                     call_func_with_variable_args,
        1248                     func,  # type: ignore[arg-type]

        /usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py in call_func_with_variable_args(func, input, config, run_manager, **kwargs)
    324     if run_manager is not None and accepts_run_manager(func):
325         kwargs[&quot;run_manager&quot;] = run_manager
--&gt; 326     return func(input, **kwargs)  # type: ignore[call-arg]
327 
328 

/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py in _invoke(self, input, run_manager, config, **kwargs)
3381                         output = chunk
3382         else:
-&gt; 3383             output = call_func_with_variable_args(
    3384                 self.func, input, config, run_manager, **kwargs
    3385             )

/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py in call_func_with_variable_args(func, input, config, run_manager, **kwargs)
324     if run_manager is not None and accepts_run_manager(func):
325         kwargs[&quot;run_manager&quot;] = run_manager
--&gt; 326     return func(input, **kwargs)  # type: ignore[call-arg]
327 
328 

/usr/lib/python3.10/json/__init__.py in loads(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)
344             parse_int is None and parse_float is None and
345             parse_constant is None and object_pairs_hook is None and not kw):
--&gt; 346         return _default_decoder.decode(s)
347     if cls is None:
348         cls = JSONDecoder

/usr/lib/python3.10/json/decoder.py in decode(self, s, _w)
338         end = _w(s, end).end()
339         if end != len(s):
--&gt; 340             raise JSONDecodeError(&quot;Extra data&quot;, s, end)
341         return obj
342 

JSONDecodeError: Extra data: line 9 column 1 (char 125)</code></pre></details>
## è¾ƒæ–°çš„æ¨¡å‹èƒ½å¤Ÿæ ¼å¼åŒ–è¾“å‡º
```python
model = ChatOpenAI(temperature=0)
chain = model | StrOutputParser() | json.loads

chain.invoke(challenge)
```

<details class="lake-collapse"><summary id="ue9128691"><span class="ne-text">outputï¼š</span></summary><pre data-language="python" id="rx8Uw" class="ne-codeblock language-python"><code>{'poem1': {'title': 'Whispers of the Wind',
  'author': 'Emily Rivers',
  'first_line': 'Softly it comes, the whisper of the wind'},
 'poem2': {'title': 'Silent Serenade',
  'author': 'Jacob Moore',
  'first_line': 'In the stillness of night, a silent serenade'},
 'poem3': {'title': 'Dancing Shadows',
  'author': 'Sophia Anderson',
  'first_line': 'Shadows dance upon the walls, a secret ballet'}}</code></pre></details>
## fallbackæœºåˆ¶
```python
final_chain = simple_chain.with_fallbacks([chain])

final_chain.invoke(challenge)
```

<details class="lake-collapse"><summary id="ub381834a"><span class="ne-text">outputï¼š</span></summary><pre data-language="python" id="qGpWi" class="ne-codeblock language-python"><code>{'poem1': {'title': 'Whispers of the Wind',
  'author': 'Emily Rivers',
  'first_line': 'Softly it comes, the whisper of the wind'},
 'poem2': {'title': 'Silent Serenade',
  'author': 'Jacob Moore',
  'first_line': 'In the stillness of night, a silent serenade'},
 'poem3': {'title': 'Dancing Shadows',
  'author': 'Sophia Anderson',
  'first_line': 'Shadows dance upon the moonlit floor'}}</code></pre></details>
## æ¥å£
```python
prompt = ChatPromptTemplate.from_template(
    "Tell me a short joke about {topic}"
)
model = ChatOpenAI()
output_parser = StrOutputParser()

chain = prompt | model | output_parser
```

### invokeæ¥å£
```python
chain.invoke({"topic": "bears"})
```

<details class="lake-collapse"><summary id="u86678713"><span class="ne-text">outputï¼š</span></summary><pre data-language="python" id="i81jA" class="ne-codeblock language-python"><code>&quot;Why don't bears wear shoes?\n\nBecause they have bear feet!&quot;</code></pre></details>
### batchæ¥å£
```python
chain.batch([{"topic": "bears"}, {"topic": "frogs"}])
```

<details class="lake-collapse"><summary id="u7216c573"><span class="ne-text">outputï¼š</span></summary><pre data-language="python" id="DE2T0" class="ne-codeblock language-python"><code>[&quot;Why don't bears wear shoes?\n\nBecause they have bear feet!&quot;,
 'Why did the frog take the bus to work?\n\nBecause his car got toad away!']</code></pre></details>
### streamæ¥å£
```python
for t in chain.stream({"topic": "bears"}):
    print(t)
```

<details class="lake-collapse"><summary id="ue6499764"><span class="ne-text">outputï¼š</span></summary><pre data-language="python" id="u9Ql6" class="ne-codeblock language-python"><code>Why
 don
't
 bears
 wear
 shoes
?


Because
 they
 have
 bear
 feet
!</code></pre></details>
### å¼‚æ­¥æ¥å£
```python
response = await chain.ainvoke({"topic": "bears"})
response
```

<details class="lake-collapse"><summary id="ucc31cb81"><span class="ne-text">outputï¼š</span></summary><pre data-language="python" id="yCewl" class="ne-codeblock language-python"><code>&quot;Why don't bears wear shoes?\n\nBecause they have bear feet!&quot;</code></pre></details>
