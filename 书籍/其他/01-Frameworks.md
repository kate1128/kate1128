## Official Version 
### ç¥ç»ç½‘ç»œæ¶æ„ï¼ˆ<font style="color:rgb(31, 35, 40);">Neural Network Architectureï¼‰</font>
+ [Transformer](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py) ![](https://img.shields.io/github/stars/tensorflow/tensor2tensor?style=social) : "Attention is All You Need". ([arXiv 2017](https://arxiv.org/abs/1706.03762)).
+ [KAN](https://github.com/KindXiaoming/pykan) ![](https://img.shields.io/github/stars/KindXiaoming/pykan?style=social) : "KAN: Kolmogorov-Arnold Networks". ([arXiv 2024](https://arxiv.org/abs/2404.19756)).

### å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰Large Language Model 
#### GPT
+ GPT-1 : "Improving Language Understanding by Generative Pre-Training". ([cs.ubc.ca, 2018](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)).
+ [GPT-2](https://github.com/openai/gpt-2) ![](https://img.shields.io/github/stars/openai/gpt-2?style=social) : "Language Models are Unsupervised Multitask Learners". ([OpenAI blog, 2019](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)). [Better language models and their implications](https://openai.com/research/better-language-models).
+ [GPT-3](https://github.com/openai/gpt-3) ![](https://img.shields.io/github/stars/openai/gpt-3?style=social) : "GPT-3: Language Models are Few-Shot Learners". ([arXiv 2020](https://arxiv.org/abs/2005.14165)).
+ InstructGPT : "Training language models to follow instructions with human feedback". ([arXiv 2022](https://arxiv.org/abs/2203.02155)). "Aligning language models to follow instructions". ([OpenAI blog, 2022](https://openai.com/research/instruction-following)).
+ [ChatGPT](https://chat.openai.com/): [Optimizing Language Models for Dialogue](https://openai.com/blog/chatgpt).
+ [GPT-4](https://openai.com/product/gpt-4): GPT-4 is OpenAIâ€™s most advanced system, producing safer and more useful responses. "Sparks of Artificial General Intelligence: Early experiments with GPT-4". ([arXiv 2023](https://arxiv.org/abs/2303.12712)). "GPT-4 Architecture, Infrastructure, Training Dataset, Costs, Vision, MoE". ([SemianAlysis, 2023](https://www.semianalysis.com/p/gpt-4-architecture-infrastructure)).
+ [GPT-4-LLM](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM) ![](https://img.shields.io/github/stars/Instruction-Tuning-with-GPT-4/GPT-4-LLM?style=social) : "Instruction Tuning with GPT-4". ([arXiv 2023](https://arxiv.org/abs/2304.03277)). [instruction-tuning-with-gpt-4.github.io/](https://instruction-tuning-with-gpt-4.github.io/)
+ [MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4) ![](https://img.shields.io/github/stars/Vision-CAIR/MiniGPT-4?style=social) : MiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models. [minigpt-4.github.io](https://minigpt-4.github.io/)
+ [minGPT](https://github.com/karpathy/minGPT) ![](https://img.shields.io/github/stars/karpathy/minGPT?style=social) : A minimal PyTorch re-implementation of the OpenAI GPT (Generative Pretrained Transformer) training.
+ [nanoGPT](https://github.com/karpathy/nanoGPT) ![](https://img.shields.io/github/stars/karpathy/nanoGPT?style=social) : The simplest, fastest repository for training/finetuning medium-sized GPTs.
+ [MicroGPT](https://github.com/muellerberndt/micro-gpt) ![](https://img.shields.io/github/stars/muellerberndt/micro-gpt?style=social) : A simple and effective autonomous agent compatible with GPT-3.5-Turbo and GPT-4. MicroGPT aims to be as compact and reliable as possible.
+ [GPT4All](https://github.com/nomic-ai/gpt4all) ![](https://img.shields.io/github/stars/nomic-ai/gpt4all?style=social) : GPT4All: An ecosystem of open-source on-edge large language models. GTP4All is an ecosystem to train and deploy powerful and customized large language models that run locally on consumer grade CPUs.
+ [WorkGPT](https://github.com/h2oai/h2ogpt) ![](https://img.shields.io/github/stars/h2oai/h2ogpt?style=social) : WorkGPT is an agent framework in a similar fashion to AutoGPT or LangChain.
+ [h2oGPT](https://github.com/team-openpm/workgpt) ![](https://img.shields.io/github/stars/team-openpm/workgpt?style=social) : h2oGPT is a large language model (LLM) fine-tuning framework and chatbot UI with document(s) question-answer capabilities. "h2oGPT: Democratizing Large Language Models". ([arXiv 2023](https://arxiv.org/abs/2306.08161)).
+ [DemoGPT](https://github.com/melih-unsal/DemoGPT) ![](https://img.shields.io/github/stars/melih-unsal/DemoGPT?style=social) : Create ğŸ¦œï¸ğŸ”— LangChain apps by just using prompts with the power of Llama 2 ğŸŒŸ Star to support our work! | åªéœ€ä½¿ç”¨å¥å­å³å¯åˆ›å»º LangChain åº”ç”¨ç¨‹åºã€‚ ç»™ä¸ªstaræ”¯æŒæˆ‘ä»¬çš„å·¥ä½œå§ï¼DemoGPT: Auto Gen-AI App Generator with the Power of Llama 2. âš¡ With just a prompt, you can create interactive Streamlit apps via ğŸ¦œï¸ğŸ”— LangChain's transformative capabilities & Llama 2.âš¡ [demogpt.io](https://www.demogpt.io/)
+ [GPT-Engineer](https://github.com/AntonOsika/gpt-engineer) ![](https://img.shields.io/github/stars/AntonOsika/gpt-engineer?style=social) : Specify what you want it to build, the AI asks for clarification, and then builds it. GPT Engineer is made to be easy to adapt, extend, and make your agent learn how you want your code to look. It generates an entire codebase based on a prompt.
+ [1595901624/gpt-aggregated-edition](https://github.com/1595901624/gpt-aggregated-edition) ![](https://img.shields.io/github/stars/1595901624/gpt-aggregated-edition?style=social) : èšåˆChatGPTå®˜æ–¹ç‰ˆã€ChatGPTå…è´¹ç‰ˆã€æ–‡å¿ƒä¸€è¨€ã€Poeã€chatchatç­‰å¤šå¹³å°ï¼Œæ”¯æŒè‡ªå®šä¹‰å¯¼å…¥å¹³å°ã€‚
+ [SpeechGPT](https://github.com/0nutation/SpeechGPT) ![](https://img.shields.io/github/stars/0nutation/SpeechGPT?style=social) : "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities". ([arXiv 2023](https://arxiv.org/abs/2305.11000)).
+ [GPT2-Chinese](https://github.com/Morizeyao/GPT2-Chinese) ![](https://img.shields.io/github/stars/Morizeyao/GPT2-Chinese?style=social) : Chinese version of GPT2 training code, using BERT tokenizer.
+ [gpt-llm-trainer](https://github.com/mshumer/gpt-llm-trainer) ![](https://img.shields.io/github/stars/mshumer/gpt-llm-trainer?style=social) : The goal of this project is to explore an experimental new pipeline to train a high-performing task-specific model. We try to abstract away all the complexity, so it's as easy as possible to go from idea -> performant fully-trained model.

#### deekseek
+ [GitHub - deepseek-ai/DeepSeek-R1](https://github.com/deepseek-ai/DeepSeek-R1)![](https://cdn.nlark.com/yuque/0/2025/png/2639475/1748399041967-f6e17b3a-9aab-4d31-97b6-1f6fe4d35282.png)ï¼šdeepseek R1

#### Llama
+ [Llama 2](https://github.com/facebookresearch/llama) ![](https://img.shields.io/github/stars/facebookresearch/llama?style=social) : Inference code for LLaMA models. "LLaMA: Open and Efficient Foundation Language Models". ([arXiv 2023](https://arxiv.org/abs/2302.13971)). "Llama 2: Open Foundation and Fine-Tuned Chat Models". ([ai.meta.com, 2023-07-18](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)). ([2023-07-18, Llama 2 is here - get it on Hugging Face](https://huggingface.co/blog/llama2)).
+ [Llama 3](https://github.com/meta-llama/llama3) ![](https://img.shields.io/github/stars/meta-llama/llama3?style=social) : The official Meta Llama 3 GitHub site.
+ [Lit-LLaMA](https://github.com/Lightning-AI/lit-llama) ![](https://img.shields.io/github/stars/Lightning-AI/lit-llama?style=social) : âš¡ Lit-LLaMA. Implementation of the LLaMA language model based on nanoGPT. Supports flash attention, Int8 and GPTQ 4bit quantization, LoRA and LLaMA-Adapter fine-tuning, pre-training. Apache 2.0-licensed.
+ [LongLLaMA ](https://github.com/CStanKonrad/long_llama) ![](https://img.shields.io/github/stars/CStanKonrad/long_llama?style=social) : LongLLaMA is a large language model capable of handling long contexts. It is based on OpenLLaMA and fine-tuned with the Focused Transformer (FoT) method.
+ [LLaMA-Adapter](https://github.com/OpenGVLab/LLaMA-Adapter) ![](https://img.shields.io/github/stars/OpenGVLab/LLaMA-Adapter?style=social) : Fine-tuning LLaMA to follow Instructions within 1 Hour and 1.2M Parameters. LLaMA-Adapter: Efficient Fine-tuning of LLaMA ğŸš€
+ [Llama-2-Onnx](https://github.com/microsoft/Llama-2-Onnx) ![](https://img.shields.io/github/stars/microsoft/Llama-2-Onnx?style=social) : Llama 2 Powered By ONNX.
+ [Chinese LLaMA and Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca) ![](https://img.shields.io/github/stars/ymcui/Chinese-LLaMA-Alpaca?style=social) : ä¸­æ–‡LLaMA&Alpacaå¤§è¯­è¨€æ¨¡å‹+æœ¬åœ°CPU/GPUè®­ç»ƒéƒ¨ç½² (Chinese LLaMA & Alpaca LLMs)ã€‚"Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca". ([arXiv 2023](https://arxiv.org/abs/2304.08177)).
+ [Chinese-LLaMA-Alpaca-2](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2) ![](https://img.shields.io/github/stars/ymcui/Chinese-LLaMA-Alpaca-2?style=social) : ä¸­æ–‡ LLaMA-2 & Alpaca-2 å¤§æ¨¡å‹äºŒæœŸé¡¹ç›® (Chinese LLaMA-2 & Alpaca-2 LLMs).
+ [FlagAlpha/Llama2-Chinese](https://github.com/FlagAlpha/Llama2-Chinese) ![](https://img.shields.io/github/stars/FlagAlpha/Llama2-Chinese?style=social) : Llamaä¸­æ–‡ç¤¾åŒºï¼Œæœ€å¥½çš„ä¸­æ–‡Llamaå¤§æ¨¡å‹ï¼Œå®Œå…¨å¼€æºå¯å•†ç”¨ã€‚
+ [michael-wzhu/Chinese-LlaMA2](https://github.com/michael-wzhu/Chinese-LlaMA2) ![](https://img.shields.io/github/stars/michael-wzhu/Chinese-LlaMA2?style=social) : Repo for adapting Meta LlaMA2 in Chinese! METAæœ€æ–°å‘å¸ƒçš„LlaMA2çš„æ±‰åŒ–ç‰ˆï¼ ï¼ˆå®Œå…¨å¼€æºå¯å•†ç”¨ï¼‰
+ [CrazyBoyM/llama3-Chinese-chat](https://github.com/CrazyBoyM/llama3-Chinese-chat) ![](https://img.shields.io/github/stars/CrazyBoyM/llama3-Chinese-chat?style=social) : Llama3 ä¸­æ–‡ç‰ˆã€‚

#### Claude
+ [Claude](https://www.anthropic.com/product) : Claude is a next-generation AI assistant based on Anthropicâ€™s research into training helpful, honest, and harmless AI systems.

#### Qwen
+ [Qwenï¼ˆé€šä¹‰åƒé—®ï¼‰](https://github.com/QwenLM/Qwen) ![](https://img.shields.io/github/stars/QwenLM/Qwen?style=social) : The official repo of Qwen (é€šä¹‰åƒé—®) chat & pretrained large language model proposed by Alibaba Cloud.
+ [Qwen2](https://github.com/QwenLM/Qwen2) ![](https://img.shields.io/github/stars/QwenLM/Qwen2?style=social) : Qwen2 is the large language model series developed by Qwen team, Alibaba Cloud.

#### ChatGLM
+ [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) ![](https://img.shields.io/github/stars/THUDM/ChatGLM-6B?style=social) : ChatGLM-6B: An Open Bilingual Dialogue Language Model | å¼€æºåŒè¯­å¯¹è¯è¯­è¨€æ¨¡å‹ã€‚ ChatGLM-6B æ˜¯ä¸€ä¸ªå¼€æºçš„ã€æ”¯æŒä¸­è‹±åŒè¯­çš„å¯¹è¯è¯­è¨€æ¨¡å‹ï¼ŒåŸºäº [General Language Model (GLM)](https://github.com/THUDM/GLM) æ¶æ„ï¼Œå…·æœ‰ 62 äº¿å‚æ•°ã€‚ "GLM: General Language Model Pretraining with Autoregressive Blank Infilling". ([ACL 2022](https://aclanthology.org/2022.acl-long.26/)).  "GLM-130B: An Open Bilingual Pre-trained Model". ([ICLR 2023](https://openreview.net/forum?id=-Aw0rrrPUF)).
+ [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B) ![](https://img.shields.io/github/stars/THUDM/ChatGLM2-6B?style=social) : ChatGLM2-6B: An Open Bilingual Chat LLM | å¼€æºåŒè¯­å¯¹è¯è¯­è¨€æ¨¡å‹ã€‚ChatGLM2-6B æ˜¯å¼€æºä¸­è‹±åŒè¯­å¯¹è¯æ¨¡å‹ ChatGLM-6B çš„ç¬¬äºŒä»£ç‰ˆæœ¬ï¼Œåœ¨ä¿ç•™äº†åˆä»£æ¨¡å‹å¯¹è¯æµç•…ã€éƒ¨ç½²é—¨æ§›è¾ƒä½ç­‰ä¼—å¤šä¼˜ç§€ç‰¹æ€§çš„åŸºç¡€ä¹‹ä¸Šï¼ŒChatGLM2-6B å¼•å…¥äº†æ›´å¼ºå¤§çš„æ€§èƒ½ã€æ›´å¼ºå¤§çš„æ€§èƒ½ã€æ›´é«˜æ•ˆçš„æ¨ç†ã€æ›´å¼€æ”¾çš„åè®®ã€‚
+ [ChatGLM3](https://github.com/THUDM/ChatGLM3) ![](https://img.shields.io/github/stars/THUDM/ChatGLM3?style=social) : ChatGLM3 series: Open Bilingual Chat LLMs | å¼€æºåŒè¯­å¯¹è¯è¯­è¨€æ¨¡å‹ã€‚

#### ä¹¦ç”Ÿæµ¦è¯­
+ [InternLMï¼ˆä¹¦ç”ŸÂ·æµ¦è¯­ï¼‰](https://github.com/InternLM/InternLM) ![](https://img.shields.io/github/stars/InternLM/InternLM?style=social) : Official release of InternLM2 7B and 20B base and chat models. 200K context support. [internlm.intern-ai.org.cn/](https://internlm.intern-ai.org.cn/)

#### ç™¾å·
+ [Baichuan-7Bï¼ˆç™¾å·-7Bï¼‰](https://github.com/baichuan-inc/Baichuan-7B) ![](https://img.shields.io/github/stars/baichuan-inc/Baichuan-7B?style=social) : A large-scale 7B pretraining language model developed by BaiChuan-Inc. Baichuan-7B æ˜¯ç”±ç™¾å·æ™ºèƒ½å¼€å‘çš„ä¸€ä¸ªå¼€æºå¯å•†ç”¨çš„å¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚åŸºäº Transformer ç»“æ„ï¼Œåœ¨å¤§çº¦ 1.2 ä¸‡äº¿ tokens ä¸Šè®­ç»ƒçš„ 70 äº¿å‚æ•°æ¨¡å‹ï¼Œæ”¯æŒä¸­è‹±åŒè¯­ï¼Œä¸Šä¸‹æ–‡çª—å£é•¿åº¦ä¸º 4096ã€‚åœ¨æ ‡å‡†çš„ä¸­æ–‡å’Œè‹±æ–‡ benchmarkï¼ˆC-Eval/MMLUï¼‰ä¸Šå‡å–å¾—åŒå°ºå¯¸æœ€å¥½çš„æ•ˆæœã€‚[huggingface.co/baichuan-inc/baichuan-7B](https://huggingface.co/baichuan-inc/Baichuan-7B)
+ [Baichuan-13Bï¼ˆç™¾å·-13Bï¼‰](https://github.com/baichuan-inc/Baichuan-13B) ![](https://img.shields.io/github/stars/baichuan-inc/Baichuan-13B?style=social) : A 13B large language model developed by Baichuan Intelligent Technology. Baichuan-13B æ˜¯ç”±ç™¾å·æ™ºèƒ½ç»§ Baichuan-7B ä¹‹åå¼€å‘çš„åŒ…å« 130 äº¿å‚æ•°çš„å¼€æºå¯å•†ç”¨çš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼Œåœ¨æƒå¨çš„ä¸­æ–‡å’Œè‹±æ–‡ benchmark ä¸Šå‡å–å¾—åŒå°ºå¯¸æœ€å¥½çš„æ•ˆæœã€‚æœ¬æ¬¡å‘å¸ƒåŒ…å«æœ‰é¢„è®­ç»ƒ (Baichuan-13B-Base) å’Œå¯¹é½ (Baichuan-13B-Chat) ä¸¤ä¸ªç‰ˆæœ¬ã€‚[huggingface.co/baichuan-inc/Baichuan-13B-Chat](https://huggingface.co/baichuan-inc/Baichuan-13B-Chat)
+ [Baichuan2](https://github.com/baichuan-inc/Baichuan2) ![](https://img.shields.io/github/stars/baichuan-inc/Baichuan2?style=social) : A series of large language models developed by Baichuan Intelligent Technology. Baichuan 2 æ˜¯ç™¾å·æ™ºèƒ½æ¨å‡ºçš„æ–°ä¸€ä»£å¼€æºå¤§è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨ 2.6 ä¸‡äº¿ Tokens çš„é«˜è´¨é‡è¯­æ–™è®­ç»ƒã€‚Baichuan 2 åœ¨å¤šä¸ªæƒå¨çš„ä¸­æ–‡ã€è‹±æ–‡å’Œå¤šè¯­è¨€çš„é€šç”¨ã€é¢†åŸŸ benchmark ä¸Šå–å¾—åŒå°ºå¯¸æœ€ä½³çš„æ•ˆæœã€‚æœ¬æ¬¡å‘å¸ƒåŒ…å«æœ‰ 7Bã€13B çš„ Base å’Œ Chat ç‰ˆæœ¬ï¼Œå¹¶æä¾›äº† Chat ç‰ˆæœ¬çš„ 4bits é‡åŒ–ã€‚[huggingface.co/baichuan-inc](https://huggingface.co/baichuan-inc). "Baichuan 2: Open Large-scale Language Models". ([arXiv 2023](https://arxiv.org/abs/2309.10305)).

#### æ–‡å¿ƒä¸€è¨€
+ [ç™¾åº¦-æ–‡å¿ƒå¤§æ¨¡å‹](https://wenxin.baidu.com/) : ç™¾åº¦å…¨æ–°ä¸€ä»£çŸ¥è¯†å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ï¼Œæ–‡å¿ƒå¤§æ¨¡å‹å®¶æ—çš„æ–°æˆå‘˜ï¼Œèƒ½å¤Ÿä¸äººå¯¹è¯äº’åŠ¨ï¼Œå›ç­”é—®é¢˜ï¼ŒååŠ©åˆ›ä½œï¼Œé«˜æ•ˆä¾¿æ·åœ°å¸®åŠ©äººä»¬è·å–ä¿¡æ¯ã€çŸ¥è¯†å’Œçµæ„Ÿã€‚
+ [ç™¾åº¦æ™ºèƒ½äº‘-åƒå¸†å¤§æ¨¡å‹](https://cloud.baidu.com/product/wenxinworkshop) : ç™¾åº¦æ™ºèƒ½äº‘åƒå¸†å¤§æ¨¡å‹å¹³å°ä¸€ç«™å¼ä¼ä¸šçº§å¤§æ¨¡å‹å¹³å°ï¼Œæä¾›å…ˆè¿›çš„ç”Ÿæˆå¼AIç”Ÿäº§åŠåº”ç”¨å…¨æµç¨‹å¼€å‘å·¥å…·é“¾ã€‚

#### ç›˜å¤å¤§æ¨¡å‹
+ [åä¸ºäº‘-ç›˜å¤å¤§æ¨¡å‹](https://www.huaweicloud.com/product/pangu.html) : ç›˜å¤å¤§æ¨¡å‹è‡´åŠ›äºæ·±è€•è¡Œä¸šï¼Œæ‰“é€ é‡‘èã€æ”¿åŠ¡ã€åˆ¶é€ ã€çŸ¿å±±ã€æ°”è±¡ã€é“è·¯ç­‰é¢†åŸŸè¡Œä¸šå¤§æ¨¡å‹å’Œèƒ½åŠ›é›†ï¼Œå°†è¡Œä¸šçŸ¥è¯†know-howä¸å¤§æ¨¡å‹èƒ½åŠ›ç›¸ç»“åˆï¼Œé‡å¡‘åƒè¡Œç™¾ä¸šï¼Œæˆä¸ºå„ç»„ç»‡ã€ä¼ä¸šã€ä¸ªäººçš„ä¸“å®¶åŠ©æ‰‹ã€‚"Accurate medium-range global weather forecasting with 3D neural networks". ([Nature 2023](https://www.nature.com/articles/s41586-023-06185-3)).

#### æ—¥æ—¥æ–°
+ [å•†æ±¤ç§‘æŠ€-æ—¥æ—¥æ–°SenseNova](https://techday.sensetime.com/?utm_source=baidu-sem-pc&utm_medium=cpc&utm_campaign=PC-%E6%8A%80%E6%9C%AF%E4%BA%A4%E6%B5%81%E6%97%A5-%E4%BA%A7%E5%93%81%E8%AF%8D-%E6%97%A5%E6%97%A5%E6%96%B0&utm_content=%E6%97%A5%E6%97%A5%E6%96%B0&utm_term=%E6%97%A5%E6%97%A5%E6%96%B0SenseNova&e_creative=73937788324&e_keywordid=594802524403) : æ—¥æ—¥æ–°ï¼ˆSenseNovaï¼‰ï¼Œæ˜¯å•†æ±¤ç§‘æŠ€å®£å¸ƒæ¨å‡ºçš„å¤§æ¨¡å‹ä½“ç³»ï¼ŒåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹â€œå•†é‡â€ï¼ˆSenseChatï¼‰ã€æ–‡ç”Ÿå›¾æ¨¡å‹â€œç§’ç”»â€å’Œæ•°å­—äººè§†é¢‘ç”Ÿæˆå¹³å°â€œå¦‚å½±â€ï¼ˆSenseAvatarï¼‰ç­‰ã€‚

#### æ˜Ÿç«å¤§æ¨¡å‹
+ [ç§‘å¤§è®¯é£-æ˜Ÿç«è®¤çŸ¥å¤§æ¨¡å‹](https://xinghuo.xfyun.cn/) : æ–°ä¸€ä»£è®¤çŸ¥æ™ºèƒ½å¤§æ¨¡å‹ï¼Œæ‹¥æœ‰è·¨é¢†åŸŸçŸ¥è¯†å’Œè¯­è¨€ç†è§£èƒ½åŠ›ï¼Œèƒ½å¤ŸåŸºäºè‡ªç„¶å¯¹è¯æ–¹å¼ç†è§£ä¸æ‰§è¡Œä»»åŠ¡ã€‚

#### è±†åŒ…
+ [å­—èŠ‚è·³åŠ¨-è±†åŒ…](https://www.doubao.com/) : è±†åŒ…ã€‚

#### å…¶ä»–
+ [Gemma](https://github.com/google/gemma_pytorch) ![](https://img.shields.io/github/stars/google/gemma_pytorch?style=social) : The official PyTorch implementation of Google's Gemma models. [ai.google.dev/gemma](https://ai.google.dev/gemma)
+ [Grok-1](https://github.com/xai-org/grok-1) ![](https://img.shields.io/github/stars/xai-org/grok-1?style=social) : This repository contains JAX example code for loading and running the Grok-1 open-weights model.
+ [Whisper](https://github.com/openai/whisper) ![](https://img.shields.io/github/stars/openai/whisper?style=social) : Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multitasking model that can perform multilingual speech recognition, speech translation, and language identification. "Robust Speech Recognition via Large-Scale Weak Supervision". ([arXiv 2022](https://arxiv.org/abs/2212.04356)).
+ [OpenChat](https://github.com/imoneoi/openchat) ![](https://img.shields.io/github/stars/imoneoi/openchat?style=social) : OpenChat: Advancing Open-source Language Models with Imperfect Data. [huggingface.co/openchat/openchat](https://huggingface.co/openchat/openchat)
+ [StableLM](https://github.com/Stability-AI/StableLM) ![](https://img.shields.io/github/stars/Stability-AI/StableLM?style=social) : StableLM: Stability AI Language Models.
+ [JARVIS](https://github.com/microsoft/JARVIS) ![](https://img.shields.io/github/stars/microsoft/JARVIS?style=social) : JARVIS, a system to connect LLMs with ML community. "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace". ([arXiv 2023](https://arxiv.org/abs/2303.17580)).
+ [Dolly](https://github.com/databrickslabs/dolly) ![](https://img.shields.io/github/stars/databrickslabs/dolly?style=social) : Databricksâ€™ Dolly, a large language model trained on the Databricks Machine Learning Platform. [Hello Dolly: Democratizing the magic of ChatGPT with open models](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html)
+ [LMFlow](https://github.com/OptimalScale/LMFlow) ![](https://img.shields.io/github/stars/OptimalScale/LMFlow?style=social) : An extensible, convenient, and efficient toolbox for finetuning large machine learning models, designed to be user-friendly, speedy and reliable, and accessible to the entire community. Large Language Model for All. [optimalscale.github.io/LMFlow/](https://optimalscale.github.io/LMFlow/)
+ [Colossal-AI](https://github.com/hpcaitech/ColossalAI) ![](https://img.shields.io/github/stars/hpcaitech/ColossalAI?style=social) : Making big AI models cheaper, easier, and scalable. [www.colossalai.org](www.colossalai.org). "Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training". ([arXiv 2021](https://arxiv.org/abs/2110.14883)).
+ [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) ![](https://img.shields.io/github/stars/tatsu-lab/stanford_alpaca?style=social) : Stanford Alpaca: An Instruction-following LLaMA Model.
+ [feizc/Visual-LLaMA](https://github.com/feizc/Visual-LLaMA) ![](https://img.shields.io/github/stars/feizc/Visual-LLaMA?style=social) : Open LLaMA Eyes to See the World. This project aims to optimize LLaMA model for visual information understanding like GPT-4 and further explore the potentional of large language model.
+ [Lightning-AI/lightning-colossalai](https://github.com/Lightning-AI/lightning-colossalai) ![](https://img.shields.io/github/stars/Lightning-AI/lightning-colossalai?style=social) : Efficient Large-Scale Distributed Training with [Colossal-AI](https://colossalai.org/) and [Lightning AI](https://lightning.ai/).
+ [ChatALL](https://github.com/sunner/ChatALL) ![](https://img.shields.io/github/stars/sunner/ChatALL?style=social) :  Concurrently chat with ChatGPT, Bing Chat, bard, Alpaca, Vincuna, Claude, ChatGLM, MOSS, iFlytek Spark, ERNIE and more, discover the best answers. [chatall.ai](http://chatall.ai/)
+ [FreedomIntelligence/LLMZoo](https://github.com/FreedomIntelligence/LLMZoo) ![](https://img.shields.io/github/stars/FreedomIntelligence/LLMZoo?style=social) : âš¡LLM Zoo is a project that provides data, models, and evaluation benchmark for large language models.âš¡ [Tech Report](https://github.com/FreedomIntelligence/LLMZoo/blob/main/assets/llmzoo.pdf)
+ [shm007g/LLaMA-Cult-and-More](https://github.com/shm007g/LLaMA-Cult-and-More) ![](https://img.shields.io/github/stars/shm007g/LLaMA-Cult-and-More?style=social) : News about ğŸ¦™ Cult and other AIGC models.
+ [X-PLUG/mPLUG-Owl](https://github.com/X-PLUG/mPLUG-Owl) ![](https://img.shields.io/github/stars/X-PLUG/mPLUG-Owl?style=social) : mPLUG-OwlğŸ¦‰: Modularization Empowers Large Language Models with Multimodality.
+ [i-Code](https://github.com/microsoft/i-Code) ![](https://img.shields.io/github/stars/microsoft/i-Code?style=social) : The ambition of the i-Code project is to build integrative and composable multimodal Artificial Intelligence. The "i" stands for integrative multimodal learning. "CoDi: Any-to-Any Generation via Composable Diffusion". ([arXiv 2023](https://arxiv.org/abs/2305.11846)).
+ [Lamini](https://github.com/lamini-ai/lamini) ![](https://img.shields.io/github/stars/lamini-ai/lamini?style=social) : Lamini: The LLM engine for rapidly customizing models ğŸ¦™
+ [xorbitsai/inference](https://github.com/xorbitsai/inference) ![](https://img.shields.io/github/stars/xorbitsai/inference?style=social) : Xorbits Inference (Xinference) is a powerful and versatile library designed to serve LLMs, speech recognition models, and multimodal models, even on your laptop. It supports a variety of models compatible with GGML, such as llama, chatglm, baichuan, whisper, vicuna, orac, and many others.
+ [epfLLM/Megatron-LLM](https://github.com/epfLLM/Megatron-LLM) ![](https://img.shields.io/github/stars/epfLLM/Megatron-LLM?style=social) : distributed trainer for LLMs.
+ [AmineDiro/cria](https://github.com/AmineDiro/cria) ![](https://img.shields.io/github/stars/AmineDiro/cria?style=social) : OpenAI compatible API for serving LLAMA-2 model.
+ [MOSS](https://github.com/OpenLMLab/MOSS) ![](https://img.shields.io/github/stars/OpenLMLab/MOSS?style=social) : An open-source tool-augmented conversational language model from Fudan University. MOSSæ˜¯ä¸€ä¸ªæ”¯æŒä¸­è‹±åŒè¯­å’Œå¤šç§æ’ä»¶çš„å¼€æºå¯¹è¯è¯­è¨€æ¨¡å‹ï¼Œmoss-moonç³»åˆ—æ¨¡å‹å…·æœ‰160äº¿å‚æ•°ï¼Œåœ¨FP16ç²¾åº¦ä¸‹å¯åœ¨å•å¼ A100/A800æˆ–ä¸¤å¼ 3090æ˜¾å¡è¿è¡Œï¼Œåœ¨INT4/8ç²¾åº¦ä¸‹å¯åœ¨å•å¼ 3090æ˜¾å¡è¿è¡Œã€‚MOSSåŸºåº§è¯­è¨€æ¨¡å‹åœ¨çº¦ä¸ƒåƒäº¿ä¸­è‹±æ–‡ä»¥åŠä»£ç å•è¯ä¸Šé¢„è®­ç»ƒå¾—åˆ°ï¼Œåç»­ç»è¿‡å¯¹è¯æŒ‡ä»¤å¾®è°ƒã€æ’ä»¶å¢å¼ºå­¦ä¹ å’Œäººç±»åå¥½è®­ç»ƒå…·å¤‡å¤šè½®å¯¹è¯èƒ½åŠ›åŠä½¿ç”¨å¤šç§æ’ä»¶çš„èƒ½åŠ›ã€‚[txsun1997.github.io/blogs/moss.html](https://txsun1997.github.io/blogs/moss.html)
+ [BayLingï¼ˆç™¾è†ï¼‰](https://github.com/ictnlp/BayLing) ![](https://img.shields.io/github/stars/OpenLMLab/MOSS?style=social) : â€œç™¾è†â€æ˜¯ä¸€ä¸ªå…·æœ‰å¢å¼ºçš„è¯­è¨€å¯¹é½çš„è‹±è¯­/ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰ä¼˜è¶Šçš„è‹±è¯­/ä¸­æ–‡èƒ½åŠ›ï¼Œåœ¨å¤šé¡¹æµ‹è¯•ä¸­å–å¾—ChatGPT 90%çš„æ€§èƒ½ã€‚BayLing is an English/Chinese LLM equipped with advanced language alignment, showing superior capability in English/Chinese generation, instruction following and multi-turn interaction. [nlp.ict.ac.cn/bayling](http://nlp.ict.ac.cn/bayling). "BayLing: Bridging Cross-lingual Alignment and Instruction Following through Interactive Translation for Large Language Models". ([arXiv 2023](https://arxiv.org/abs/2306.10968)).
+ [FlagAIï¼ˆæ‚Ÿé“Â·å¤©é¹°ï¼ˆAquilaï¼‰ï¼‰](https://github.com/FlagAI-Open/FlagAI) ![](https://img.shields.io/github/stars/FlagAI-Open/FlagAI?style=social) : FlagAI (Fast LArge-scale General AI models) is a fast, easy-to-use and extensible toolkit for large-scale model. Our goal is to support training, fine-tuning, and deployment of large-scale models on various downstream tasks with multi-modality.
+ [YuLan-Chatï¼ˆç‰å…°ï¼‰](https://github.com/RUC-GSAI/YuLan-Chat/) ![](https://img.shields.io/github/stars/RUC-GSAI/YuLan-Chat?style=social) : YuLan-Chat models are chat-based large language models, which are developed by the researchers in GSAI, Renmin University of China (YuLan, which represents Yulan Magnolia, is the campus flower of Renmin University of China). The newest version is developed by continually-pretraining and instruction-tuning [LLaMA-2](https://github.com/facebookresearch/llama) with high-quality English and Chinese data. YuLan-Chatç³»åˆ—æ¨¡å‹æ˜¯ä¸­å›½äººæ°‘å¤§å­¦é«˜ç“´äººå·¥æ™ºèƒ½å­¦é™¢å¸ˆç”Ÿå…±åŒå¼€å‘çš„æ”¯æŒèŠå¤©çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆåå­—"ç‰å…°"å–è‡ªä¸­å›½äººæ°‘å¤§å­¦æ ¡èŠ±ï¼‰ã€‚ æœ€æ–°ç‰ˆæœ¬åŸºäºLLaMA-2è¿›è¡Œäº†ä¸­è‹±æ–‡åŒè¯­çš„ç»§ç»­é¢„è®­ç»ƒå’ŒæŒ‡ä»¤å¾®è°ƒã€‚
+ [Yi-1.5](https://github.com/01-ai/Yi-1.5) ![](https://img.shields.io/github/stars/01-ai/Yi-1.5?style=social) : Yi-1.5 is an upgraded version of Yi, delivering stronger performance in coding, math, reasoning, and instruction-following capability.
+ [æ™ºæµ·-å½•é—®](https://github.com/zhihaiLLM/wisdomInterrogatory) ![](https://img.shields.io/github/stars/zhihaiLLM/wisdomInterrogatory?style=social) : æ™ºæµ·-å½•é—®(wisdomInterrogatory)æ˜¯ç”±æµ™æ±Ÿå¤§å­¦ã€é˜¿é‡Œå·´å·´è¾¾æ‘©é™¢ä»¥åŠåé™¢è®¡ç®—ä¸‰å®¶å•ä½å…±åŒè®¾è®¡ç ”å‘çš„æ³•å¾‹å¤§æ¨¡å‹ã€‚æ ¸å¿ƒæ€æƒ³ï¼šä»¥â€œæ™®æ³•å…±äº«å’Œå¸æ³•æ•ˆèƒ½æå‡â€ä¸ºç›®æ ‡ï¼Œä»æ¨åŠ¨æ³•å¾‹æ™ºèƒ½åŒ–ä½“ç³»å…¥å¸æ³•å®è·µã€æ•°å­—åŒ–æ¡ˆä¾‹å»ºè®¾ã€è™šæ‹Ÿæ³•å¾‹å’¨è¯¢æœåŠ¡èµ‹èƒ½ç­‰æ–¹é¢æä¾›æ”¯æŒï¼Œå½¢æˆæ•°å­—åŒ–å’Œæ™ºèƒ½åŒ–çš„å¸æ³•åŸºåº§èƒ½åŠ›ã€‚
+ [æ´»å­—](https://github.com/HIT-SCIR/huozi) ![](https://img.shields.io/github/stars/HIT-SCIR/huozi?style=social) : æ´»å­—æ˜¯ç”±å“ˆå·¥å¤§è‡ªç„¶è¯­è¨€å¤„ç†ç ”ç©¶æ‰€å¤šä½è€å¸ˆå’Œå­¦ç”Ÿå‚ä¸å¼€å‘çš„ä¸€ä¸ªå¼€æºå¯å•†ç”¨çš„å¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚ è¯¥æ¨¡å‹åŸºäº Bloom ç»“æ„çš„70 äº¿å‚æ•°æ¨¡å‹ï¼Œæ”¯æŒä¸­è‹±åŒè¯­ï¼Œä¸Šä¸‹æ–‡çª—å£é•¿åº¦ä¸º 2048ã€‚ åœ¨æ ‡å‡†çš„ä¸­æ–‡å’Œè‹±æ–‡åŸºå‡†ä»¥åŠä¸»è§‚è¯„æµ‹ä¸Šå‡å–å¾—åŒå°ºå¯¸ä¸­ä¼˜å¼‚çš„ç»“æœã€‚
+ [MiLM-6B](https://github.com/XiaoMi/MiLM-6B) ![](https://img.shields.io/github/stars/XiaoMi/MiLM-6B?style=social) : MiLM-6B æ˜¯ç”±å°ç±³å¼€å‘çš„ä¸€ä¸ªå¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œå‚æ•°è§„æ¨¡ä¸º64äº¿ã€‚åœ¨ C-Eval å’Œ CMMLU ä¸Šå‡å–å¾—åŒå°ºå¯¸æœ€å¥½çš„æ•ˆæœã€‚
+ [CPM-Bee](https://github.com/OpenBMB/CPM-Bee) ![](https://img.shields.io/github/stars/OpenBMB/CPM-Bee?style=social) : CPM-Beeæ˜¯ä¸€ä¸ªå®Œå…¨å¼€æºã€å…è®¸å•†ç”¨çš„ç™¾äº¿å‚æ•°ä¸­è‹±æ–‡åŸºåº§æ¨¡å‹ï¼Œä¹Ÿæ˜¯[CPM-Live](https://live.openbmb.org/)è®­ç»ƒçš„ç¬¬äºŒä¸ªé‡Œç¨‹ç¢‘ã€‚
+ [PandaLM](https://github.com/WeOpenML/PandaLM) ![](https://img.shields.io/github/stars/WeOpenML/PandaLM?style=social) : PandaLM: Reproducible and Automated Language Model Assessment.
+ [Chinese-Tiny-LLM](https://github.com/Chinese-Tiny-LLM/Chinese-Tiny-LLM) ![](https://img.shields.io/github/stars/Chinese-Tiny-LLM/Chinese-Tiny-LLM?style=social) : "Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model". ([arXiv 2024](https://arxiv.org/abs/2404.04167)).
+ [æ½˜å¤šæ‹‰ (Pandora)](https://github.com/pengzhile/pandora) ![](https://img.shields.io/github/stars/pengzhile/pandora?style=social) : æ½˜å¤šæ‹‰ï¼Œä¸€ä¸ªè®©ä½ å‘¼å¸é¡ºç•…çš„ChatGPTã€‚Pandora, a ChatGPT that helps you breathe smoothly.

### è§†è§‰å¤§æ¨¡å‹ï¼ˆVFMï¼‰Vision Foundation Model
+ [Visual ChatGPT](https://github.com/microsoft/visual-chatgpt) ![](https://img.shields.io/github/stars/microsoft/visual-chatgpt?style=social) : Visual ChatGPT connects ChatGPT and a series of Visual Foundation Models to enable sending and receiving images during chatting. "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models". ([arXiv 2023](https://arxiv.org/abs/2303.04671)).
+ [InternImage](https://github.com/OpenGVLab/InternImage) ![](https://img.shields.io/github/stars/OpenGVLab/InternImage?style=social) : "InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions". ([CVPR 2023](https://arxiv.org/abs/2211.05778)).
+ [GLIP](https://github.com/microsoft/GLIP) ![](https://img.shields.io/github/stars/microsoft/GLIP?style=social) : "Grounded Language-Image Pre-training". ([CVPR 2022](https://arxiv.org/abs/2112.03857)).
+ [GLIPv2](https://github.com/microsoft/GLIP) ![](https://img.shields.io/github/stars/microsoft/GLIP?style=social) : "GLIPv2: Unifying Localization and Vision-Language Understanding". ([arXiv 2022](https://arxiv.org/abs/2206.05836)).
+ [DINO](https://github.com/IDEA-Research/DINO) ![](https://img.shields.io/github/stars/IDEA-Research/DINO?style=social) : "DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection". ([ICLR 2023](https://arxiv.org/abs/2203.03605)).
+ [DINOv2](https://github.com/facebookresearch/dinov2) ![](https://img.shields.io/github/stars/facebookresearch/dinov2?style=social) : "DINOv2: Learning Robust Visual Features without Supervision". ([arXiv 2023](https://arxiv.org/abs/2304.07193)).
+ [Grounding DINO](https://github.com/IDEA-Research/GroundingDINO) ![](https://img.shields.io/github/stars/IDEA-Research/GroundingDINO?style=social) : "Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection". ([arXiv 2023](https://arxiv.org/abs/2303.05499)). "çŸ¥ä¹ã€Œä¸‰åˆ†é’Ÿçƒ­åº¦ã€ã€Š[ååˆ†é’Ÿè§£è¯»Grounding DINO-æ ¹æ®æ–‡å­—æç¤ºæ£€æµ‹ä»»æ„ç›®æ ‡](https://zhuanlan.zhihu.com/p/627646794)ã€‹"ã€‚
+ [SAM](https://github.com/facebookresearch/segment-anything) ![](https://img.shields.io/github/stars/facebookresearch/segment-anything?style=social) : The repository provides code for running inference with the Segment Anything Model (SAM), links for downloading the trained model checkpoints, and example notebooks that show how to use the model. "Segment Anything". ([arXiv 2023](https://arxiv.org/abs/2304.02643)).
+ [Grounded-SAM](https://github.com/IDEA-Research/Grounded-Segment-Anything) ![](https://img.shields.io/github/stars/IDEA-Research/Grounded-Segment-Anything?style=social) : Marrying Grounding DINO with Segment Anything & Stable Diffusion & Tag2Text & BLIP & Whisper & ChatBot - Automatically Detect , Segment and Generate Anything with Image, Text, and Audio Inputs. We plan to create a very interesting demo by combining [Grounding DINO](https://github.com/IDEA-Research/GroundingDINO) and [Segment Anything](https://github.com/facebookresearch/segment-anything) which aims to detect and segment Anything with text inputs!
+ [SEEM](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once) ![](https://img.shields.io/github/stars/UX-Decoder/Segment-Everything-Everywhere-All-At-Once?style=social) : We introduce SEEM that can Segment Everything Everywhere with Multi-modal prompts all at once. SEEM allows users to easily segment an image using prompts of different types including visual prompts (points, marks, boxes, scribbles and image segments) and language prompts (text and audio), etc. It can also work with any combinations of prompts or generalize to custom prompts! "Segment Everything Everywhere All at Once". ([arXiv 2023](https://arxiv.org/abs/2304.06718)).
+ [SAM3D](https://github.com/DYZhang09/SAM3D) ![](https://img.shields.io/github/stars/DYZhang09/SAM3D?style=social) : "SAM3D: Zero-Shot 3D Object Detection via [Segment Anything](https://github.com/facebookresearch/segment-anything) Model". ([arXiv 2023](https://arxiv.org/abs/2306.02245)).
+ [ImageBind](https://github.com/facebookresearch/ImageBind) ![](https://img.shields.io/github/stars/facebookresearch/ImageBind?style=social) : "ImageBind: One Embedding Space To Bind Them All". ([CVPR 2023](https://arxiv.org/abs/2305.05665)).
+ [Track-Anything](https://github.com/gaomingqi/Track-Anything) ![](https://img.shields.io/github/stars/gaomingqi/Track-Anything?style=social) : Track-Anything is a flexible and interactive tool for video object tracking and segmentation, based on Segment Anything, XMem, and E2FGVI. "Track Anything: Segment Anything Meets Videos". ([arXiv 2023](https://arxiv.org/abs/2304.11968)).
+ [qianqianwang68/omnimotion](https://github.com/qianqianwang68/omnimotion) ![](https://img.shields.io/github/stars/qianqianwang68/omnimotion?style=social) : "Tracking Everything Everywhere All at Once". ([arXiv 2023](https://arxiv.org/abs/2306.05422)).
+ [LLaVA](https://github.com/haotian-liu/LLaVA) ![](https://img.shields.io/github/stars/haotian-liu/LLaVA?style=social) : ğŸŒ‹ LLaVA: Large Language and Vision Assistant. Visual instruction tuning towards large language and vision models with GPT-4 level capabilities. [llava.hliu.cc](https://llava.hliu.cc/). "Visual Instruction Tuning". ([arXiv 2023](https://arxiv.org/abs/2304.08485)).
+ [M3I-Pretraining](https://github.com/OpenGVLab/M3I-Pretraining) ![](https://img.shields.io/github/stars/OpenGVLab/M3I-Pretraining?style=social) : "Towards All-in-one Pre-training via Maximizing Multi-modal Mutual Information". ([arXiv 2022](https://arxiv.org/abs/2211.09807)).
+ [BEVFormer](https://github.com/fundamentalvision/BEVFormer) ![](https://img.shields.io/github/stars/fundamentalvision/BEVFormer?style=social) : BEVFormer: a Cutting-edge Baseline for Camera-based Detection. "BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers". ([arXiv 2022](https://arxiv.org/abs/2203.17270)).
+ [Uni-Perceiver](https://github.com/fundamentalvision/Uni-Perceiver) ![](https://img.shields.io/github/stars/fundamentalvision/Uni-Perceiver?style=social) : "Uni-Perceiver: Pre-training Unified Architecture for Generic Perception for Zero-shot and Few-shot Tasks". ([CVPR 2022](https://openaccess.thecvf.com/content/CVPR2022/html/Zhu_Uni-Perceiver_Pre-Training_Unified_Architecture_for_Generic_Perception_for_Zero-Shot_and_CVPR_2022_paper.html)).
+ [AnyLabeling](https://github.com/vietanhdev/anylabeling) ![](https://img.shields.io/github/stars/vietanhdev/anylabeling?style=social) : ğŸŒŸ AnyLabeling ğŸŒŸ. Effortless data labeling with AI support from YOLO and Segment Anything! Effortless data labeling with AI support from YOLO and Segment Anything!
+ [X-AnyLabeling](https://github.com/CVHub520/X-AnyLabeling) ![](https://img.shields.io/github/stars/CVHub520/X-AnyLabeling?style=social) : ğŸ’« X-AnyLabeling ğŸ’«. Effortless data labeling with AI support from Segment Anything and other awesome models!
+ [Label Anything](https://github.com/open-mmlab/playground/tree/main/label_anything) ![](https://img.shields.io/github/stars/open-mmlab/playground?style=social) : OpenMMLab PlayGround: Semi-Automated Annotation with Label-Studio and SAM.
+ [RevCol](https://github.com/megvii-research/RevCol) ![](https://img.shields.io/github/stars/megvii-research/RevCol?style=social) : "Reversible Column Networks". ([arXiv 2023](https://arxiv.org/abs/2212.11696)).
+ [Macaw-LLM](https://github.com/lyuchenyang/Macaw-LLM) ![](https://img.shields.io/github/stars/lyuchenyang/Macaw-LLM?style=social) : Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration.
+ [SAM-PT](https://github.com/SysCV/sam-pt) ![](https://img.shields.io/github/stars/SysCV/sam-pt?style=social) : SAM-PT: Extending SAM to zero-shot video segmentation with point-based tracking. "Segment Anything Meets Point Tracking". ([arXiv 2023](https://arxiv.org/abs/2307.01197)).
+ [Video-LLaMA](https://github.com/DAMO-NLP-SG/Video-LLaMA) ![](https://img.shields.io/github/stars/DAMO-NLP-SG/Video-LLaMA?style=social) : "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding". ([arXiv 2023](https://arxiv.org/abs/2306.02858)).
+ [MobileSAM](https://github.com/ChaoningZhang/MobileSAM) ![](https://img.shields.io/github/stars/ChaoningZhang/MobileSAM?style=social) : "Faster Segment Anything: Towards Lightweight SAM for Mobile Applications". ([arXiv 2023](https://arxiv.org/abs/2306.14289)).
+ [BuboGPT](https://github.com/magic-research/bubogpt) ![](https://img.shields.io/github/stars/magic-research/bubogpt?style=social) : "BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs". ([arXiv 2023](https://arxiv.org/abs/2307.08581)).

### äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰AI Generated Content
+ [Sora](https://openai.com/sora) : Sora is an AI model that can create realistic and imaginative scenes from text instructions.
+ [Open Sora Plan](https://github.com/PKU-YuanGroup/Open-Sora-Plan) ![](https://img.shields.io/github/stars/PKU-YuanGroup/Open-Sora-Plan?style=social) : This project aim to reproducing [Sora](https://openai.com/sora) (Open AI T2V model), but we only have limited resource. We deeply wish the all open source community can contribute to this project. æœ¬é¡¹ç›®å¸Œæœ›é€šè¿‡å¼€æºç¤¾åŒºçš„åŠ›é‡å¤ç°Soraï¼Œç”±åŒ—å¤§-å…”å±•AIGCè”åˆå®éªŒå®¤å…±åŒå‘èµ·ï¼Œå½“å‰æˆ‘ä»¬èµ„æºæœ‰é™ä»…æ­å»ºäº†åŸºç¡€æ¶æ„ï¼Œæ— æ³•è¿›è¡Œå®Œæ•´è®­ç»ƒï¼Œå¸Œæœ›é€šè¿‡å¼€æºç¤¾åŒºé€æ­¥å¢åŠ æ¨¡å—å¹¶ç­¹é›†èµ„æºè¿›è¡Œè®­ç»ƒï¼Œå½“å‰ç‰ˆæœ¬ç¦»ç›®æ ‡å·®è·å·¨å¤§ï¼Œä»éœ€æŒç»­å®Œå–„å’Œå¿«é€Ÿè¿­ä»£ï¼Œæ¬¢è¿Pull requestï¼ï¼ï¼[Project Page](https://pku-yuangroup.github.io/Open-Sora-Plan/) [ä¸­æ–‡ä¸»é¡µ](https://pku-yuangroup.github.io/Open-Sora-Plan/blog_cn.html)
+ [Mini Sora](https://github.com/mini-sora/minisora) ![](https://img.shields.io/github/stars/mini-sora/minisora?style=social) : The Mini Sora project aims to explore the implementation path and future development direction of Sora.
+ [EMO](https://github.com/HumanAIGC/EMO) ![](https://img.shields.io/github/stars/HumanAIGC/EMO?style=social) : "EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions". ([arXiv 2024](https://arxiv.org/abs/2402.17485)).
+ [Stable Diffusion](https://github.com/CompVis/stable-diffusion) ![](https://img.shields.io/github/stars/CompVis/stable-diffusion?style=social) : Stable Diffusion is a latent text-to-image diffusion model. Stable Diffusion was made possible thanks to a collaboration with [Stability AI](https://stability.ai/) and [Runway](https://runwayml.com/) and builds upon our previous work "High-Resolution Image Synthesis with Latent Diffusion Models". ([CVPR 2022](https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html)).
+ [Stable Diffusion Version 2](https://github.com/Stability-AI/stablediffusion) ![](https://img.shields.io/github/stars/Stability-AI/stablediffusion?style=social) : This repository contains [Stable Diffusion](https://github.com/CompVis/stable-diffusion) models trained from scratch and will be continuously updated with new checkpoints. "High-Resolution Image Synthesis with Latent Diffusion Models". ([CVPR 2022](https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html)).
+ [StableStudio](https://github.com/Stability-AI/StableStudio) ![](https://img.shields.io/github/stars/Stability-AI/StableStudio?style=social) : StableStudio by [Stability AI](https://stability.ai/). ğŸ‘‹ Welcome to the community repository for StableStudio, the open-source version of [DreamStudio](https://dreamstudio.ai/).
+ [AudioCraft](https://github.com/facebookresearch/audiocraft) ![](https://img.shields.io/github/stars/facebookresearch/audiocraft?style=social) : Audiocraft is a library for audio processing and generation with deep learning. It features the state-of-the-art EnCodec audio compressor / tokenizer, along with MusicGen, a simple and controllable music generation LM with textual and melodic conditioning.
+ [InvokeAI](https://github.com/invoke-ai/InvokeAI) ![](https://img.shields.io/github/stars/invoke-ai/InvokeAI?style=social) : Invoke AI - Generative AI for Professional Creatives. Professional Creative Tools for Stable Diffusion, Custom-Trained Models, and more. [invoke-ai.github.io/InvokeAI/](https://invoke-ai.github.io/InvokeAI/)
+ [DragGAN](https://github.com/XingangPan/DragGAN) ![](https://img.shields.io/github/stars/XingangPan/DragGAN?style=social) : "Stable Diffusion Training with MosaicML. This repo contains code used to train your own Stable Diffusion model on your own data". ([SIGGRAPH 2023](https://vcai.mpi-inf.mpg.de/projects/DragGAN/)).
+ [AudioGPT](https://github.com/AIGC-Audio/AudioGPT) ![](https://img.shields.io/github/stars/AIGC-Audio/AudioGPT?style=social) : AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head.
+ [PandasAI](https://github.com/gventuri/pandas-ai) ![](https://img.shields.io/github/stars/gventuri/pandas-ai?style=social) : Pandas AI is a Python library that adds generative artificial intelligence capabilities to Pandas, the popular data analysis and manipulation tool. It is designed to be used in conjunction with Pandas, and is not a replacement for it.
+ [mosaicml/diffusion](https://github.com/mosaicml/diffusion) ![](https://img.shields.io/github/stars/mosaicml/diffusion?style=social) : Stable Diffusion Training with MosaicML. This repo contains code used to train your own Stable Diffusion model on your own data.
+ [VisorGPT](https://github.com/Sierkinhane/VisorGPT) ![](https://img.shields.io/github/stars/Sierkinhane/VisorGPT?style=social) : Customize spatial layouts for conditional image synthesis models, e.g., ControlNet, using GPT. "VisorGPT: Learning Visual Prior via Generative Pre-Training". ([arXiv 2023](https://arxiv.org/abs/2305.13777)).
+ [ControlNet](https://github.com/lllyasviel/ControlNet) ![](https://img.shields.io/github/stars/lllyasviel/ControlNet?style=social) : Let us control diffusion models! "Adding Conditional Control to Text-to-Image Diffusion Models". ([arXiv 2023](https://arxiv.org/abs/2302.05543)).
+ [Fooocus](https://github.com/lllyasviel/Fooocus) ![](https://img.shields.io/github/stars/lllyasviel/Fooocus?style=social) : Fooocus is an image generating software. Fooocus is a rethinking of Stable Diffusion and Midjourneyâ€™s designs. "å¾®ä¿¡å…¬ä¼—å·ã€ŒGitHubStoreã€ã€Š[Fooocus : é›†Stable Diffusion å’Œ Midjourney ä¼˜ç‚¹äºä¸€èº«çš„å¼€æºAIç»˜å›¾è½¯ä»¶](https://mp.weixin.qq.com/s/adyXek6xcz5aOPAGqZBrvg)ã€‹"ã€‚
+ [MindDiffuser](https://github.com/ReedOnePeck/MindDiffuser) ![](https://img.shields.io/github/stars/ReedOnePeck/MindDiffuser?style=social) : "MindDiffuser: Controlled Image Reconstruction from Human Brain Activity with Semantic and Structural Diffusion". ([arXiv 2023](https://arxiv.org/abs/2308.04249)).



+ [Midjourney](https://www.midjourney.com/) : Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species.
+ [DreamStudio](https://dreamstudio.ai/) : Effortless image generation for creators with big dreams.
+ [Firefly](https://www.adobe.com/sensei/generative-ai/firefly.html) : Adobe Firefly: Experiment, imagine, and make an infinite range of creations with Firefly, a family of creative generative AI models coming to Adobe products.
+ [Jasper](https://www.jasper.ai/) : Meet Jasper. On-brand AI content wherever you create.
+ [Copy.ai](https://www.copy.ai/) : Whatever you want to ask, our chat has the answers.
+ [Peppertype.ai](https://www.peppercontent.io/peppertype-ai/) : Leverage the AI-powered platform to ideate, create, distribute, and measure your content and prove your content marketing ROI.
+ [ChatPPT](https://chat-ppt.com/) : ChatPPTæ¥è¢­å‘½ä»¤å¼ä¸€é”®ç”ŸæˆPPTã€‚

## åº”ç”¨ç¨‹åºå¼€å‘å¹³å° Application Development Platform
+ [LangChain](https://github.com/langchain-ai/langchain) ![](https://img.shields.io/github/stars/hwchase17/langchain?style=social) :  ğŸ¦œï¸ğŸ”— LangChain. âš¡ Building applications with LLMs through composability âš¡ [python.langchain.com](https://python.langchain.com/docs/get_started/introduction.html)
+ [Dify](https://github.com/langgenius/dify) ![](https://img.shields.io/github/stars/langgenius/dify?style=social) : An Open-Source Assistants API and GPTs alternative. Dify.AI is an LLM application development platform. It integrates the concepts of Backend as a Service and LLMOps, covering the core tech stack required for building generative AI-native applications, including a built-in RAG engine. [dify.ai](https://dify.ai/)
+ [AutoChain](https://github.com/Forethought-Technologies/AutoChain) ![](https://img.shields.io/github/stars/Forethought-Technologies/AutoChain?style=social) :  AutoChain: Build lightweight, extensible, and testable LLM Agents. [autochain.forethought.ai](https://autochain.forethought.ai/)
+ [Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT) ![](https://img.shields.io/github/stars/Significant-Gravitas/Auto-GPT?style=social) : Auto-GPT: An Autonomous GPT-4 Experiment. Auto-GPT is an experimental open-source application showcasing the capabilities of the GPT-4 language model. This program, driven by GPT-4, chains together LLM "thoughts", to autonomously achieve whatever goal you set. As one of the first examples of GPT-4 running fully autonomously, Auto-GPT pushes the boundaries of what is possible with AI. [agpt.co](https://news.agpt.co/)
+ [LiteChain](https://github.com/rogeriochaves/litechain) ![](https://img.shields.io/github/stars/rogeriochaves/litechain?style=social) : Build robust LLM applications with true composability ğŸ”—. [rogeriochaves.github.io/litechain/](https://rogeriochaves.github.io/litechain/)
+ [Open-Assistant](https://github.com/LAION-AI/Open-Assistant) ![](https://img.shields.io/github/stars/LAION-AI/Open-Assistant?style=social) : OpenAssistant is a chat-based assistant that understands tasks, can interact with third-party systems, and retrieve information dynamically to do so. [open-assistant.io](https://open-assistant.io/)

## å¾®è°ƒæ¡†æ¶ Fine-Tuning Framework 
+ [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) ![](https://img.shields.io/github/stars/hiyouga/LLaMA-Factory?style=social) : Unify Efficient Fine-Tuning of 100+ LLMs. Fine-tuning a large language model can be easy as...

## â‡ï¸æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ RAG Framework 
+ [LlamaIndex](https://github.com/run-llama/llama_index) ![](https://img.shields.io/github/stars/run-llama/llama_index?style=social) : <font style="background-color:#FBDE28;">LlamaIndex is a data framework for your LLM applications. </font>[docs.llamaindex.ai](https://docs.llamaindex.ai/)
+ [Embedchain](https://github.com/embedchain/embedchain) ![](https://img.shields.io/github/stars/embedchain/embedchain?style=social) : The Open Source RAG framework. [docs.embedchain.ai](https://docs.embedchain.ai/)
+ [QAnything](https://github.com/netease-youdao/QAnything) ![](https://img.shields.io/github/stars/netease-youdao/QAnything?style=social) : Question and Answer based on Anything. [qanything.ai](https://qanything.ai/)
+ [R2R](https://github.com/SciPhi-AI/R2R) ![](https://img.shields.io/github/stars/SciPhi-AI/R2R?style=social) : A framework for rapid development and deployment of production-ready RAG systems. [docs.sciphi.ai](https://docs.sciphi.ai/)
+ [langchain-ai/rag-from-scratch](https://github.com/langchain-ai/rag-from-scratch) ![](https://img.shields.io/github/stars/langchain-ai/rag-from-scratch?style=social) : Retrieval augmented generation (RAG) comes is a general methodology for connecting LLMs with external data sources. These notebooks accompany a video series will build up an understanding of RAG from scratch, starting with the basics of indexing, retrieval, and generation.

## å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ¡†æ¶ LLM Inference Framework 
### LLM Inference Benchmark æ¨ç†åŸºå‡†
+ [ninehills/llm-inference-benchmark](https://github.com/ninehills/llm-inference-benchmark) ![](https://img.shields.io/github/stars/ninehills/llm-inference-benchmark?style=social) : LLM Inference benchmark.
+ [csbench/csbench](https://github.com/csbench/csbench) ![](https://img.shields.io/github/stars/csbench/csbench?style=social) : "CS-Bench: A Comprehensive Benchmark for Large Language Models towards Computer Science Mastery". ([arXiv 2024](https://arxiv.org/abs/2406.08587)).

### LLM Deployment Engine éƒ¨ç½²ä¸é…ç½®å¼•æ“
+ [vllm-project/vllm](https://github.com/vllm-project/vllm) ![](https://img.shields.io/github/stars/vllm-project/vllm?style=social) : A high-throughput and memory-efficient inference and serving engine for LLMs. [vllm.readthedocs.io](https://vllm.readthedocs.io/en/latest/)
+ [MLC LLM](https://github.com/mlc-ai/mlc-llm) ![](https://img.shields.io/github/stars/mlc-ai/mlc-llm?style=social) : Enable everyone to develop, optimize and deploy AI models natively on everyone's devices. [mlc.ai/mlc-llm](https://mlc.ai/mlc-llm/)
+ [Lamini](https://github.com/lamini-ai/lamini) ![](https://img.shields.io/github/stars/lamini-ai/lamini?style=social) : Lamini: The LLM engine for rapidly customizing models ğŸ¦™.
+ [datawhalechina/self-llm](https://github.com/datawhalechina/self-llm) ![](https://img.shields.io/github/stars/datawhalechina/self-llm?style=social) :  ã€Šå¼€æºå¤§æ¨¡å‹é£Ÿç”¨æŒ‡å—ã€‹åŸºäºLinuxç¯å¢ƒå¿«é€Ÿéƒ¨ç½²å¼€æºå¤§æ¨¡å‹ï¼Œæ›´é€‚åˆä¸­å›½å®å®çš„éƒ¨ç½²æ•™ç¨‹ã€‚

### C Implementation
+ [llm.c](https://github.com/karpathy/llm.c) ![](https://img.shields.io/github/stars/karpathy/llm.c?style=social) : LLM training in simple, pure C/CUDA. There is no need for 245MB of PyTorch or 107MB of cPython. For example, training GPT-2 (CPU, fp32) is ~1,000 lines of clean code in a single file. It compiles and runs instantly, and exactly matches the PyTorch reference implementation.
+ [llama2.c](https://github.com/karpathy/llama2.c) ![](https://img.shields.io/github/stars/karpathy/llama2.c?style=social) : Inference Llama 2 in one file of pure C. Train the Llama 2 LLM architecture in PyTorch then inference it with one simple 700-line C file (run.c).

### CPP Implementation
+ [TensorRT](https://github.com/NVIDIA/TensorRT) ![](https://img.shields.io/github/stars/NVIDIA/TensorRT?style=social) : NVIDIAÂ® TensorRTâ„¢ is an SDK for high-performance deep learning inference on NVIDIA GPUs. This repository contains the open source components of TensorRT. [developer.nvidia.com/tensorrt](https://developer.nvidia.com/tensorrt)
+ [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) ![](https://img.shields.io/github/stars/NVIDIA/TensorRT-LLM?style=social) : TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that execute those TensorRT engines. [nvidia.github.io/TensorRT-LLM](https://nvidia.github.io/TensorRT-LLM)
+ [gemma.cpp](https://github.com/google/gemma.cpp) ![](https://img.shields.io/github/stars/google/gemma.cpp?style=social) :  gemma.cpp is a lightweight, standalone C++ inference engine for the Gemma foundation models from Google.
+ [llama.cpp](https://github.com/ggerganov/llama.cpp) ![](https://img.shields.io/github/stars/ggerganov/llama.cpp?style=social) : Inference of [LLaMA](https://github.com/facebookresearch/llama) model in pure C/C++.
+ [whisper.cpp](https://github.com/ggerganov/whisper.cpp) ![](https://img.shields.io/github/stars/ggerganov/whisper.cpp?style=social) : High-performance inference of [OpenAI's Whisper](https://github.com/openai/whisper) automatic speech recognition (ASR) model.
+ [ChatGLM.cpp](https://github.com/li-plus/chatglm.cpp) ![](https://img.shields.io/github/stars/li-plus/chatglm.cpp?style=social) : C++ implementation of [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) and [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B).
+ [MegEngine/InferLLM](https://github.com/MegEngine/InferLLM) ![](https://img.shields.io/github/stars/MegEngine/InferLLM?style=social) : InferLLM is a lightweight LLM model inference framework that mainly references and borrows from the llama.cpp project.
+ [DeployAI/nndeploy](https://github.com/DeployAI/nndeploy) ![](https://img.shields.io/github/stars/DeployAI/nndeploy?style=social) : nndeployæ˜¯ä¸€æ¬¾æ¨¡å‹ç«¯åˆ°ç«¯éƒ¨ç½²æ¡†æ¶ã€‚ä»¥å¤šç«¯æ¨ç†ä»¥åŠåŸºäºæœ‰å‘æ— ç¯å›¾æ¨¡å‹éƒ¨ç½²ä¸ºå†…æ ¸ï¼Œè‡´åŠ›ä¸ºç”¨æˆ·æä¾›è·¨å¹³å°ã€ç®€å•æ˜“ç”¨ã€é«˜æ€§èƒ½çš„æ¨¡å‹éƒ¨ç½²ä½“éªŒã€‚[nndeploy-zh.readthedocs.io/zh/latest/](https://nndeploy-zh.readthedocs.io/zh/latest/)
+ [zjhellofss/KuiperInfer (è‡ªåˆ¶æ·±åº¦å­¦ä¹ æ¨ç†æ¡†æ¶)](https://github.com/zjhellofss/KuiperInfer) ![](https://img.shields.io/github/stars/zjhellofss/KuiperInfer?style=social) :  å¸¦ä½ ä»é›¶å®ç°ä¸€ä¸ªé«˜æ€§èƒ½çš„æ·±åº¦å­¦ä¹ æ¨ç†åº“ï¼Œæ”¯æŒllama ã€Unetã€Yolov5ã€Resnetç­‰æ¨¡å‹çš„æ¨ç†ã€‚Implement a high-performance deep learning inference library step by step.
+ [skeskinen/llama-lite](https://github.com/skeskinen/llama-lite) ![](https://img.shields.io/github/stars/skeskinen/llama-lite?style=social) : Embeddings focused small version of Llama NLP model.
+ [Const-me/Whisper](https://github.com/Const-me/Whisper) ![](https://img.shields.io/github/stars/Const-me/Whisper?style=social) : High-performance GPGPU inference of OpenAI's Whisper automatic speech recognition (ASR) model.
+ [wangzhaode/ChatGLM-MNN](https://github.com/wangzhaode/ChatGLM-MNN) ![](https://img.shields.io/github/stars/wangzhaode/ChatGLM-MNN?style=social) : Pure C++, Easy Deploy ChatGLM-6B.
+ [ztxz16/fastllm](https://github.com/ztxz16/fastllm) ![](https://img.shields.io/github/stars/ztxz16/fastllm?style=social) : çº¯c++å®ç°ï¼Œæ— ç¬¬ä¸‰æ–¹ä¾èµ–çš„å¤§æ¨¡å‹åº“ï¼Œæ”¯æŒCUDAåŠ é€Ÿï¼Œç›®å‰æ”¯æŒå›½äº§å¤§æ¨¡å‹ChatGLM-6Bï¼ŒMOSS; å¯ä»¥åœ¨å®‰å“è®¾å¤‡ä¸Šæµç•…è¿è¡ŒChatGLM-6Bã€‚
+ [davidar/eigenGPT](https://github.com/davidar/eigenGPT) ![](https://img.shields.io/github/stars/davidar/eigenGPT?style=social) : Minimal C++ implementation of GPT2.
+ [Tlntin/Qwen-TensorRT-LLM](https://github.com/Tlntin/Qwen-TensorRT-LLM) ![](https://img.shields.io/github/stars/Tlntin/Qwen-TensorRT-LLM?style=social) : ä½¿ç”¨TRT-LLMå®Œæˆå¯¹Qwen-7B-Chatå®ç°æ¨ç†åŠ é€Ÿã€‚
+ [FeiGeChuanShu/trt2023](https://github.com/FeiGeChuanShu/trt2023) ![](https://img.shields.io/github/stars/FeiGeChuanShu/trt2023?style=social) : NVIDIA TensorRT Hackathon 2023å¤èµ›é€‰é¢˜ï¼šé€šä¹‰åƒé—®Qwen-7Bç”¨TensorRT-LLMæ¨¡å‹æ­å»ºåŠä¼˜åŒ–ã€‚
+ [TRT2022/trtllm-llama](https://github.com/TRT2022/trtllm-llama) ![](https://img.shields.io/github/stars/TRT2022/trtllm-llama?style=social) : â˜¢ï¸ TensorRT 2023å¤èµ›â€”â€”åŸºäºTensorRT-LLMçš„Llamaæ¨¡å‹æ¨æ–­åŠ é€Ÿä¼˜åŒ–ã€‚
+ [AmeyaWagh/llama2.cpp](https://github.com/AmeyaWagh/llama2.cpp) ![](https://img.shields.io/github/stars/AmeyaWagh/llama2.cpp?style=social) : Inference Llama 2 in C++.

### Python Implementation
+ [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) ![](https://img.shields.io/github/stars/abetlen/llama-cpp-python?style=social) : Python bindings for llama.cpp. [llama-cpp-python.readthedocs.io](https://llama-cpp-python.readthedocs.io/)
+ [ggml-python](https://github.com/abetlen/ggml-python) ![](https://img.shields.io/github/stars/abetlen/ggml-python?style=social) : Python bindings for ggml. [ggml-python.readthedocs.io](https://ggml-python.readthedocs.io/)

### Mojo Implementation
+ [llama2.mojo](https://github.com/tairov/llama2.mojo) ![](https://img.shields.io/github/stars/tairov/llama2.mojo?style=social) : Inference Llama 2 in one file of pure ğŸ”¥
+ [dorjeduck/llm.mojo](https://github.com/dorjeduck/llm.mojo) ![](https://img.shields.io/github/stars/dorjeduck/llm.mojo?style=social) : port of Andrjey Karpathy's llm.c to Mojo.

### Rust Implementation
+ [Candle](https://github.com/huggingface/candle) ![](https://img.shields.io/github/stars/huggingface/candle?style=social) : Minimalist ML framework for Rust.
+ [Safetensors](https://github.com/huggingface/safetensors) ![](https://img.shields.io/github/stars/huggingface/safetensors?style=social) : Simple, safe way to store and distribute tensors. [huggingface.co/docs/safetensors](https://huggingface.co/docs/safetensors/index)
+ [Tokenizers](https://github.com/huggingface/tokenizers) ![](https://img.shields.io/github/stars/huggingface/tokenizers?style=social) : ğŸ’¥ Fast State-of-the-Art Tokenizers optimized for Research and Production. [huggingface.co/docs/tokenizers](https://huggingface.co/docs/tokenizers/index)
+ [Burn](https://github.com/burn-rs/burn) ![](https://img.shields.io/github/stars/burn-rs/burn?style=social) : Burn - A Flexible and Comprehensive Deep Learning Framework in Rust. [burn-rs.github.io/](https://burn-rs.github.io/)
+ [dfdx](https://github.com/coreylowman/dfdx) ![](https://img.shields.io/github/stars/coreylowman/dfdx?style=social) : Deep learning in Rust, with shape checked tensors and neural networks.
+ [luminal](https://github.com/jafioti/luminal) ![](https://img.shields.io/github/stars/jafioti/luminal?style=social) : Deep learning at the speed of light. [www.luminalai.com/](https://www.luminalai.com/)
+ [crabml](https://github.com/crabml/crabml) ![](https://img.shields.io/github/stars/crabml/crabml?style=social) : crabml is focusing on the reimplementation of GGML using the Rust programming language.
+ [TensorFlow Rust](https://github.com/tensorflow/rust) ![](https://img.shields.io/github/stars/tensorflow/rust?style=social) : Rust language bindings for TensorFlow.
+ [tch-rs](https://github.com/LaurentMazare/tch-rs) ![](https://img.shields.io/github/stars/LaurentMazare/tch-rs?style=social) : Rust bindings for the C++ api of PyTorch.
+ [rustai-solutions/candle_demo_openchat_35](https://github.com/rustai-solutions/candle_demo_openchat_35) ![](https://img.shields.io/github/stars/rustai-solutions/candle_demo_openchat_35?style=social) : candle_demo_openchat_35.
+ [llama2.rs](https://github.com/srush/llama2.rs) ![](https://img.shields.io/github/stars/srush/llama2.rs?style=social) : A fast llama2 decoder in pure Rust.
+ [Llama2-burn](https://github.com/Gadersd/llama2-burn) ![](https://img.shields.io/github/stars/Gadersd/llama2-burn?style=social) : Llama2 LLM ported to Rust burn.
+ [gaxler/llama2.rs](https://github.com/gaxler/llama2.rs) ![](https://img.shields.io/github/stars/gaxler/llama2.rs?style=social) : Inference Llama 2 in one file of pure Rust ğŸ¦€
+ [whisper-burn](https://github.com/Gadersd/whisper-burn) ![](https://img.shields.io/github/stars/Gadersd/whisper-burn?style=social) : A Rust implementation of OpenAI's Whisper model using the burn framework.
+ [stable-diffusion-burn](https://github.com/Gadersd/stable-diffusion-burn) ![](https://img.shields.io/github/stars/Gadersd/stable-diffusion-burn?style=social) : Stable Diffusion v1.4 ported to Rust's burn framework.
+ [coreylowman/llama-dfdx](https://github.com/coreylowman/llama-dfdx) ![](https://img.shields.io/github/stars/coreylowman/llama-dfdx?style=social) : [LLaMa 7b](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) with CUDA acceleration implemented in rust. Minimal GPU memory needed!
+ [tazz4843/whisper-rs](https://github.com/tazz4843/whisper-rs) ![](https://img.shields.io/github/stars/tazz4843/whisper-rs?style=social) : Rust bindings to [whisper.cpp](https://github.com/ggerganov/whisper.cpp).
+ [rustformers/llm](https://github.com/rustformers/llm) ![](https://img.shields.io/github/stars/rustformers/llm?style=social) : Run inference for Large Language Models on CPU, with Rust ğŸ¦€ğŸš€ğŸ¦™.
+ [Chidori](https://github.com/ThousandBirdsInc/chidori) ![](https://img.shields.io/github/stars/ThousandBirdsInc/chidori?style=social) : A reactive runtime for building durable AI agents. [docs.thousandbirds.ai](https://docs.thousandbirds.ai/).
+ [llm-chain](https://github.com/sobelio/llm-chain) ![](https://img.shields.io/github/stars/sobelio/llm-chain?style=social) : llm-chain is a collection of Rust crates designed to help you work with Large Language Models (LLMs) more effectively. [llm-chain.xyz](https://llm-chain.xyz/)
+ [Abraxas-365/langchain-rust](https://github.com/Abraxas-365/langchain-rust) ![](https://img.shields.io/github/stars/Abraxas-365/langchain-rust?style=social) : ğŸ¦œï¸ğŸ”—LangChain for Rust, the easiest way to write LLM-based programs in Rust.
+ [Atome-FE/llama-node](https://github.com/Atome-FE/llama-node) ![](https://img.shields.io/github/stars/Atome-FE/llama-node?style=social) : Believe in AI democratization. llama for nodejs backed by llama-rs and llama.cpp, work locally on your laptop CPU. support llama/alpaca/gpt4all/vicuna model. [www.npmjs.com/package/llama-node](https://www.npmjs.com/package/llama-node)
+ [Noeda/rllama](https://github.com/Noeda/rllama) ![](https://img.shields.io/github/stars/Noeda/rllama?style=social) : Rust+OpenCL+AVX2 implementation of LLaMA inference code.
+ [lencx/ChatGPT](https://github.com/lencx/ChatGPT) ![](https://img.shields.io/github/stars/lencx/ChatGPT?style=social) : ğŸ”® ChatGPT Desktop Application (Mac, Windows and Linux). [NoFWL](https://app.nofwl.com/).
+ [Synaptrix/ChatGPT-Desktop](https://github.com/Synaptrix/ChatGPT-Desktop) ![](https://img.shields.io/github/stars/Synaptrix/ChatGPT-Desktop?style=social) : Fuel your productivity with ChatGPT-Desktop - Blazingly fast and supercharged!
+ [Poordeveloper/chatgpt-app](https://github.com/Poordeveloper/chatgpt-app) ![](https://img.shields.io/github/stars/Poordeveloper/chatgpt-app?style=social) : A ChatGPT App for all platforms. Built with Rust + Tauri + Vue + Axum.
+ [mxismean/chatgpt-app](https://github.com/mxismean/chatgpt-app) ![](https://img.shields.io/github/stars/mxismean/chatgpt-app?style=social) : Tauri é¡¹ç›®ï¼šChatGPT App.
+ [sonnylazuardi/chat-ai-desktop](https://github.com/sonnylazuardi/chat-ai-desktop) ![](https://img.shields.io/github/stars/sonnylazuardi/chat-ai-desktop?style=social) : Chat AI Desktop App. Unofficial ChatGPT desktop app for Mac & Windows menubar using Tauri & Rust.
+ [yetone/openai-translator](https://github.com/yetone/openai-translator) ![](https://img.shields.io/github/stars/yetone/openai-translator?style=social) : The translator that does more than just translation - powered by OpenAI.
+ [m1guelpf/browser-agent](https://github.com/m1guelpf/browser-agent) ![](https://img.shields.io/github/stars/m1guelpf/browser-agent?style=social) : A browser AI agent, using GPT-4. [docs.rs/browser-agent](https://docs.rs/browser-agent/latest/browser_agent/)
+ [sigoden/aichat](https://github.com/sigoden/aichat) ![](https://img.shields.io/github/stars/sigoden/aichat?style=social) : Using ChatGPT/GPT-3.5/GPT-4 in the terminal.
+ [uiuifree/rust-openai-chatgpt-api](https://github.com/uiuifree/rust-openai-chatgpt-api) ![](https://img.shields.io/github/stars/uiuifree/rust-openai-chatgpt-api?style=social) : "rust-openai-chatgpt-api" is a Rust library for accessing the ChatGPT API, a powerful NLP platform by OpenAI. The library provides a simple and efficient interface for sending requests and receiving responses, including chat. It uses reqwest and serde for HTTP requests and JSON serialization.
+ [1595901624/gpt-aggregated-edition](https://github.com/1595901624/gpt-aggregated-edition) ![](https://img.shields.io/github/stars/1595901624/gpt-aggregated-edition?style=social) : èšåˆChatGPTå®˜æ–¹ç‰ˆã€ChatGPTå…è´¹ç‰ˆã€æ–‡å¿ƒä¸€è¨€ã€Poeã€chatchatç­‰å¤šå¹³å°ï¼Œæ”¯æŒè‡ªå®šä¹‰å¯¼å…¥å¹³å°ã€‚
+ [Cormanz/smartgpt](https://github.com/Cormanz/smartgpt) ![](https://img.shields.io/github/stars/Cormanz/smartgpt?style=social) : A program that provides LLMs with the ability to complete complex tasks using plugins.
+ [femtoGPT](https://github.com/keyvank/femtoGPT) ![](https://img.shields.io/github/stars/keyvank/femtoGPT?style=social) : femtoGPT is a pure Rust implementation of a minimal Generative Pretrained Transformer. [discord.gg/wTJFaDVn45](https://github.com/keyvank/femtoGPT)
+ [shafishlabs/llmchain-rs](https://github.com/shafishlabs/llmchain-rs) ![](https://img.shields.io/github/stars/shafishlabs/llmchain-rs?style=social) : ğŸ¦€Rust + Large Language Models - Make AI Services Freely and Easily. Inspired by LangChain.
+ [flaneur2020/llama2.rs](https://github.com/flaneur2020/llama2.rs) ![](https://img.shields.io/github/stars/flaneur2020/llama2.rs?style=social) : An rust reimplementatin of [https://github.com/karpathy/llama2.c](https://github.com/karpathy/llama2.c).
+ [Heng30/chatbox](https://github.com/Heng30/chatbox) ![](https://img.shields.io/github/stars/Heng30/chatbox?style=social) : A Chatbot for OpenAI ChatGPT. Based on Slint-ui and Rust.
+ [fairjm/dioxus-openai-qa-gui](https://github.com/fairjm/dioxus-openai-qa-gui) ![](https://img.shields.io/github/stars/fairjm/dioxus-openai-qa-gui?style=social) : a simple openai qa desktop app built with dioxus.
+ [purton-tech/bionicgpt](https://github.com/purton-tech/bionicgpt) ![](https://img.shields.io/github/stars/purton-tech/bionicgpt?style=social) : Accelerate LLM adoption in your organisation. Chat with your confidential data safely and securely. [bionic-gpt.com](https://bionic-gpt.com/)
+ [InfiniTensor/transformer-rs](https://github.com/InfiniTensor/transformer-rs) ![](https://img.shields.io/github/stars/InfiniTensor/transformer-rs?style=social) : ä» [YdrMaster/llama2.rs](https://github.com/YdrMaster/llama2.rs) å‘å±•æ¥çš„æ‰‹å†™ transformer æ¨¡å‹é¡¹ç›®ã€‚

### Zig Implementation
+ [llama2.zig](https://github.com/cgbur/llama2.zig) ![](https://img.shields.io/github/stars/cgbur/llama2.zig?style=social) : Inference Llama 2 in one file of pure Zig.
+ [renerocksai/gpt4all.zig](https://github.com/renerocksai/gpt4all.zig) ![](https://img.shields.io/github/stars/renerocksai/gpt4all.zig?style=social) : ZIG build for a terminal-based chat client for an assistant-style large language model with ~800k GPT-3.5-Turbo Generations based on LLaMa.
+ [EugenHotaj/zig_inference](https://github.com/EugenHotaj/zig_inference) ![](https://img.shields.io/github/stars/EugenHotaj/zig_inference?style=social) : Neural Network Inference Engine in Zig.

### Go Implementation
+ [Ollama](https://github.com/ollama/ollama/) ![](https://img.shields.io/github/stars/ollama/ollama?style=social) : Get up and running with Llama 2, Mistral, Gemma, and other large language models. [ollama.com](https://ollama.com/)
+ [go-skynet/LocalAI](https://github.com/go-skynet/LocalAI) ![](https://img.shields.io/github/stars/go-skynet/LocalAI?style=social) : ğŸ¤– Self-hosted, community-driven, local OpenAI-compatible API. Drop-in replacement for OpenAI running LLMs on consumer-grade hardware. Free Open Source OpenAI alternative. No GPU required. LocalAI is an API to run ggml compatible models: llama, gpt4all, rwkv, whisper, vicuna, koala, gpt4all-j, cerebras, falcon, dolly, starcoder, and many other. [localai.io](https://localai.io/)

## â‡ï¸Vector Database å‘é‡æ•°æ®åº“
+ [Milvus](https://github.com/milvus-io/milvus) ![](https://img.shields.io/github/stars/milvus-io/milvus?style=social) : <font style="background-color:#FBDE28;">Milvus is an open-source vector database built to power embedding similarity search and AI applications. Milvus makes unstructured data search more accessible, and provides a consistent user experience regardless of the deployment environment. </font>[milvus.io](https://milvus.io/)
+ [Qdrant](https://github.com/qdrant/qdrant) ![](https://img.shields.io/github/stars/qdrant/qdrant?style=social) : Qdrant - Vector Database for the next generation of AI applications. Also available in the cloud [https://cloud.qdrant.io/](https://cloud.qdrant.io/). [qdrant.tech](https://qdrant.tech/)

## æ¨¡å‹çš„ç§æœ‰åŒ–éƒ¨ç½²
åœ°å€ï¼š[https://github.com/gptlink/gptlink-deploy/blob/master/README.md](https://github.com/gptlink/gptlink-deploy/blob/master/README.md)

