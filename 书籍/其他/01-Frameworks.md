## Official Version 
### 神经网络架构（<font style="color:rgb(31, 35, 40);">Neural Network Architecture）</font>
+ [Transformer](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py) ![](https://img.shields.io/github/stars/tensorflow/tensor2tensor?style=social) : "Attention is All You Need". ([arXiv 2017](https://arxiv.org/abs/1706.03762)).
+ [KAN](https://github.com/KindXiaoming/pykan) ![](https://img.shields.io/github/stars/KindXiaoming/pykan?style=social) : "KAN: Kolmogorov-Arnold Networks". ([arXiv 2024](https://arxiv.org/abs/2404.19756)).

### 大语言模型（LLM）Large Language Model 
#### GPT
+ GPT-1 : "Improving Language Understanding by Generative Pre-Training". ([cs.ubc.ca, 2018](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)).
+ [GPT-2](https://github.com/openai/gpt-2) ![](https://img.shields.io/github/stars/openai/gpt-2?style=social) : "Language Models are Unsupervised Multitask Learners". ([OpenAI blog, 2019](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)). [Better language models and their implications](https://openai.com/research/better-language-models).
+ [GPT-3](https://github.com/openai/gpt-3) ![](https://img.shields.io/github/stars/openai/gpt-3?style=social) : "GPT-3: Language Models are Few-Shot Learners". ([arXiv 2020](https://arxiv.org/abs/2005.14165)).
+ InstructGPT : "Training language models to follow instructions with human feedback". ([arXiv 2022](https://arxiv.org/abs/2203.02155)). "Aligning language models to follow instructions". ([OpenAI blog, 2022](https://openai.com/research/instruction-following)).
+ [ChatGPT](https://chat.openai.com/): [Optimizing Language Models for Dialogue](https://openai.com/blog/chatgpt).
+ [GPT-4](https://openai.com/product/gpt-4): GPT-4 is OpenAI’s most advanced system, producing safer and more useful responses. "Sparks of Artificial General Intelligence: Early experiments with GPT-4". ([arXiv 2023](https://arxiv.org/abs/2303.12712)). "GPT-4 Architecture, Infrastructure, Training Dataset, Costs, Vision, MoE". ([SemianAlysis, 2023](https://www.semianalysis.com/p/gpt-4-architecture-infrastructure)).
+ [GPT-4-LLM](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM) ![](https://img.shields.io/github/stars/Instruction-Tuning-with-GPT-4/GPT-4-LLM?style=social) : "Instruction Tuning with GPT-4". ([arXiv 2023](https://arxiv.org/abs/2304.03277)). [instruction-tuning-with-gpt-4.github.io/](https://instruction-tuning-with-gpt-4.github.io/)
+ [MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4) ![](https://img.shields.io/github/stars/Vision-CAIR/MiniGPT-4?style=social) : MiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models. [minigpt-4.github.io](https://minigpt-4.github.io/)
+ [minGPT](https://github.com/karpathy/minGPT) ![](https://img.shields.io/github/stars/karpathy/minGPT?style=social) : A minimal PyTorch re-implementation of the OpenAI GPT (Generative Pretrained Transformer) training.
+ [nanoGPT](https://github.com/karpathy/nanoGPT) ![](https://img.shields.io/github/stars/karpathy/nanoGPT?style=social) : The simplest, fastest repository for training/finetuning medium-sized GPTs.
+ [MicroGPT](https://github.com/muellerberndt/micro-gpt) ![](https://img.shields.io/github/stars/muellerberndt/micro-gpt?style=social) : A simple and effective autonomous agent compatible with GPT-3.5-Turbo and GPT-4. MicroGPT aims to be as compact and reliable as possible.
+ [GPT4All](https://github.com/nomic-ai/gpt4all) ![](https://img.shields.io/github/stars/nomic-ai/gpt4all?style=social) : GPT4All: An ecosystem of open-source on-edge large language models. GTP4All is an ecosystem to train and deploy powerful and customized large language models that run locally on consumer grade CPUs.
+ [WorkGPT](https://github.com/h2oai/h2ogpt) ![](https://img.shields.io/github/stars/h2oai/h2ogpt?style=social) : WorkGPT is an agent framework in a similar fashion to AutoGPT or LangChain.
+ [h2oGPT](https://github.com/team-openpm/workgpt) ![](https://img.shields.io/github/stars/team-openpm/workgpt?style=social) : h2oGPT is a large language model (LLM) fine-tuning framework and chatbot UI with document(s) question-answer capabilities. "h2oGPT: Democratizing Large Language Models". ([arXiv 2023](https://arxiv.org/abs/2306.08161)).
+ [DemoGPT](https://github.com/melih-unsal/DemoGPT) ![](https://img.shields.io/github/stars/melih-unsal/DemoGPT?style=social) : Create 🦜️🔗 LangChain apps by just using prompts with the power of Llama 2 🌟 Star to support our work! | 只需使用句子即可创建 LangChain 应用程序。 给个star支持我们的工作吧！DemoGPT: Auto Gen-AI App Generator with the Power of Llama 2. ⚡ With just a prompt, you can create interactive Streamlit apps via 🦜️🔗 LangChain's transformative capabilities & Llama 2.⚡ [demogpt.io](https://www.demogpt.io/)
+ [GPT-Engineer](https://github.com/AntonOsika/gpt-engineer) ![](https://img.shields.io/github/stars/AntonOsika/gpt-engineer?style=social) : Specify what you want it to build, the AI asks for clarification, and then builds it. GPT Engineer is made to be easy to adapt, extend, and make your agent learn how you want your code to look. It generates an entire codebase based on a prompt.
+ [1595901624/gpt-aggregated-edition](https://github.com/1595901624/gpt-aggregated-edition) ![](https://img.shields.io/github/stars/1595901624/gpt-aggregated-edition?style=social) : 聚合ChatGPT官方版、ChatGPT免费版、文心一言、Poe、chatchat等多平台，支持自定义导入平台。
+ [SpeechGPT](https://github.com/0nutation/SpeechGPT) ![](https://img.shields.io/github/stars/0nutation/SpeechGPT?style=social) : "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities". ([arXiv 2023](https://arxiv.org/abs/2305.11000)).
+ [GPT2-Chinese](https://github.com/Morizeyao/GPT2-Chinese) ![](https://img.shields.io/github/stars/Morizeyao/GPT2-Chinese?style=social) : Chinese version of GPT2 training code, using BERT tokenizer.
+ [gpt-llm-trainer](https://github.com/mshumer/gpt-llm-trainer) ![](https://img.shields.io/github/stars/mshumer/gpt-llm-trainer?style=social) : The goal of this project is to explore an experimental new pipeline to train a high-performing task-specific model. We try to abstract away all the complexity, so it's as easy as possible to go from idea -> performant fully-trained model.

#### deekseek
+ [GitHub - deepseek-ai/DeepSeek-R1](https://github.com/deepseek-ai/DeepSeek-R1)![](https://cdn.nlark.com/yuque/0/2025/png/2639475/1748399041967-f6e17b3a-9aab-4d31-97b6-1f6fe4d35282.png)：deepseek R1

#### Llama
+ [Llama 2](https://github.com/facebookresearch/llama) ![](https://img.shields.io/github/stars/facebookresearch/llama?style=social) : Inference code for LLaMA models. "LLaMA: Open and Efficient Foundation Language Models". ([arXiv 2023](https://arxiv.org/abs/2302.13971)). "Llama 2: Open Foundation and Fine-Tuned Chat Models". ([ai.meta.com, 2023-07-18](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)). ([2023-07-18, Llama 2 is here - get it on Hugging Face](https://huggingface.co/blog/llama2)).
+ [Llama 3](https://github.com/meta-llama/llama3) ![](https://img.shields.io/github/stars/meta-llama/llama3?style=social) : The official Meta Llama 3 GitHub site.
+ [Lit-LLaMA](https://github.com/Lightning-AI/lit-llama) ![](https://img.shields.io/github/stars/Lightning-AI/lit-llama?style=social) : ⚡ Lit-LLaMA. Implementation of the LLaMA language model based on nanoGPT. Supports flash attention, Int8 and GPTQ 4bit quantization, LoRA and LLaMA-Adapter fine-tuning, pre-training. Apache 2.0-licensed.
+ [LongLLaMA ](https://github.com/CStanKonrad/long_llama) ![](https://img.shields.io/github/stars/CStanKonrad/long_llama?style=social) : LongLLaMA is a large language model capable of handling long contexts. It is based on OpenLLaMA and fine-tuned with the Focused Transformer (FoT) method.
+ [LLaMA-Adapter](https://github.com/OpenGVLab/LLaMA-Adapter) ![](https://img.shields.io/github/stars/OpenGVLab/LLaMA-Adapter?style=social) : Fine-tuning LLaMA to follow Instructions within 1 Hour and 1.2M Parameters. LLaMA-Adapter: Efficient Fine-tuning of LLaMA 🚀
+ [Llama-2-Onnx](https://github.com/microsoft/Llama-2-Onnx) ![](https://img.shields.io/github/stars/microsoft/Llama-2-Onnx?style=social) : Llama 2 Powered By ONNX.
+ [Chinese LLaMA and Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca) ![](https://img.shields.io/github/stars/ymcui/Chinese-LLaMA-Alpaca?style=social) : 中文LLaMA&Alpaca大语言模型+本地CPU/GPU训练部署 (Chinese LLaMA & Alpaca LLMs)。"Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca". ([arXiv 2023](https://arxiv.org/abs/2304.08177)).
+ [Chinese-LLaMA-Alpaca-2](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2) ![](https://img.shields.io/github/stars/ymcui/Chinese-LLaMA-Alpaca-2?style=social) : 中文 LLaMA-2 & Alpaca-2 大模型二期项目 (Chinese LLaMA-2 & Alpaca-2 LLMs).
+ [FlagAlpha/Llama2-Chinese](https://github.com/FlagAlpha/Llama2-Chinese) ![](https://img.shields.io/github/stars/FlagAlpha/Llama2-Chinese?style=social) : Llama中文社区，最好的中文Llama大模型，完全开源可商用。
+ [michael-wzhu/Chinese-LlaMA2](https://github.com/michael-wzhu/Chinese-LlaMA2) ![](https://img.shields.io/github/stars/michael-wzhu/Chinese-LlaMA2?style=social) : Repo for adapting Meta LlaMA2 in Chinese! META最新发布的LlaMA2的汉化版！ （完全开源可商用）
+ [CrazyBoyM/llama3-Chinese-chat](https://github.com/CrazyBoyM/llama3-Chinese-chat) ![](https://img.shields.io/github/stars/CrazyBoyM/llama3-Chinese-chat?style=social) : Llama3 中文版。

#### Claude
+ [Claude](https://www.anthropic.com/product) : Claude is a next-generation AI assistant based on Anthropic’s research into training helpful, honest, and harmless AI systems.

#### Qwen
+ [Qwen（通义千问）](https://github.com/QwenLM/Qwen) ![](https://img.shields.io/github/stars/QwenLM/Qwen?style=social) : The official repo of Qwen (通义千问) chat & pretrained large language model proposed by Alibaba Cloud.
+ [Qwen2](https://github.com/QwenLM/Qwen2) ![](https://img.shields.io/github/stars/QwenLM/Qwen2?style=social) : Qwen2 is the large language model series developed by Qwen team, Alibaba Cloud.

#### ChatGLM
+ [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) ![](https://img.shields.io/github/stars/THUDM/ChatGLM-6B?style=social) : ChatGLM-6B: An Open Bilingual Dialogue Language Model | 开源双语对话语言模型。 ChatGLM-6B 是一个开源的、支持中英双语的对话语言模型，基于 [General Language Model (GLM)](https://github.com/THUDM/GLM) 架构，具有 62 亿参数。 "GLM: General Language Model Pretraining with Autoregressive Blank Infilling". ([ACL 2022](https://aclanthology.org/2022.acl-long.26/)).  "GLM-130B: An Open Bilingual Pre-trained Model". ([ICLR 2023](https://openreview.net/forum?id=-Aw0rrrPUF)).
+ [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B) ![](https://img.shields.io/github/stars/THUDM/ChatGLM2-6B?style=social) : ChatGLM2-6B: An Open Bilingual Chat LLM | 开源双语对话语言模型。ChatGLM2-6B 是开源中英双语对话模型 ChatGLM-6B 的第二代版本，在保留了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上，ChatGLM2-6B 引入了更强大的性能、更强大的性能、更高效的推理、更开放的协议。
+ [ChatGLM3](https://github.com/THUDM/ChatGLM3) ![](https://img.shields.io/github/stars/THUDM/ChatGLM3?style=social) : ChatGLM3 series: Open Bilingual Chat LLMs | 开源双语对话语言模型。

#### 书生浦语
+ [InternLM（书生·浦语）](https://github.com/InternLM/InternLM) ![](https://img.shields.io/github/stars/InternLM/InternLM?style=social) : Official release of InternLM2 7B and 20B base and chat models. 200K context support. [internlm.intern-ai.org.cn/](https://internlm.intern-ai.org.cn/)

#### 百川
+ [Baichuan-7B（百川-7B）](https://github.com/baichuan-inc/Baichuan-7B) ![](https://img.shields.io/github/stars/baichuan-inc/Baichuan-7B?style=social) : A large-scale 7B pretraining language model developed by BaiChuan-Inc. Baichuan-7B 是由百川智能开发的一个开源可商用的大规模预训练语言模型。基于 Transformer 结构，在大约 1.2 万亿 tokens 上训练的 70 亿参数模型，支持中英双语，上下文窗口长度为 4096。在标准的中文和英文 benchmark（C-Eval/MMLU）上均取得同尺寸最好的效果。[huggingface.co/baichuan-inc/baichuan-7B](https://huggingface.co/baichuan-inc/Baichuan-7B)
+ [Baichuan-13B（百川-13B）](https://github.com/baichuan-inc/Baichuan-13B) ![](https://img.shields.io/github/stars/baichuan-inc/Baichuan-13B?style=social) : A 13B large language model developed by Baichuan Intelligent Technology. Baichuan-13B 是由百川智能继 Baichuan-7B 之后开发的包含 130 亿参数的开源可商用的大规模语言模型，在权威的中文和英文 benchmark 上均取得同尺寸最好的效果。本次发布包含有预训练 (Baichuan-13B-Base) 和对齐 (Baichuan-13B-Chat) 两个版本。[huggingface.co/baichuan-inc/Baichuan-13B-Chat](https://huggingface.co/baichuan-inc/Baichuan-13B-Chat)
+ [Baichuan2](https://github.com/baichuan-inc/Baichuan2) ![](https://img.shields.io/github/stars/baichuan-inc/Baichuan2?style=social) : A series of large language models developed by Baichuan Intelligent Technology. Baichuan 2 是百川智能推出的新一代开源大语言模型，采用 2.6 万亿 Tokens 的高质量语料训练。Baichuan 2 在多个权威的中文、英文和多语言的通用、领域 benchmark 上取得同尺寸最佳的效果。本次发布包含有 7B、13B 的 Base 和 Chat 版本，并提供了 Chat 版本的 4bits 量化。[huggingface.co/baichuan-inc](https://huggingface.co/baichuan-inc). "Baichuan 2: Open Large-scale Language Models". ([arXiv 2023](https://arxiv.org/abs/2309.10305)).

#### 文心一言
+ [百度-文心大模型](https://wenxin.baidu.com/) : 百度全新一代知识增强大语言模型，文心大模型家族的新成员，能够与人对话互动，回答问题，协助创作，高效便捷地帮助人们获取信息、知识和灵感。
+ [百度智能云-千帆大模型](https://cloud.baidu.com/product/wenxinworkshop) : 百度智能云千帆大模型平台一站式企业级大模型平台，提供先进的生成式AI生产及应用全流程开发工具链。

#### 盘古大模型
+ [华为云-盘古大模型](https://www.huaweicloud.com/product/pangu.html) : 盘古大模型致力于深耕行业，打造金融、政务、制造、矿山、气象、铁路等领域行业大模型和能力集，将行业知识know-how与大模型能力相结合，重塑千行百业，成为各组织、企业、个人的专家助手。"Accurate medium-range global weather forecasting with 3D neural networks". ([Nature 2023](https://www.nature.com/articles/s41586-023-06185-3)).

#### 日日新
+ [商汤科技-日日新SenseNova](https://techday.sensetime.com/?utm_source=baidu-sem-pc&utm_medium=cpc&utm_campaign=PC-%E6%8A%80%E6%9C%AF%E4%BA%A4%E6%B5%81%E6%97%A5-%E4%BA%A7%E5%93%81%E8%AF%8D-%E6%97%A5%E6%97%A5%E6%96%B0&utm_content=%E6%97%A5%E6%97%A5%E6%96%B0&utm_term=%E6%97%A5%E6%97%A5%E6%96%B0SenseNova&e_creative=73937788324&e_keywordid=594802524403) : 日日新（SenseNova），是商汤科技宣布推出的大模型体系，包括自然语言处理模型“商量”（SenseChat）、文生图模型“秒画”和数字人视频生成平台“如影”（SenseAvatar）等。

#### 星火大模型
+ [科大讯飞-星火认知大模型](https://xinghuo.xfyun.cn/) : 新一代认知智能大模型，拥有跨领域知识和语言理解能力，能够基于自然对话方式理解与执行任务。

#### 豆包
+ [字节跳动-豆包](https://www.doubao.com/) : 豆包。

#### 其他
+ [Gemma](https://github.com/google/gemma_pytorch) ![](https://img.shields.io/github/stars/google/gemma_pytorch?style=social) : The official PyTorch implementation of Google's Gemma models. [ai.google.dev/gemma](https://ai.google.dev/gemma)
+ [Grok-1](https://github.com/xai-org/grok-1) ![](https://img.shields.io/github/stars/xai-org/grok-1?style=social) : This repository contains JAX example code for loading and running the Grok-1 open-weights model.
+ [Whisper](https://github.com/openai/whisper) ![](https://img.shields.io/github/stars/openai/whisper?style=social) : Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multitasking model that can perform multilingual speech recognition, speech translation, and language identification. "Robust Speech Recognition via Large-Scale Weak Supervision". ([arXiv 2022](https://arxiv.org/abs/2212.04356)).
+ [OpenChat](https://github.com/imoneoi/openchat) ![](https://img.shields.io/github/stars/imoneoi/openchat?style=social) : OpenChat: Advancing Open-source Language Models with Imperfect Data. [huggingface.co/openchat/openchat](https://huggingface.co/openchat/openchat)
+ [StableLM](https://github.com/Stability-AI/StableLM) ![](https://img.shields.io/github/stars/Stability-AI/StableLM?style=social) : StableLM: Stability AI Language Models.
+ [JARVIS](https://github.com/microsoft/JARVIS) ![](https://img.shields.io/github/stars/microsoft/JARVIS?style=social) : JARVIS, a system to connect LLMs with ML community. "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace". ([arXiv 2023](https://arxiv.org/abs/2303.17580)).
+ [Dolly](https://github.com/databrickslabs/dolly) ![](https://img.shields.io/github/stars/databrickslabs/dolly?style=social) : Databricks’ Dolly, a large language model trained on the Databricks Machine Learning Platform. [Hello Dolly: Democratizing the magic of ChatGPT with open models](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html)
+ [LMFlow](https://github.com/OptimalScale/LMFlow) ![](https://img.shields.io/github/stars/OptimalScale/LMFlow?style=social) : An extensible, convenient, and efficient toolbox for finetuning large machine learning models, designed to be user-friendly, speedy and reliable, and accessible to the entire community. Large Language Model for All. [optimalscale.github.io/LMFlow/](https://optimalscale.github.io/LMFlow/)
+ [Colossal-AI](https://github.com/hpcaitech/ColossalAI) ![](https://img.shields.io/github/stars/hpcaitech/ColossalAI?style=social) : Making big AI models cheaper, easier, and scalable. [www.colossalai.org](www.colossalai.org). "Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training". ([arXiv 2021](https://arxiv.org/abs/2110.14883)).
+ [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) ![](https://img.shields.io/github/stars/tatsu-lab/stanford_alpaca?style=social) : Stanford Alpaca: An Instruction-following LLaMA Model.
+ [feizc/Visual-LLaMA](https://github.com/feizc/Visual-LLaMA) ![](https://img.shields.io/github/stars/feizc/Visual-LLaMA?style=social) : Open LLaMA Eyes to See the World. This project aims to optimize LLaMA model for visual information understanding like GPT-4 and further explore the potentional of large language model.
+ [Lightning-AI/lightning-colossalai](https://github.com/Lightning-AI/lightning-colossalai) ![](https://img.shields.io/github/stars/Lightning-AI/lightning-colossalai?style=social) : Efficient Large-Scale Distributed Training with [Colossal-AI](https://colossalai.org/) and [Lightning AI](https://lightning.ai/).
+ [ChatALL](https://github.com/sunner/ChatALL) ![](https://img.shields.io/github/stars/sunner/ChatALL?style=social) :  Concurrently chat with ChatGPT, Bing Chat, bard, Alpaca, Vincuna, Claude, ChatGLM, MOSS, iFlytek Spark, ERNIE and more, discover the best answers. [chatall.ai](http://chatall.ai/)
+ [FreedomIntelligence/LLMZoo](https://github.com/FreedomIntelligence/LLMZoo) ![](https://img.shields.io/github/stars/FreedomIntelligence/LLMZoo?style=social) : ⚡LLM Zoo is a project that provides data, models, and evaluation benchmark for large language models.⚡ [Tech Report](https://github.com/FreedomIntelligence/LLMZoo/blob/main/assets/llmzoo.pdf)
+ [shm007g/LLaMA-Cult-and-More](https://github.com/shm007g/LLaMA-Cult-and-More) ![](https://img.shields.io/github/stars/shm007g/LLaMA-Cult-and-More?style=social) : News about 🦙 Cult and other AIGC models.
+ [X-PLUG/mPLUG-Owl](https://github.com/X-PLUG/mPLUG-Owl) ![](https://img.shields.io/github/stars/X-PLUG/mPLUG-Owl?style=social) : mPLUG-Owl🦉: Modularization Empowers Large Language Models with Multimodality.
+ [i-Code](https://github.com/microsoft/i-Code) ![](https://img.shields.io/github/stars/microsoft/i-Code?style=social) : The ambition of the i-Code project is to build integrative and composable multimodal Artificial Intelligence. The "i" stands for integrative multimodal learning. "CoDi: Any-to-Any Generation via Composable Diffusion". ([arXiv 2023](https://arxiv.org/abs/2305.11846)).
+ [Lamini](https://github.com/lamini-ai/lamini) ![](https://img.shields.io/github/stars/lamini-ai/lamini?style=social) : Lamini: The LLM engine for rapidly customizing models 🦙
+ [xorbitsai/inference](https://github.com/xorbitsai/inference) ![](https://img.shields.io/github/stars/xorbitsai/inference?style=social) : Xorbits Inference (Xinference) is a powerful and versatile library designed to serve LLMs, speech recognition models, and multimodal models, even on your laptop. It supports a variety of models compatible with GGML, such as llama, chatglm, baichuan, whisper, vicuna, orac, and many others.
+ [epfLLM/Megatron-LLM](https://github.com/epfLLM/Megatron-LLM) ![](https://img.shields.io/github/stars/epfLLM/Megatron-LLM?style=social) : distributed trainer for LLMs.
+ [AmineDiro/cria](https://github.com/AmineDiro/cria) ![](https://img.shields.io/github/stars/AmineDiro/cria?style=social) : OpenAI compatible API for serving LLAMA-2 model.
+ [MOSS](https://github.com/OpenLMLab/MOSS) ![](https://img.shields.io/github/stars/OpenLMLab/MOSS?style=social) : An open-source tool-augmented conversational language model from Fudan University. MOSS是一个支持中英双语和多种插件的开源对话语言模型，moss-moon系列模型具有160亿参数，在FP16精度下可在单张A100/A800或两张3090显卡运行，在INT4/8精度下可在单张3090显卡运行。MOSS基座语言模型在约七千亿中英文以及代码单词上预训练得到，后续经过对话指令微调、插件增强学习和人类偏好训练具备多轮对话能力及使用多种插件的能力。[txsun1997.github.io/blogs/moss.html](https://txsun1997.github.io/blogs/moss.html)
+ [BayLing（百聆）](https://github.com/ictnlp/BayLing) ![](https://img.shields.io/github/stars/OpenLMLab/MOSS?style=social) : “百聆”是一个具有增强的语言对齐的英语/中文大语言模型，具有优越的英语/中文能力，在多项测试中取得ChatGPT 90%的性能。BayLing is an English/Chinese LLM equipped with advanced language alignment, showing superior capability in English/Chinese generation, instruction following and multi-turn interaction. [nlp.ict.ac.cn/bayling](http://nlp.ict.ac.cn/bayling). "BayLing: Bridging Cross-lingual Alignment and Instruction Following through Interactive Translation for Large Language Models". ([arXiv 2023](https://arxiv.org/abs/2306.10968)).
+ [FlagAI（悟道·天鹰（Aquila））](https://github.com/FlagAI-Open/FlagAI) ![](https://img.shields.io/github/stars/FlagAI-Open/FlagAI?style=social) : FlagAI (Fast LArge-scale General AI models) is a fast, easy-to-use and extensible toolkit for large-scale model. Our goal is to support training, fine-tuning, and deployment of large-scale models on various downstream tasks with multi-modality.
+ [YuLan-Chat（玉兰）](https://github.com/RUC-GSAI/YuLan-Chat/) ![](https://img.shields.io/github/stars/RUC-GSAI/YuLan-Chat?style=social) : YuLan-Chat models are chat-based large language models, which are developed by the researchers in GSAI, Renmin University of China (YuLan, which represents Yulan Magnolia, is the campus flower of Renmin University of China). The newest version is developed by continually-pretraining and instruction-tuning [LLaMA-2](https://github.com/facebookresearch/llama) with high-quality English and Chinese data. YuLan-Chat系列模型是中国人民大学高瓴人工智能学院师生共同开发的支持聊天的大语言模型（名字"玉兰"取自中国人民大学校花）。 最新版本基于LLaMA-2进行了中英文双语的继续预训练和指令微调。
+ [Yi-1.5](https://github.com/01-ai/Yi-1.5) ![](https://img.shields.io/github/stars/01-ai/Yi-1.5?style=social) : Yi-1.5 is an upgraded version of Yi, delivering stronger performance in coding, math, reasoning, and instruction-following capability.
+ [智海-录问](https://github.com/zhihaiLLM/wisdomInterrogatory) ![](https://img.shields.io/github/stars/zhihaiLLM/wisdomInterrogatory?style=social) : 智海-录问(wisdomInterrogatory)是由浙江大学、阿里巴巴达摩院以及华院计算三家单位共同设计研发的法律大模型。核心思想：以“普法共享和司法效能提升”为目标，从推动法律智能化体系入司法实践、数字化案例建设、虚拟法律咨询服务赋能等方面提供支持，形成数字化和智能化的司法基座能力。
+ [活字](https://github.com/HIT-SCIR/huozi) ![](https://img.shields.io/github/stars/HIT-SCIR/huozi?style=social) : 活字是由哈工大自然语言处理研究所多位老师和学生参与开发的一个开源可商用的大规模预训练语言模型。 该模型基于 Bloom 结构的70 亿参数模型，支持中英双语，上下文窗口长度为 2048。 在标准的中文和英文基准以及主观评测上均取得同尺寸中优异的结果。
+ [MiLM-6B](https://github.com/XiaoMi/MiLM-6B) ![](https://img.shields.io/github/stars/XiaoMi/MiLM-6B?style=social) : MiLM-6B 是由小米开发的一个大规模预训练语言模型，参数规模为64亿。在 C-Eval 和 CMMLU 上均取得同尺寸最好的效果。
+ [CPM-Bee](https://github.com/OpenBMB/CPM-Bee) ![](https://img.shields.io/github/stars/OpenBMB/CPM-Bee?style=social) : CPM-Bee是一个完全开源、允许商用的百亿参数中英文基座模型，也是[CPM-Live](https://live.openbmb.org/)训练的第二个里程碑。
+ [PandaLM](https://github.com/WeOpenML/PandaLM) ![](https://img.shields.io/github/stars/WeOpenML/PandaLM?style=social) : PandaLM: Reproducible and Automated Language Model Assessment.
+ [Chinese-Tiny-LLM](https://github.com/Chinese-Tiny-LLM/Chinese-Tiny-LLM) ![](https://img.shields.io/github/stars/Chinese-Tiny-LLM/Chinese-Tiny-LLM?style=social) : "Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model". ([arXiv 2024](https://arxiv.org/abs/2404.04167)).
+ [潘多拉 (Pandora)](https://github.com/pengzhile/pandora) ![](https://img.shields.io/github/stars/pengzhile/pandora?style=social) : 潘多拉，一个让你呼吸顺畅的ChatGPT。Pandora, a ChatGPT that helps you breathe smoothly.

### 视觉大模型（VFM）Vision Foundation Model
+ [Visual ChatGPT](https://github.com/microsoft/visual-chatgpt) ![](https://img.shields.io/github/stars/microsoft/visual-chatgpt?style=social) : Visual ChatGPT connects ChatGPT and a series of Visual Foundation Models to enable sending and receiving images during chatting. "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models". ([arXiv 2023](https://arxiv.org/abs/2303.04671)).
+ [InternImage](https://github.com/OpenGVLab/InternImage) ![](https://img.shields.io/github/stars/OpenGVLab/InternImage?style=social) : "InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions". ([CVPR 2023](https://arxiv.org/abs/2211.05778)).
+ [GLIP](https://github.com/microsoft/GLIP) ![](https://img.shields.io/github/stars/microsoft/GLIP?style=social) : "Grounded Language-Image Pre-training". ([CVPR 2022](https://arxiv.org/abs/2112.03857)).
+ [GLIPv2](https://github.com/microsoft/GLIP) ![](https://img.shields.io/github/stars/microsoft/GLIP?style=social) : "GLIPv2: Unifying Localization and Vision-Language Understanding". ([arXiv 2022](https://arxiv.org/abs/2206.05836)).
+ [DINO](https://github.com/IDEA-Research/DINO) ![](https://img.shields.io/github/stars/IDEA-Research/DINO?style=social) : "DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection". ([ICLR 2023](https://arxiv.org/abs/2203.03605)).
+ [DINOv2](https://github.com/facebookresearch/dinov2) ![](https://img.shields.io/github/stars/facebookresearch/dinov2?style=social) : "DINOv2: Learning Robust Visual Features without Supervision". ([arXiv 2023](https://arxiv.org/abs/2304.07193)).
+ [Grounding DINO](https://github.com/IDEA-Research/GroundingDINO) ![](https://img.shields.io/github/stars/IDEA-Research/GroundingDINO?style=social) : "Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection". ([arXiv 2023](https://arxiv.org/abs/2303.05499)). "知乎「三分钟热度」《[十分钟解读Grounding DINO-根据文字提示检测任意目标](https://zhuanlan.zhihu.com/p/627646794)》"。
+ [SAM](https://github.com/facebookresearch/segment-anything) ![](https://img.shields.io/github/stars/facebookresearch/segment-anything?style=social) : The repository provides code for running inference with the Segment Anything Model (SAM), links for downloading the trained model checkpoints, and example notebooks that show how to use the model. "Segment Anything". ([arXiv 2023](https://arxiv.org/abs/2304.02643)).
+ [Grounded-SAM](https://github.com/IDEA-Research/Grounded-Segment-Anything) ![](https://img.shields.io/github/stars/IDEA-Research/Grounded-Segment-Anything?style=social) : Marrying Grounding DINO with Segment Anything & Stable Diffusion & Tag2Text & BLIP & Whisper & ChatBot - Automatically Detect , Segment and Generate Anything with Image, Text, and Audio Inputs. We plan to create a very interesting demo by combining [Grounding DINO](https://github.com/IDEA-Research/GroundingDINO) and [Segment Anything](https://github.com/facebookresearch/segment-anything) which aims to detect and segment Anything with text inputs!
+ [SEEM](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once) ![](https://img.shields.io/github/stars/UX-Decoder/Segment-Everything-Everywhere-All-At-Once?style=social) : We introduce SEEM that can Segment Everything Everywhere with Multi-modal prompts all at once. SEEM allows users to easily segment an image using prompts of different types including visual prompts (points, marks, boxes, scribbles and image segments) and language prompts (text and audio), etc. It can also work with any combinations of prompts or generalize to custom prompts! "Segment Everything Everywhere All at Once". ([arXiv 2023](https://arxiv.org/abs/2304.06718)).
+ [SAM3D](https://github.com/DYZhang09/SAM3D) ![](https://img.shields.io/github/stars/DYZhang09/SAM3D?style=social) : "SAM3D: Zero-Shot 3D Object Detection via [Segment Anything](https://github.com/facebookresearch/segment-anything) Model". ([arXiv 2023](https://arxiv.org/abs/2306.02245)).
+ [ImageBind](https://github.com/facebookresearch/ImageBind) ![](https://img.shields.io/github/stars/facebookresearch/ImageBind?style=social) : "ImageBind: One Embedding Space To Bind Them All". ([CVPR 2023](https://arxiv.org/abs/2305.05665)).
+ [Track-Anything](https://github.com/gaomingqi/Track-Anything) ![](https://img.shields.io/github/stars/gaomingqi/Track-Anything?style=social) : Track-Anything is a flexible and interactive tool for video object tracking and segmentation, based on Segment Anything, XMem, and E2FGVI. "Track Anything: Segment Anything Meets Videos". ([arXiv 2023](https://arxiv.org/abs/2304.11968)).
+ [qianqianwang68/omnimotion](https://github.com/qianqianwang68/omnimotion) ![](https://img.shields.io/github/stars/qianqianwang68/omnimotion?style=social) : "Tracking Everything Everywhere All at Once". ([arXiv 2023](https://arxiv.org/abs/2306.05422)).
+ [LLaVA](https://github.com/haotian-liu/LLaVA) ![](https://img.shields.io/github/stars/haotian-liu/LLaVA?style=social) : 🌋 LLaVA: Large Language and Vision Assistant. Visual instruction tuning towards large language and vision models with GPT-4 level capabilities. [llava.hliu.cc](https://llava.hliu.cc/). "Visual Instruction Tuning". ([arXiv 2023](https://arxiv.org/abs/2304.08485)).
+ [M3I-Pretraining](https://github.com/OpenGVLab/M3I-Pretraining) ![](https://img.shields.io/github/stars/OpenGVLab/M3I-Pretraining?style=social) : "Towards All-in-one Pre-training via Maximizing Multi-modal Mutual Information". ([arXiv 2022](https://arxiv.org/abs/2211.09807)).
+ [BEVFormer](https://github.com/fundamentalvision/BEVFormer) ![](https://img.shields.io/github/stars/fundamentalvision/BEVFormer?style=social) : BEVFormer: a Cutting-edge Baseline for Camera-based Detection. "BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers". ([arXiv 2022](https://arxiv.org/abs/2203.17270)).
+ [Uni-Perceiver](https://github.com/fundamentalvision/Uni-Perceiver) ![](https://img.shields.io/github/stars/fundamentalvision/Uni-Perceiver?style=social) : "Uni-Perceiver: Pre-training Unified Architecture for Generic Perception for Zero-shot and Few-shot Tasks". ([CVPR 2022](https://openaccess.thecvf.com/content/CVPR2022/html/Zhu_Uni-Perceiver_Pre-Training_Unified_Architecture_for_Generic_Perception_for_Zero-Shot_and_CVPR_2022_paper.html)).
+ [AnyLabeling](https://github.com/vietanhdev/anylabeling) ![](https://img.shields.io/github/stars/vietanhdev/anylabeling?style=social) : 🌟 AnyLabeling 🌟. Effortless data labeling with AI support from YOLO and Segment Anything! Effortless data labeling with AI support from YOLO and Segment Anything!
+ [X-AnyLabeling](https://github.com/CVHub520/X-AnyLabeling) ![](https://img.shields.io/github/stars/CVHub520/X-AnyLabeling?style=social) : 💫 X-AnyLabeling 💫. Effortless data labeling with AI support from Segment Anything and other awesome models!
+ [Label Anything](https://github.com/open-mmlab/playground/tree/main/label_anything) ![](https://img.shields.io/github/stars/open-mmlab/playground?style=social) : OpenMMLab PlayGround: Semi-Automated Annotation with Label-Studio and SAM.
+ [RevCol](https://github.com/megvii-research/RevCol) ![](https://img.shields.io/github/stars/megvii-research/RevCol?style=social) : "Reversible Column Networks". ([arXiv 2023](https://arxiv.org/abs/2212.11696)).
+ [Macaw-LLM](https://github.com/lyuchenyang/Macaw-LLM) ![](https://img.shields.io/github/stars/lyuchenyang/Macaw-LLM?style=social) : Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration.
+ [SAM-PT](https://github.com/SysCV/sam-pt) ![](https://img.shields.io/github/stars/SysCV/sam-pt?style=social) : SAM-PT: Extending SAM to zero-shot video segmentation with point-based tracking. "Segment Anything Meets Point Tracking". ([arXiv 2023](https://arxiv.org/abs/2307.01197)).
+ [Video-LLaMA](https://github.com/DAMO-NLP-SG/Video-LLaMA) ![](https://img.shields.io/github/stars/DAMO-NLP-SG/Video-LLaMA?style=social) : "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding". ([arXiv 2023](https://arxiv.org/abs/2306.02858)).
+ [MobileSAM](https://github.com/ChaoningZhang/MobileSAM) ![](https://img.shields.io/github/stars/ChaoningZhang/MobileSAM?style=social) : "Faster Segment Anything: Towards Lightweight SAM for Mobile Applications". ([arXiv 2023](https://arxiv.org/abs/2306.14289)).
+ [BuboGPT](https://github.com/magic-research/bubogpt) ![](https://img.shields.io/github/stars/magic-research/bubogpt?style=social) : "BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs". ([arXiv 2023](https://arxiv.org/abs/2307.08581)).

### 人工智能生成内容（AIGC）AI Generated Content
+ [Sora](https://openai.com/sora) : Sora is an AI model that can create realistic and imaginative scenes from text instructions.
+ [Open Sora Plan](https://github.com/PKU-YuanGroup/Open-Sora-Plan) ![](https://img.shields.io/github/stars/PKU-YuanGroup/Open-Sora-Plan?style=social) : This project aim to reproducing [Sora](https://openai.com/sora) (Open AI T2V model), but we only have limited resource. We deeply wish the all open source community can contribute to this project. 本项目希望通过开源社区的力量复现Sora，由北大-兔展AIGC联合实验室共同发起，当前我们资源有限仅搭建了基础架构，无法进行完整训练，希望通过开源社区逐步增加模块并筹集资源进行训练，当前版本离目标差距巨大，仍需持续完善和快速迭代，欢迎Pull request！！！[Project Page](https://pku-yuangroup.github.io/Open-Sora-Plan/) [中文主页](https://pku-yuangroup.github.io/Open-Sora-Plan/blog_cn.html)
+ [Mini Sora](https://github.com/mini-sora/minisora) ![](https://img.shields.io/github/stars/mini-sora/minisora?style=social) : The Mini Sora project aims to explore the implementation path and future development direction of Sora.
+ [EMO](https://github.com/HumanAIGC/EMO) ![](https://img.shields.io/github/stars/HumanAIGC/EMO?style=social) : "EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions". ([arXiv 2024](https://arxiv.org/abs/2402.17485)).
+ [Stable Diffusion](https://github.com/CompVis/stable-diffusion) ![](https://img.shields.io/github/stars/CompVis/stable-diffusion?style=social) : Stable Diffusion is a latent text-to-image diffusion model. Stable Diffusion was made possible thanks to a collaboration with [Stability AI](https://stability.ai/) and [Runway](https://runwayml.com/) and builds upon our previous work "High-Resolution Image Synthesis with Latent Diffusion Models". ([CVPR 2022](https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html)).
+ [Stable Diffusion Version 2](https://github.com/Stability-AI/stablediffusion) ![](https://img.shields.io/github/stars/Stability-AI/stablediffusion?style=social) : This repository contains [Stable Diffusion](https://github.com/CompVis/stable-diffusion) models trained from scratch and will be continuously updated with new checkpoints. "High-Resolution Image Synthesis with Latent Diffusion Models". ([CVPR 2022](https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html)).
+ [StableStudio](https://github.com/Stability-AI/StableStudio) ![](https://img.shields.io/github/stars/Stability-AI/StableStudio?style=social) : StableStudio by [Stability AI](https://stability.ai/). 👋 Welcome to the community repository for StableStudio, the open-source version of [DreamStudio](https://dreamstudio.ai/).
+ [AudioCraft](https://github.com/facebookresearch/audiocraft) ![](https://img.shields.io/github/stars/facebookresearch/audiocraft?style=social) : Audiocraft is a library for audio processing and generation with deep learning. It features the state-of-the-art EnCodec audio compressor / tokenizer, along with MusicGen, a simple and controllable music generation LM with textual and melodic conditioning.
+ [InvokeAI](https://github.com/invoke-ai/InvokeAI) ![](https://img.shields.io/github/stars/invoke-ai/InvokeAI?style=social) : Invoke AI - Generative AI for Professional Creatives. Professional Creative Tools for Stable Diffusion, Custom-Trained Models, and more. [invoke-ai.github.io/InvokeAI/](https://invoke-ai.github.io/InvokeAI/)
+ [DragGAN](https://github.com/XingangPan/DragGAN) ![](https://img.shields.io/github/stars/XingangPan/DragGAN?style=social) : "Stable Diffusion Training with MosaicML. This repo contains code used to train your own Stable Diffusion model on your own data". ([SIGGRAPH 2023](https://vcai.mpi-inf.mpg.de/projects/DragGAN/)).
+ [AudioGPT](https://github.com/AIGC-Audio/AudioGPT) ![](https://img.shields.io/github/stars/AIGC-Audio/AudioGPT?style=social) : AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head.
+ [PandasAI](https://github.com/gventuri/pandas-ai) ![](https://img.shields.io/github/stars/gventuri/pandas-ai?style=social) : Pandas AI is a Python library that adds generative artificial intelligence capabilities to Pandas, the popular data analysis and manipulation tool. It is designed to be used in conjunction with Pandas, and is not a replacement for it.
+ [mosaicml/diffusion](https://github.com/mosaicml/diffusion) ![](https://img.shields.io/github/stars/mosaicml/diffusion?style=social) : Stable Diffusion Training with MosaicML. This repo contains code used to train your own Stable Diffusion model on your own data.
+ [VisorGPT](https://github.com/Sierkinhane/VisorGPT) ![](https://img.shields.io/github/stars/Sierkinhane/VisorGPT?style=social) : Customize spatial layouts for conditional image synthesis models, e.g., ControlNet, using GPT. "VisorGPT: Learning Visual Prior via Generative Pre-Training". ([arXiv 2023](https://arxiv.org/abs/2305.13777)).
+ [ControlNet](https://github.com/lllyasviel/ControlNet) ![](https://img.shields.io/github/stars/lllyasviel/ControlNet?style=social) : Let us control diffusion models! "Adding Conditional Control to Text-to-Image Diffusion Models". ([arXiv 2023](https://arxiv.org/abs/2302.05543)).
+ [Fooocus](https://github.com/lllyasviel/Fooocus) ![](https://img.shields.io/github/stars/lllyasviel/Fooocus?style=social) : Fooocus is an image generating software. Fooocus is a rethinking of Stable Diffusion and Midjourney’s designs. "微信公众号「GitHubStore」《[Fooocus : 集Stable Diffusion 和 Midjourney 优点于一身的开源AI绘图软件](https://mp.weixin.qq.com/s/adyXek6xcz5aOPAGqZBrvg)》"。
+ [MindDiffuser](https://github.com/ReedOnePeck/MindDiffuser) ![](https://img.shields.io/github/stars/ReedOnePeck/MindDiffuser?style=social) : "MindDiffuser: Controlled Image Reconstruction from Human Brain Activity with Semantic and Structural Diffusion". ([arXiv 2023](https://arxiv.org/abs/2308.04249)).



+ [Midjourney](https://www.midjourney.com/) : Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species.
+ [DreamStudio](https://dreamstudio.ai/) : Effortless image generation for creators with big dreams.
+ [Firefly](https://www.adobe.com/sensei/generative-ai/firefly.html) : Adobe Firefly: Experiment, imagine, and make an infinite range of creations with Firefly, a family of creative generative AI models coming to Adobe products.
+ [Jasper](https://www.jasper.ai/) : Meet Jasper. On-brand AI content wherever you create.
+ [Copy.ai](https://www.copy.ai/) : Whatever you want to ask, our chat has the answers.
+ [Peppertype.ai](https://www.peppercontent.io/peppertype-ai/) : Leverage the AI-powered platform to ideate, create, distribute, and measure your content and prove your content marketing ROI.
+ [ChatPPT](https://chat-ppt.com/) : ChatPPT来袭命令式一键生成PPT。

## 应用程序开发平台 Application Development Platform
+ [LangChain](https://github.com/langchain-ai/langchain) ![](https://img.shields.io/github/stars/hwchase17/langchain?style=social) :  🦜️🔗 LangChain. ⚡ Building applications with LLMs through composability ⚡ [python.langchain.com](https://python.langchain.com/docs/get_started/introduction.html)
+ [Dify](https://github.com/langgenius/dify) ![](https://img.shields.io/github/stars/langgenius/dify?style=social) : An Open-Source Assistants API and GPTs alternative. Dify.AI is an LLM application development platform. It integrates the concepts of Backend as a Service and LLMOps, covering the core tech stack required for building generative AI-native applications, including a built-in RAG engine. [dify.ai](https://dify.ai/)
+ [AutoChain](https://github.com/Forethought-Technologies/AutoChain) ![](https://img.shields.io/github/stars/Forethought-Technologies/AutoChain?style=social) :  AutoChain: Build lightweight, extensible, and testable LLM Agents. [autochain.forethought.ai](https://autochain.forethought.ai/)
+ [Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT) ![](https://img.shields.io/github/stars/Significant-Gravitas/Auto-GPT?style=social) : Auto-GPT: An Autonomous GPT-4 Experiment. Auto-GPT is an experimental open-source application showcasing the capabilities of the GPT-4 language model. This program, driven by GPT-4, chains together LLM "thoughts", to autonomously achieve whatever goal you set. As one of the first examples of GPT-4 running fully autonomously, Auto-GPT pushes the boundaries of what is possible with AI. [agpt.co](https://news.agpt.co/)
+ [LiteChain](https://github.com/rogeriochaves/litechain) ![](https://img.shields.io/github/stars/rogeriochaves/litechain?style=social) : Build robust LLM applications with true composability 🔗. [rogeriochaves.github.io/litechain/](https://rogeriochaves.github.io/litechain/)
+ [Open-Assistant](https://github.com/LAION-AI/Open-Assistant) ![](https://img.shields.io/github/stars/LAION-AI/Open-Assistant?style=social) : OpenAssistant is a chat-based assistant that understands tasks, can interact with third-party systems, and retrieve information dynamically to do so. [open-assistant.io](https://open-assistant.io/)

## 微调框架 Fine-Tuning Framework 
+ [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) ![](https://img.shields.io/github/stars/hiyouga/LLaMA-Factory?style=social) : Unify Efficient Fine-Tuning of 100+ LLMs. Fine-tuning a large language model can be easy as...

## ❇️检索增强生成框架 RAG Framework 
+ [LlamaIndex](https://github.com/run-llama/llama_index) ![](https://img.shields.io/github/stars/run-llama/llama_index?style=social) : <font style="background-color:#FBDE28;">LlamaIndex is a data framework for your LLM applications. </font>[docs.llamaindex.ai](https://docs.llamaindex.ai/)
+ [Embedchain](https://github.com/embedchain/embedchain) ![](https://img.shields.io/github/stars/embedchain/embedchain?style=social) : The Open Source RAG framework. [docs.embedchain.ai](https://docs.embedchain.ai/)
+ [QAnything](https://github.com/netease-youdao/QAnything) ![](https://img.shields.io/github/stars/netease-youdao/QAnything?style=social) : Question and Answer based on Anything. [qanything.ai](https://qanything.ai/)
+ [R2R](https://github.com/SciPhi-AI/R2R) ![](https://img.shields.io/github/stars/SciPhi-AI/R2R?style=social) : A framework for rapid development and deployment of production-ready RAG systems. [docs.sciphi.ai](https://docs.sciphi.ai/)
+ [langchain-ai/rag-from-scratch](https://github.com/langchain-ai/rag-from-scratch) ![](https://img.shields.io/github/stars/langchain-ai/rag-from-scratch?style=social) : Retrieval augmented generation (RAG) comes is a general methodology for connecting LLMs with external data sources. These notebooks accompany a video series will build up an understanding of RAG from scratch, starting with the basics of indexing, retrieval, and generation.

## 大语言模型推理框架 LLM Inference Framework 
### LLM Inference Benchmark 推理基准
+ [ninehills/llm-inference-benchmark](https://github.com/ninehills/llm-inference-benchmark) ![](https://img.shields.io/github/stars/ninehills/llm-inference-benchmark?style=social) : LLM Inference benchmark.
+ [csbench/csbench](https://github.com/csbench/csbench) ![](https://img.shields.io/github/stars/csbench/csbench?style=social) : "CS-Bench: A Comprehensive Benchmark for Large Language Models towards Computer Science Mastery". ([arXiv 2024](https://arxiv.org/abs/2406.08587)).

### LLM Deployment Engine 部署与配置引擎
+ [vllm-project/vllm](https://github.com/vllm-project/vllm) ![](https://img.shields.io/github/stars/vllm-project/vllm?style=social) : A high-throughput and memory-efficient inference and serving engine for LLMs. [vllm.readthedocs.io](https://vllm.readthedocs.io/en/latest/)
+ [MLC LLM](https://github.com/mlc-ai/mlc-llm) ![](https://img.shields.io/github/stars/mlc-ai/mlc-llm?style=social) : Enable everyone to develop, optimize and deploy AI models natively on everyone's devices. [mlc.ai/mlc-llm](https://mlc.ai/mlc-llm/)
+ [Lamini](https://github.com/lamini-ai/lamini) ![](https://img.shields.io/github/stars/lamini-ai/lamini?style=social) : Lamini: The LLM engine for rapidly customizing models 🦙.
+ [datawhalechina/self-llm](https://github.com/datawhalechina/self-llm) ![](https://img.shields.io/github/stars/datawhalechina/self-llm?style=social) :  《开源大模型食用指南》基于Linux环境快速部署开源大模型，更适合中国宝宝的部署教程。

### C Implementation
+ [llm.c](https://github.com/karpathy/llm.c) ![](https://img.shields.io/github/stars/karpathy/llm.c?style=social) : LLM training in simple, pure C/CUDA. There is no need for 245MB of PyTorch or 107MB of cPython. For example, training GPT-2 (CPU, fp32) is ~1,000 lines of clean code in a single file. It compiles and runs instantly, and exactly matches the PyTorch reference implementation.
+ [llama2.c](https://github.com/karpathy/llama2.c) ![](https://img.shields.io/github/stars/karpathy/llama2.c?style=social) : Inference Llama 2 in one file of pure C. Train the Llama 2 LLM architecture in PyTorch then inference it with one simple 700-line C file (run.c).

### CPP Implementation
+ [TensorRT](https://github.com/NVIDIA/TensorRT) ![](https://img.shields.io/github/stars/NVIDIA/TensorRT?style=social) : NVIDIA® TensorRT™ is an SDK for high-performance deep learning inference on NVIDIA GPUs. This repository contains the open source components of TensorRT. [developer.nvidia.com/tensorrt](https://developer.nvidia.com/tensorrt)
+ [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) ![](https://img.shields.io/github/stars/NVIDIA/TensorRT-LLM?style=social) : TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that execute those TensorRT engines. [nvidia.github.io/TensorRT-LLM](https://nvidia.github.io/TensorRT-LLM)
+ [gemma.cpp](https://github.com/google/gemma.cpp) ![](https://img.shields.io/github/stars/google/gemma.cpp?style=social) :  gemma.cpp is a lightweight, standalone C++ inference engine for the Gemma foundation models from Google.
+ [llama.cpp](https://github.com/ggerganov/llama.cpp) ![](https://img.shields.io/github/stars/ggerganov/llama.cpp?style=social) : Inference of [LLaMA](https://github.com/facebookresearch/llama) model in pure C/C++.
+ [whisper.cpp](https://github.com/ggerganov/whisper.cpp) ![](https://img.shields.io/github/stars/ggerganov/whisper.cpp?style=social) : High-performance inference of [OpenAI's Whisper](https://github.com/openai/whisper) automatic speech recognition (ASR) model.
+ [ChatGLM.cpp](https://github.com/li-plus/chatglm.cpp) ![](https://img.shields.io/github/stars/li-plus/chatglm.cpp?style=social) : C++ implementation of [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) and [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B).
+ [MegEngine/InferLLM](https://github.com/MegEngine/InferLLM) ![](https://img.shields.io/github/stars/MegEngine/InferLLM?style=social) : InferLLM is a lightweight LLM model inference framework that mainly references and borrows from the llama.cpp project.
+ [DeployAI/nndeploy](https://github.com/DeployAI/nndeploy) ![](https://img.shields.io/github/stars/DeployAI/nndeploy?style=social) : nndeploy是一款模型端到端部署框架。以多端推理以及基于有向无环图模型部署为内核，致力为用户提供跨平台、简单易用、高性能的模型部署体验。[nndeploy-zh.readthedocs.io/zh/latest/](https://nndeploy-zh.readthedocs.io/zh/latest/)
+ [zjhellofss/KuiperInfer (自制深度学习推理框架)](https://github.com/zjhellofss/KuiperInfer) ![](https://img.shields.io/github/stars/zjhellofss/KuiperInfer?style=social) :  带你从零实现一个高性能的深度学习推理库，支持llama 、Unet、Yolov5、Resnet等模型的推理。Implement a high-performance deep learning inference library step by step.
+ [skeskinen/llama-lite](https://github.com/skeskinen/llama-lite) ![](https://img.shields.io/github/stars/skeskinen/llama-lite?style=social) : Embeddings focused small version of Llama NLP model.
+ [Const-me/Whisper](https://github.com/Const-me/Whisper) ![](https://img.shields.io/github/stars/Const-me/Whisper?style=social) : High-performance GPGPU inference of OpenAI's Whisper automatic speech recognition (ASR) model.
+ [wangzhaode/ChatGLM-MNN](https://github.com/wangzhaode/ChatGLM-MNN) ![](https://img.shields.io/github/stars/wangzhaode/ChatGLM-MNN?style=social) : Pure C++, Easy Deploy ChatGLM-6B.
+ [ztxz16/fastllm](https://github.com/ztxz16/fastllm) ![](https://img.shields.io/github/stars/ztxz16/fastllm?style=social) : 纯c++实现，无第三方依赖的大模型库，支持CUDA加速，目前支持国产大模型ChatGLM-6B，MOSS; 可以在安卓设备上流畅运行ChatGLM-6B。
+ [davidar/eigenGPT](https://github.com/davidar/eigenGPT) ![](https://img.shields.io/github/stars/davidar/eigenGPT?style=social) : Minimal C++ implementation of GPT2.
+ [Tlntin/Qwen-TensorRT-LLM](https://github.com/Tlntin/Qwen-TensorRT-LLM) ![](https://img.shields.io/github/stars/Tlntin/Qwen-TensorRT-LLM?style=social) : 使用TRT-LLM完成对Qwen-7B-Chat实现推理加速。
+ [FeiGeChuanShu/trt2023](https://github.com/FeiGeChuanShu/trt2023) ![](https://img.shields.io/github/stars/FeiGeChuanShu/trt2023?style=social) : NVIDIA TensorRT Hackathon 2023复赛选题：通义千问Qwen-7B用TensorRT-LLM模型搭建及优化。
+ [TRT2022/trtllm-llama](https://github.com/TRT2022/trtllm-llama) ![](https://img.shields.io/github/stars/TRT2022/trtllm-llama?style=social) : ☢️ TensorRT 2023复赛——基于TensorRT-LLM的Llama模型推断加速优化。
+ [AmeyaWagh/llama2.cpp](https://github.com/AmeyaWagh/llama2.cpp) ![](https://img.shields.io/github/stars/AmeyaWagh/llama2.cpp?style=social) : Inference Llama 2 in C++.

### Python Implementation
+ [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) ![](https://img.shields.io/github/stars/abetlen/llama-cpp-python?style=social) : Python bindings for llama.cpp. [llama-cpp-python.readthedocs.io](https://llama-cpp-python.readthedocs.io/)
+ [ggml-python](https://github.com/abetlen/ggml-python) ![](https://img.shields.io/github/stars/abetlen/ggml-python?style=social) : Python bindings for ggml. [ggml-python.readthedocs.io](https://ggml-python.readthedocs.io/)

### Mojo Implementation
+ [llama2.mojo](https://github.com/tairov/llama2.mojo) ![](https://img.shields.io/github/stars/tairov/llama2.mojo?style=social) : Inference Llama 2 in one file of pure 🔥
+ [dorjeduck/llm.mojo](https://github.com/dorjeduck/llm.mojo) ![](https://img.shields.io/github/stars/dorjeduck/llm.mojo?style=social) : port of Andrjey Karpathy's llm.c to Mojo.

### Rust Implementation
+ [Candle](https://github.com/huggingface/candle) ![](https://img.shields.io/github/stars/huggingface/candle?style=social) : Minimalist ML framework for Rust.
+ [Safetensors](https://github.com/huggingface/safetensors) ![](https://img.shields.io/github/stars/huggingface/safetensors?style=social) : Simple, safe way to store and distribute tensors. [huggingface.co/docs/safetensors](https://huggingface.co/docs/safetensors/index)
+ [Tokenizers](https://github.com/huggingface/tokenizers) ![](https://img.shields.io/github/stars/huggingface/tokenizers?style=social) : 💥 Fast State-of-the-Art Tokenizers optimized for Research and Production. [huggingface.co/docs/tokenizers](https://huggingface.co/docs/tokenizers/index)
+ [Burn](https://github.com/burn-rs/burn) ![](https://img.shields.io/github/stars/burn-rs/burn?style=social) : Burn - A Flexible and Comprehensive Deep Learning Framework in Rust. [burn-rs.github.io/](https://burn-rs.github.io/)
+ [dfdx](https://github.com/coreylowman/dfdx) ![](https://img.shields.io/github/stars/coreylowman/dfdx?style=social) : Deep learning in Rust, with shape checked tensors and neural networks.
+ [luminal](https://github.com/jafioti/luminal) ![](https://img.shields.io/github/stars/jafioti/luminal?style=social) : Deep learning at the speed of light. [www.luminalai.com/](https://www.luminalai.com/)
+ [crabml](https://github.com/crabml/crabml) ![](https://img.shields.io/github/stars/crabml/crabml?style=social) : crabml is focusing on the reimplementation of GGML using the Rust programming language.
+ [TensorFlow Rust](https://github.com/tensorflow/rust) ![](https://img.shields.io/github/stars/tensorflow/rust?style=social) : Rust language bindings for TensorFlow.
+ [tch-rs](https://github.com/LaurentMazare/tch-rs) ![](https://img.shields.io/github/stars/LaurentMazare/tch-rs?style=social) : Rust bindings for the C++ api of PyTorch.
+ [rustai-solutions/candle_demo_openchat_35](https://github.com/rustai-solutions/candle_demo_openchat_35) ![](https://img.shields.io/github/stars/rustai-solutions/candle_demo_openchat_35?style=social) : candle_demo_openchat_35.
+ [llama2.rs](https://github.com/srush/llama2.rs) ![](https://img.shields.io/github/stars/srush/llama2.rs?style=social) : A fast llama2 decoder in pure Rust.
+ [Llama2-burn](https://github.com/Gadersd/llama2-burn) ![](https://img.shields.io/github/stars/Gadersd/llama2-burn?style=social) : Llama2 LLM ported to Rust burn.
+ [gaxler/llama2.rs](https://github.com/gaxler/llama2.rs) ![](https://img.shields.io/github/stars/gaxler/llama2.rs?style=social) : Inference Llama 2 in one file of pure Rust 🦀
+ [whisper-burn](https://github.com/Gadersd/whisper-burn) ![](https://img.shields.io/github/stars/Gadersd/whisper-burn?style=social) : A Rust implementation of OpenAI's Whisper model using the burn framework.
+ [stable-diffusion-burn](https://github.com/Gadersd/stable-diffusion-burn) ![](https://img.shields.io/github/stars/Gadersd/stable-diffusion-burn?style=social) : Stable Diffusion v1.4 ported to Rust's burn framework.
+ [coreylowman/llama-dfdx](https://github.com/coreylowman/llama-dfdx) ![](https://img.shields.io/github/stars/coreylowman/llama-dfdx?style=social) : [LLaMa 7b](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) with CUDA acceleration implemented in rust. Minimal GPU memory needed!
+ [tazz4843/whisper-rs](https://github.com/tazz4843/whisper-rs) ![](https://img.shields.io/github/stars/tazz4843/whisper-rs?style=social) : Rust bindings to [whisper.cpp](https://github.com/ggerganov/whisper.cpp).
+ [rustformers/llm](https://github.com/rustformers/llm) ![](https://img.shields.io/github/stars/rustformers/llm?style=social) : Run inference for Large Language Models on CPU, with Rust 🦀🚀🦙.
+ [Chidori](https://github.com/ThousandBirdsInc/chidori) ![](https://img.shields.io/github/stars/ThousandBirdsInc/chidori?style=social) : A reactive runtime for building durable AI agents. [docs.thousandbirds.ai](https://docs.thousandbirds.ai/).
+ [llm-chain](https://github.com/sobelio/llm-chain) ![](https://img.shields.io/github/stars/sobelio/llm-chain?style=social) : llm-chain is a collection of Rust crates designed to help you work with Large Language Models (LLMs) more effectively. [llm-chain.xyz](https://llm-chain.xyz/)
+ [Abraxas-365/langchain-rust](https://github.com/Abraxas-365/langchain-rust) ![](https://img.shields.io/github/stars/Abraxas-365/langchain-rust?style=social) : 🦜️🔗LangChain for Rust, the easiest way to write LLM-based programs in Rust.
+ [Atome-FE/llama-node](https://github.com/Atome-FE/llama-node) ![](https://img.shields.io/github/stars/Atome-FE/llama-node?style=social) : Believe in AI democratization. llama for nodejs backed by llama-rs and llama.cpp, work locally on your laptop CPU. support llama/alpaca/gpt4all/vicuna model. [www.npmjs.com/package/llama-node](https://www.npmjs.com/package/llama-node)
+ [Noeda/rllama](https://github.com/Noeda/rllama) ![](https://img.shields.io/github/stars/Noeda/rllama?style=social) : Rust+OpenCL+AVX2 implementation of LLaMA inference code.
+ [lencx/ChatGPT](https://github.com/lencx/ChatGPT) ![](https://img.shields.io/github/stars/lencx/ChatGPT?style=social) : 🔮 ChatGPT Desktop Application (Mac, Windows and Linux). [NoFWL](https://app.nofwl.com/).
+ [Synaptrix/ChatGPT-Desktop](https://github.com/Synaptrix/ChatGPT-Desktop) ![](https://img.shields.io/github/stars/Synaptrix/ChatGPT-Desktop?style=social) : Fuel your productivity with ChatGPT-Desktop - Blazingly fast and supercharged!
+ [Poordeveloper/chatgpt-app](https://github.com/Poordeveloper/chatgpt-app) ![](https://img.shields.io/github/stars/Poordeveloper/chatgpt-app?style=social) : A ChatGPT App for all platforms. Built with Rust + Tauri + Vue + Axum.
+ [mxismean/chatgpt-app](https://github.com/mxismean/chatgpt-app) ![](https://img.shields.io/github/stars/mxismean/chatgpt-app?style=social) : Tauri 项目：ChatGPT App.
+ [sonnylazuardi/chat-ai-desktop](https://github.com/sonnylazuardi/chat-ai-desktop) ![](https://img.shields.io/github/stars/sonnylazuardi/chat-ai-desktop?style=social) : Chat AI Desktop App. Unofficial ChatGPT desktop app for Mac & Windows menubar using Tauri & Rust.
+ [yetone/openai-translator](https://github.com/yetone/openai-translator) ![](https://img.shields.io/github/stars/yetone/openai-translator?style=social) : The translator that does more than just translation - powered by OpenAI.
+ [m1guelpf/browser-agent](https://github.com/m1guelpf/browser-agent) ![](https://img.shields.io/github/stars/m1guelpf/browser-agent?style=social) : A browser AI agent, using GPT-4. [docs.rs/browser-agent](https://docs.rs/browser-agent/latest/browser_agent/)
+ [sigoden/aichat](https://github.com/sigoden/aichat) ![](https://img.shields.io/github/stars/sigoden/aichat?style=social) : Using ChatGPT/GPT-3.5/GPT-4 in the terminal.
+ [uiuifree/rust-openai-chatgpt-api](https://github.com/uiuifree/rust-openai-chatgpt-api) ![](https://img.shields.io/github/stars/uiuifree/rust-openai-chatgpt-api?style=social) : "rust-openai-chatgpt-api" is a Rust library for accessing the ChatGPT API, a powerful NLP platform by OpenAI. The library provides a simple and efficient interface for sending requests and receiving responses, including chat. It uses reqwest and serde for HTTP requests and JSON serialization.
+ [1595901624/gpt-aggregated-edition](https://github.com/1595901624/gpt-aggregated-edition) ![](https://img.shields.io/github/stars/1595901624/gpt-aggregated-edition?style=social) : 聚合ChatGPT官方版、ChatGPT免费版、文心一言、Poe、chatchat等多平台，支持自定义导入平台。
+ [Cormanz/smartgpt](https://github.com/Cormanz/smartgpt) ![](https://img.shields.io/github/stars/Cormanz/smartgpt?style=social) : A program that provides LLMs with the ability to complete complex tasks using plugins.
+ [femtoGPT](https://github.com/keyvank/femtoGPT) ![](https://img.shields.io/github/stars/keyvank/femtoGPT?style=social) : femtoGPT is a pure Rust implementation of a minimal Generative Pretrained Transformer. [discord.gg/wTJFaDVn45](https://github.com/keyvank/femtoGPT)
+ [shafishlabs/llmchain-rs](https://github.com/shafishlabs/llmchain-rs) ![](https://img.shields.io/github/stars/shafishlabs/llmchain-rs?style=social) : 🦀Rust + Large Language Models - Make AI Services Freely and Easily. Inspired by LangChain.
+ [flaneur2020/llama2.rs](https://github.com/flaneur2020/llama2.rs) ![](https://img.shields.io/github/stars/flaneur2020/llama2.rs?style=social) : An rust reimplementatin of [https://github.com/karpathy/llama2.c](https://github.com/karpathy/llama2.c).
+ [Heng30/chatbox](https://github.com/Heng30/chatbox) ![](https://img.shields.io/github/stars/Heng30/chatbox?style=social) : A Chatbot for OpenAI ChatGPT. Based on Slint-ui and Rust.
+ [fairjm/dioxus-openai-qa-gui](https://github.com/fairjm/dioxus-openai-qa-gui) ![](https://img.shields.io/github/stars/fairjm/dioxus-openai-qa-gui?style=social) : a simple openai qa desktop app built with dioxus.
+ [purton-tech/bionicgpt](https://github.com/purton-tech/bionicgpt) ![](https://img.shields.io/github/stars/purton-tech/bionicgpt?style=social) : Accelerate LLM adoption in your organisation. Chat with your confidential data safely and securely. [bionic-gpt.com](https://bionic-gpt.com/)
+ [InfiniTensor/transformer-rs](https://github.com/InfiniTensor/transformer-rs) ![](https://img.shields.io/github/stars/InfiniTensor/transformer-rs?style=social) : 从 [YdrMaster/llama2.rs](https://github.com/YdrMaster/llama2.rs) 发展来的手写 transformer 模型项目。

### Zig Implementation
+ [llama2.zig](https://github.com/cgbur/llama2.zig) ![](https://img.shields.io/github/stars/cgbur/llama2.zig?style=social) : Inference Llama 2 in one file of pure Zig.
+ [renerocksai/gpt4all.zig](https://github.com/renerocksai/gpt4all.zig) ![](https://img.shields.io/github/stars/renerocksai/gpt4all.zig?style=social) : ZIG build for a terminal-based chat client for an assistant-style large language model with ~800k GPT-3.5-Turbo Generations based on LLaMa.
+ [EugenHotaj/zig_inference](https://github.com/EugenHotaj/zig_inference) ![](https://img.shields.io/github/stars/EugenHotaj/zig_inference?style=social) : Neural Network Inference Engine in Zig.

### Go Implementation
+ [Ollama](https://github.com/ollama/ollama/) ![](https://img.shields.io/github/stars/ollama/ollama?style=social) : Get up and running with Llama 2, Mistral, Gemma, and other large language models. [ollama.com](https://ollama.com/)
+ [go-skynet/LocalAI](https://github.com/go-skynet/LocalAI) ![](https://img.shields.io/github/stars/go-skynet/LocalAI?style=social) : 🤖 Self-hosted, community-driven, local OpenAI-compatible API. Drop-in replacement for OpenAI running LLMs on consumer-grade hardware. Free Open Source OpenAI alternative. No GPU required. LocalAI is an API to run ggml compatible models: llama, gpt4all, rwkv, whisper, vicuna, koala, gpt4all-j, cerebras, falcon, dolly, starcoder, and many other. [localai.io](https://localai.io/)

## ❇️Vector Database 向量数据库
+ [Milvus](https://github.com/milvus-io/milvus) ![](https://img.shields.io/github/stars/milvus-io/milvus?style=social) : <font style="background-color:#FBDE28;">Milvus is an open-source vector database built to power embedding similarity search and AI applications. Milvus makes unstructured data search more accessible, and provides a consistent user experience regardless of the deployment environment. </font>[milvus.io](https://milvus.io/)
+ [Qdrant](https://github.com/qdrant/qdrant) ![](https://img.shields.io/github/stars/qdrant/qdrant?style=social) : Qdrant - Vector Database for the next generation of AI applications. Also available in the cloud [https://cloud.qdrant.io/](https://cloud.qdrant.io/). [qdrant.tech](https://qdrant.tech/)

## 模型的私有化部署
地址：[https://github.com/gptlink/gptlink-deploy/blob/master/README.md](https://github.com/gptlink/gptlink-deploy/blob/master/README.md)

