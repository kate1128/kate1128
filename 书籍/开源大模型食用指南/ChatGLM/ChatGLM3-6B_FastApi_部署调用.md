# <font style="color:rgb(31, 35, 40);">ç¯å¢ƒå‡†å¤‡</font>
<font style="color:rgb(31, 35, 40);">åœ¨</font>[autodl](https://www.autodl.com/)<font style="color:rgb(31, 35, 40);">å¹³å°ç§Ÿæœºå™¨ï¼Œpipæ¢æºå’Œå®‰è£…ä¾èµ–åŒ…ï¼š</font>

```python
# å‡çº§pip
python -m pip install --upgrade pip
# æ›´æ¢ pypi æºåŠ é€Ÿåº“çš„å®‰è£…
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

pip install fastapi==0.104.1
pip install uvicorn==0.24.0.post1
pip install requests==2.25.1
pip install modelscope==1.9.5
pip install transformers==4.37.2
pip install streamlit==1.24.0
pip install sentencepiece==0.1.99
pip install accelerate==0.24.1
```

# <font style="color:rgb(31, 35, 40);">æ¨¡å‹ä¸‹è½½</font>
<font style="color:rgb(31, 35, 40);">ä½¿ç”¨</font><font style="color:rgb(31, 35, 40);"> </font>`<font style="color:rgb(31, 35, 40);">modelscope</font>`<font style="color:rgb(31, 35, 40);"> </font><font style="color:rgb(31, 35, 40);">ä¸­çš„</font>`<font style="color:rgb(31, 35, 40);">snapshot_download</font>`<font style="color:rgb(31, 35, 40);">å‡½æ•°ä¸‹è½½æ¨¡å‹ï¼Œç¬¬ä¸€ä¸ªå‚æ•°ä¸ºæ¨¡å‹åç§°ï¼Œå‚æ•°</font>`<font style="color:rgb(31, 35, 40);">cache_dir</font>`<font style="color:rgb(31, 35, 40);">ä¸ºæ¨¡å‹çš„ä¸‹è½½è·¯å¾„ã€‚</font>

<font style="color:rgb(31, 35, 40);">åœ¨</font><font style="color:rgb(31, 35, 40);"> </font>`<font style="color:rgb(31, 35, 40);">/root/autodl-tmp</font>`<font style="color:rgb(31, 35, 40);"> </font><font style="color:rgb(31, 35, 40);">è·¯å¾„ä¸‹æ–°å»º</font><font style="color:rgb(31, 35, 40);"> </font>`<font style="color:rgb(31, 35, 40);">download.py</font>`<font style="color:rgb(31, 35, 40);"> </font><font style="color:rgb(31, 35, 40);">æ–‡ä»¶å¹¶åœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹ï¼Œç²˜è´´ä»£ç åè®°å¾—ä¿å­˜æ–‡ä»¶ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚å¹¶è¿è¡Œ</font><font style="color:rgb(31, 35, 40);"> </font>`<font style="color:rgb(31, 35, 40);">python /root/autodl-tmp/download.py</font>`<font style="color:rgb(31, 35, 40);">æ‰§è¡Œä¸‹è½½ï¼Œæ¨¡å‹å¤§å°ä¸º 14 GBï¼Œä¸‹è½½æ¨¡å‹å¤§æ¦‚éœ€è¦ 10~20 åˆ†é’Ÿ</font>

```python
import torch
from modelscope import snapshot_download, AutoModel, AutoTokenizer
import os
model_dir = snapshot_download('ZhipuAI/chatglm3-6b', cache_dir='/root/autodl-tmp', revision='master')
```

# <font style="color:rgb(31, 35, 40);">ä»£ç å‡†å¤‡</font>
<font style="color:rgb(31, 35, 40);">åœ¨</font>`<font style="color:rgb(31, 35, 40);">/root/autodl-tmp</font>`<font style="color:rgb(31, 35, 40);">è·¯å¾„ä¸‹æ–°å»º</font>`<font style="color:rgb(31, 35, 40);">api.py</font>`<font style="color:rgb(31, 35, 40);">æ–‡ä»¶å¹¶åœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹ï¼š</font>

```python
from fastapi import FastAPI, Request
from transformers import AutoTokenizer, AutoModelForCausalLM
import uvicorn
import json
import datetime
import torch

# è®¾ç½®è®¾å¤‡å‚æ•°
DEVICE = "cuda"  # ä½¿ç”¨CUDA
DEVICE_ID = "0"  # CUDAè®¾å¤‡IDï¼Œå¦‚æœæœªè®¾ç½®åˆ™ä¸ºç©º
CUDA_DEVICE = f"{DEVICE}:{DEVICE_ID}" if DEVICE_ID else DEVICE  # ç»„åˆCUDAè®¾å¤‡ä¿¡æ¯

# æ¸…ç†GPUå†…å­˜å‡½æ•°
def torch_gc():
    if torch.cuda.is_available():  # æ£€æŸ¥æ˜¯å¦å¯ç”¨CUDA
        with torch.cuda.device(CUDA_DEVICE):  # æŒ‡å®šCUDAè®¾å¤‡
            torch.cuda.empty_cache()  # æ¸…ç©ºCUDAç¼“å­˜
            torch.cuda.ipc_collect()  # æ”¶é›†CUDAå†…å­˜ç¢ç‰‡

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI()

# å¤„ç†POSTè¯·æ±‚çš„ç«¯ç‚¹
@app.post("/")
async def create_item(request: Request):
    global model, tokenizer  # å£°æ˜å…¨å±€å˜é‡ä»¥ä¾¿åœ¨å‡½æ•°å†…éƒ¨ä½¿ç”¨æ¨¡å‹å’Œåˆ†è¯å™¨
    json_post_raw = await request.json()  # è·å–POSTè¯·æ±‚çš„JSONæ•°æ®
    json_post = json.dumps(json_post_raw)  # å°†JSONæ•°æ®è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    json_post_list = json.loads(json_post)  # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºPythonå¯¹è±¡
    prompt = json_post_list.get('prompt')  # è·å–è¯·æ±‚ä¸­çš„æç¤º
    history = json_post_list.get('history')  # è·å–è¯·æ±‚ä¸­çš„å†å²è®°å½•
    max_length = json_post_list.get('max_length')  # è·å–è¯·æ±‚ä¸­çš„æœ€å¤§é•¿åº¦
    top_p = json_post_list.get('top_p')  # è·å–è¯·æ±‚ä¸­çš„top_på‚æ•°
    temperature = json_post_list.get('temperature')  # è·å–è¯·æ±‚ä¸­çš„æ¸©åº¦å‚æ•°
    
    # è°ƒç”¨æ¨¡å‹è¿›è¡Œå¯¹è¯ç”Ÿæˆ
    response, history = model.chat(
        tokenizer,
        prompt,
        history=history,
        max_length=max_length if max_length else 2048,  # å¦‚æœæœªæä¾›æœ€å¤§é•¿åº¦ï¼Œé»˜è®¤ä½¿ç”¨2048
        top_p=top_p if top_p else 0.7,  # å¦‚æœæœªæä¾›top_på‚æ•°ï¼Œé»˜è®¤ä½¿ç”¨0.7
        temperature=temperature if temperature else 0.95  # å¦‚æœæœªæä¾›æ¸©åº¦å‚æ•°ï¼Œé»˜è®¤ä½¿ç”¨0.95
    )
    now = datetime.datetime.now()  # è·å–å½“å‰æ—¶é—´
    time = now.strftime("%Y-%m-%d %H:%M:%S")  # æ ¼å¼åŒ–æ—¶é—´ä¸ºå­—ç¬¦ä¸²
    
    # æ„å»ºå“åº”JSON
    answer = {
        "response": response,
        "history": history,
        "status": 200,
        "time": time
    }
    
    # æ„å»ºæ—¥å¿—ä¿¡æ¯
    log = "[" + time + "] " + '", prompt:"' + prompt + '", response:"' + repr(response) + '"'
    print(log)  # æ‰“å°æ—¥å¿—
    torch_gc()  # æ‰§è¡ŒGPUå†…å­˜æ¸…ç†
    return answer  # è¿”å›å“åº”

# ä¸»å‡½æ•°å…¥å£
if __name__ == '__main__':
    # åŠ è½½é¢„è®­ç»ƒçš„åˆ†è¯å™¨å’Œæ¨¡å‹
    tokenizer = AutoTokenizer.from_pretrained("/root/autodl-tmp/ZhipuAI/chatglm3-6b", trust_remote_code=True)
    
    model = AutoModelForCausalLM.from_pretrained("/root/autodl-tmp/ZhipuAI/chatglm3-6b", trust_remote_code=True).to(torch.bfloat16).cuda()
    
    model.eval()  # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
    
    # å¯åŠ¨FastAPIåº”ç”¨
    # ç”¨6006ç«¯å£å¯ä»¥å°†autodlçš„ç«¯å£æ˜ å°„åˆ°æœ¬åœ°ï¼Œä»è€Œåœ¨æœ¬åœ°ä½¿ç”¨api
    uvicorn.run(app, host='0.0.0.0', port=6006, workers=1)  # åœ¨æŒ‡å®šç«¯å£å’Œä¸»æœºä¸Šå¯åŠ¨åº”ç”¨
```

# <font style="color:rgb(31, 35, 40);">Api éƒ¨ç½²</font>
<font style="color:rgb(31, 35, 40);">åœ¨ç»ˆç«¯è¾“å…¥ä»¥ä¸‹å‘½ä»¤å¯åŠ¨</font>`<font style="color:rgb(31, 35, 40);">api</font>`<font style="color:rgb(31, 35, 40);">æœåŠ¡</font>

```python
cd /root/autodl-tmp
python api.py
```

<font style="color:rgb(31, 35, 40);">é»˜è®¤éƒ¨ç½²åœ¨ 6006 ç«¯å£ï¼Œé€šè¿‡ POST æ–¹æ³•è¿›è¡Œè°ƒç”¨ï¼Œå¯ä»¥ä½¿ç”¨</font>`<font style="color:rgb(31, 35, 40);">curl</font>`<font style="color:rgb(31, 35, 40);">è°ƒç”¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š</font>

```python
curl -X POST "http://127.0.0.1:6006" \
-H 'Content-Type: application/json' \
-d '{"prompt": "ä½ å¥½", "history": []}'
```

<font style="color:rgb(31, 35, 40);">è°ƒç”¨ç¤ºä¾‹ç»“æœå¦‚ä¸‹å›¾æ‰€ç¤º</font>

![](https://cdn.nlark.com/yuque/0/2025/png/2639475/1735717701539-9a9f5a94-3a20-4b66-88fc-e51f1e6b4d87.png)

<font style="color:rgb(31, 35, 40);">ä¹Ÿå¯ä»¥ä½¿ç”¨</font>`<font style="color:rgb(31, 35, 40);">python</font>`<font style="color:rgb(31, 35, 40);">ä¸­çš„</font>`<font style="color:rgb(31, 35, 40);">requests</font>`<font style="color:rgb(31, 35, 40);">åº“è¿›è¡Œè°ƒç”¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š</font>

```python
import requests
import json

def get_completion(prompt):
    headers = {'Content-Type': 'application/json'}
    data = {"prompt": prompt, "history": []}
    response = requests.post(url='http://127.0.0.1:6006', headers=headers, data=json.dumps(data))
    return response.json()['response']

if __name__ == '__main__':
    print(get_completion('ä½ å¥½'))
```

<font style="color:rgb(31, 35, 40);">å¾—åˆ°çš„è¿”å›å€¼å¦‚ä¸‹æ‰€ç¤ºï¼š</font>

```python
{
    "response": "ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM3-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚",
    "history": [{"role": "user", "content": "ä½ å¥½"}, {"role": "assistant", "metadata": "", "content": "ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM3-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚"}],
    "status": 200,
    "time": "2023-11-28 11:16:06"
}
```

<font style="color:rgb(31, 35, 40);">è°ƒç”¨ç»“æœå¦‚ä¸‹å›¾æ‰€ç¤º</font>

![](https://cdn.nlark.com/yuque/0/2025/png/2639475/1735717724709-07464527-572a-49c2-a87f-2b0218f6497a.png)

