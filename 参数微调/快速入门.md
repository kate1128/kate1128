> ç”¨ hugging face åšå‚æ•°å¾®è°ƒ
>

[GitHub - huggingface/peft: ğŸ¤— PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.](https://github.com/huggingface/peft)

<font style="background-color:rgba(255, 255, 255, 0);">Install PEFT from pip:</font>

```python
pip install peft
```

<font style="background-color:rgba(255, 255, 255, 0);">Prepare a model for training with a PEFT method such as LoRA by wrapping the base model and PEFT configuration with</font><font style="background-color:rgba(255, 255, 255, 0);"> </font>`<font style="background-color:rgba(255, 255, 255, 0);">get_peft_model</font>`<font style="background-color:rgba(255, 255, 255, 0);">. For the bigscience/mt0-large model, you're only training 0.19% of the parameters!</font>

```python
from transformers import AutoModelForSeq2SeqLM
from peft import get_peft_config, get_peft_model, LoraConfig, TaskType
model_name_or_path = "bigscience/mt0-large"
tokenizer_name_or_path = "bigscience/mt0-large"

peft_config = LoraConfig(
    task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1
)

model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()
"trainable params: 2359296 || all params: 1231940608 || trainable%: 0.19151053100118282"
```

<font style="background-color:rgba(255, 255, 255, 0);">To load a PEFT model for inference:</font>

```python
from peft import AutoPeftModelForCausalLM
from transformers import AutoTokenizer
import torch

model = AutoPeftModelForCausalLM.from_pretrained("ybelkada/opt-350m-lora").to("cuda")
tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")

model.eval()
inputs = tokenizer("Preheat the oven to 350 degrees and place the cookie dough", return_tensors="pt")

outputs = model.generate(input_ids=inputs["input_ids"].to("cuda"), max_new_tokens=50)
print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])

"Preheat the oven to 350 degrees and place the cookie dough in the center of the oven. In a large bowl, combine the flour, baking powder, baking soda, salt, and cinnamon. In a separate bowl, combine the egg yolks, sugar, and vanilla."
```

<font style="background-color:rgba(255, 255, 255, 0);">  
</font>

---

åœ¨ Hugging Face ä¸Šè¿›è¡Œ Parameter-efficient Fine-Tuning (PEFT) æ˜¯ä¸€ç§åœ¨ä¸éœ€è¦è°ƒæ•´æ•´ä¸ªæ¨¡å‹çš„å‚æ•°æƒ…å†µä¸‹è¿›è¡Œå¾®è°ƒçš„æŠ€æœ¯ã€‚PEFT æ—¨åœ¨é€šè¿‡å°‘é‡çš„å‚æ•°æ›´æ–°æ¥ä¼˜åŒ–æ¨¡å‹ï¼ŒåŒæ—¶å°½é‡å‡å°‘è®¡ç®—å’Œå­˜å‚¨å¼€é”€ã€‚æœ€å¸¸è§çš„ PEFT æŠ€æœ¯åŒ…æ‹¬ LoRA (Low-Rank Adaptation)ã€Adapterã€Prompt Tuning å’Œ Prefix Tuning ç­‰ã€‚

åœ¨ Hugging Face ä¸­ï¼ŒPEFT å¯ä»¥é€šè¿‡å‡ ä¸ªä¸åŒçš„åº“å’Œå·¥å…·æ¥å®ç°ï¼Œæ¯”å¦‚ `peft` åº“ï¼Œæˆ–è€…é€šè¿‡ `transformers` åº“ç»“åˆè‡ªå®šä¹‰çš„ PEFT æ–¹æ³•ã€‚

### 1. å®‰è£…å¿…è¦çš„åº“
é¦–å…ˆï¼Œç¡®ä¿ä½ å·²ç»å®‰è£…äº† Hugging Face çš„ `transformers` åº“å’Œ `peft` åº“ï¼š

```bash
pip install transformers
pip install peft
```

### 2. ä½¿ç”¨ LoRAï¼ˆLow-Rank Adaptationï¼‰
LoRA æ˜¯ PEFT ä¸­ä¸€ç§éå¸¸æµè¡Œçš„æŠ€æœ¯ï¼Œå®ƒé€šè¿‡å¼•å…¥ä½ç§©çŸ©é˜µæ¥åœ¨æ¯ä¸€å±‚ä¸­ä¿®æ”¹æƒé‡çŸ©é˜µï¼Œä»è€Œåœ¨ä¸æ›´æ–°å¤§é‡å‚æ•°çš„æƒ…å†µä¸‹å®Œæˆå¾®è°ƒã€‚

#### 2.1 åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
ä½¿ç”¨ Hugging Face çš„ `transformers` åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ˆæ¯”å¦‚ GPT æˆ– BERTï¼‰ï¼š

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer

# ä»¥ BERT ä¸ºä¾‹
model_name = "bert-base-uncased"
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)
tokenizer = AutoTokenizer.from_pretrained(model_name)
```

#### 2.2 é…ç½® LoRA
åœ¨ `peft` åº“ä¸­ï¼Œä½ å¯ä»¥é€šè¿‡ `LoraConfig` å’Œ `get_peft_model` æ¥é…ç½® LoRA å‚æ•°ã€‚

```python
from peft import LoraConfig, get_peft_model

# é…ç½® LoRA å‚æ•°
lora_config = LoraConfig(
    r=8,  # ä½ç§©çŸ©é˜µçš„ç§©
    lora_alpha=32,  # LoRA æŸå¤±ç³»æ•°
    lora_dropout=0.1,  # Dropout æ¯”ä¾‹
    bias="none",  # æ§åˆ¶æ˜¯å¦è°ƒæ•´åç½®
)

# é€šè¿‡ LoRA è¿›è¡Œå¾®è°ƒ
peft_model = get_peft_model(model, lora_config)
```

#### 2.3 å¾®è°ƒæ¨¡å‹
ç°åœ¨ï¼Œä½ å¯ä»¥åƒé€šå¸¸ä¸€æ ·è¿›è¡Œæ¨¡å‹å¾®è°ƒã€‚ä½ åªéœ€è¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¾®è°ƒ LoRA å‚æ•°ï¼Œè€Œæ— éœ€è°ƒæ•´æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚

```python
from transformers import Trainer, TrainingArguments

# é…ç½®è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    num_train_epochs=3,
    logging_dir="./logs",
    logging_steps=10,
)

# å®šä¹‰ Trainer
trainer = Trainer(
    model=peft_model,
    args=training_args,
    train_dataset=train_dataset,  # éœ€è¦å‡†å¤‡å¥½çš„è®­ç»ƒæ•°æ®é›†
    eval_dataset=eval_dataset,    # éœ€è¦å‡†å¤‡å¥½çš„éªŒè¯æ•°æ®é›†
)

# å¼€å§‹è®­ç»ƒ
trainer.train()
```

#### 2.4 ä¿å­˜æ¨¡å‹
è®­ç»ƒå®Œæˆåï¼Œä½ å¯ä»¥ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹ï¼š

```python
peft_model.save_pretrained("./peft_model")
```

### 3. ä½¿ç”¨å…¶ä»– PEFT æŠ€æœ¯
é™¤äº† LoRAï¼Œä½ è¿˜å¯ä»¥ä½¿ç”¨ Adapterã€Prompt Tuning ç­‰å…¶ä»– PEFT æŠ€æœ¯ã€‚åœ¨ Hugging Face ä¸­ï¼Œé€šå¸¸æ˜¯é€šè¿‡é…ç½®ä¸åŒçš„ `peft` åº“æˆ–è€… `transformers` é…ç½®æ¥å®ç°ã€‚

ä¾‹å¦‚ï¼Œä½¿ç”¨ Adapterï¼š

```python
from transformers import AdapterConfig

# é…ç½® Adapter
adapter_config = AdapterConfig.load("pfeiffer")

# å°† Adapter æ·»åŠ åˆ°æ¨¡å‹ä¸­
model.add_adapter("my_adapter", config=adapter_config)
model.train_adapter("my_adapter")
```

### æ€»ç»“
é€šè¿‡ Hugging Face è¿›è¡Œ PEFT å¾®è°ƒï¼Œå…³é”®æ˜¯é€‰æ‹©åˆé€‚çš„æŠ€æœ¯ï¼ˆä¾‹å¦‚ LoRA æˆ– Adapterï¼‰ï¼Œç„¶ååŠ è½½é¢„è®­ç»ƒæ¨¡å‹å¹¶é…ç½®ç›¸åº”çš„ PEFT å‚æ•°ã€‚å¾®è°ƒè¿‡ç¨‹ä¸­çš„å¤§éƒ¨åˆ†è®¡ç®—åªä¼šæ¶‰åŠè¿™äº›é¢å¤–çš„å‚æ•°ï¼Œè€Œä¸éœ€è¦å¯¹åŸå§‹æ¨¡å‹çš„æ‰€æœ‰å‚æ•°è¿›è¡Œè°ƒæ•´ï¼Œä»è€Œæé«˜äº†è®­ç»ƒæ•ˆç‡å’ŒèŠ‚çœäº†èµ„æºã€‚

