> 1. **<font style="color:rgb(0, 0, 0);">æ–‡æ¡£å‡†å¤‡ï¼š</font>**`**<font style="color:rgb(0, 0, 0);">from llama_index.node_parser import SentenceWindowNodeParser</font>**`
> 2. **<font style="color:rgb(0, 0, 0);">æ„å»ºç´¢å¼•ï¼š</font>**`**<font style="color:rgb(0, 0, 0);">from llama_index import VectorStoreIndex</font>**`
> 3. **<font style="color:rgb(0, 0, 0);">å¢è®¾é‡æ–°æ’åºå—ï¼š</font>**
>
> `from llama_index.indices.postprocessor import MetadataReplacementPostProcessor`
>
> `from llama_index.indices.postprocessor import SentenceTransformerRerank`
>
> 4. **<font style="color:rgb(0, 0, 0);">æ£€ç´¢ç»“æœï¼š</font>**`**<font style="color:rgb(0, 0, 0);">from llama_index.response.notebook_utils import display_response</font>**`
>

[åŸºæœ¬ç¯å¢ƒé…ç½®](https://www.yuque.com/qiaokate/su87gb/kwr2le446w124rlu)

##  å¥å­æ»‘çª—æ£€ç´¢è®¾ç½®
åˆ›å»ºäº†ä¸€ä¸ªåä¸º `node_parser` çš„è§£æå™¨å¯¹è±¡ï¼ŒæŒ‡å®šäº†çª—å£å¤§å°ä¸º3ï¼ŒåŸå§‹æ–‡æœ¬å…ƒæ•°æ®é”®è¢«è®¾ç½®ä¸º`original_text`ã€‚è¿™æ ·åˆ›å»ºçš„è§£æå™¨å¯ä»¥ç”¨äºä»æ–‡æœ¬ä¸­æå–èŠ‚ç‚¹

```python
from llama_index.node_parser import SentenceWindowNodeParser

# create the sentence window node parser w/ default settings
node_parser = SentenceWindowNodeParser.from_defaults(
    window_size=3,
    window_metadata_key="window",
    original_text_metadata_key="original_text",
)
```

å®šä¹‰ä¸€ä¸ªä¸­æ–‡æ–‡æœ¬å­—ç¬¦ä¸²ï¼Œä½¿ç”¨ `node_parser` çš„ `get_nodes_from_documents` æ–¹æ³•ä»æä¾›çš„æ–‡æœ¬ä¸­æå–èŠ‚ç‚¹ã€‚

```python
text = "ä½ å¥½. ä½ æ€ä¹ˆæ ·? æˆ‘å¾ˆå¥½!  "

nodes = node_parser.get_nodes_from_documents([Document(text=text)])
```

æ¯ä¸ªå•ç‹¬çš„è¯

```python
print([x.text for x in nodes])
```

```python
['ä½ å¥½. ', 'ä½ æ€ä¹ˆæ ·? ', 'æˆ‘å¾ˆå¥½!  ']
```

åŸæ•´å¥

```python
print(nodes[1].metadata["window"])
```

```python
ä½ å¥½.  ä½ æ€ä¹ˆæ ·?  æˆ‘å¾ˆå¥½!  
```

### åˆ›å»ºç´¢å¼•
ä½¿ç”¨ `OpenAI` çš„ `GPT-3.5-turbo` æ¨¡å‹åˆ›å»ºäº†ä¸€ä¸ªè¯­è¨€æ¨¡å‹çš„å®ä¾‹ï¼Œè®¾ç½®äº†æ¸©åº¦å‚æ•°ä¸º0.1ã€‚

```python
from llama_index.llms import OpenAI

llm = OpenAI(model="gpt-3.5-turbo", temperature=0.1)
```

ä½¿ç”¨ `ServiceContext.from_defaults` æ–¹æ³•åˆ›å»ºäº†ä¸€ä¸ª `ServiceContext` å¯¹è±¡ï¼Œè¯¥å¯¹è±¡åŒ…å«äº†ç”¨äºç´¢å¼•æ„å»ºçš„æœåŠ¡ç›¸å…³çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒåŒ…æ‹¬è¯­è¨€æ¨¡å‹ã€åµŒå…¥æ¨¡å‹ä»¥åŠèŠ‚ç‚¹è§£æå™¨ã€‚

```python
from llama_index import ServiceContext

sentence_context = ServiceContext.from_defaults(
    llm=llm,
    embed_model="local:BAAI/bge-small-zh-v1.5",
    node_parser=node_parser,
)
```

ä½¿ç”¨ `VectorStoreIndex.from_documents` æ–¹æ³•åˆ›å»ºäº†ä¸€ä¸ª `VectorStoreIndex` å¯¹è±¡ï¼Œè¯¥å¯¹è±¡ç”¨äºå­˜å‚¨å’Œæ£€ç´¢ä¸æ–‡æ¡£ç›¸å…³çš„å‘é‡ä¿¡æ¯ã€‚

```python
from llama_index import VectorStoreIndex

# æ„å»ºç´¢å¼•
sentence_index = VectorStoreIndex.from_documents(
    [document], service_context=sentence_context
)
```

å°†åˆ›å»ºçš„ç´¢å¼•æŒä¹…åŒ–åˆ°æŒ‡å®šç›®å½•`ï¼ˆ"./sentence_index"ï¼‰`ã€‚è¿™æ ·åšå¯ä»¥åœ¨ä¹‹åçš„è¿è¡Œä¸­é‡æ–°åŠ è½½ç´¢å¼•ï¼Œè€Œä¸å¿…é‡æ–°æ„å»ºã€‚

```python
sentence_index.storage_context.persist(persist_dir="./sentence_index")
```

æ£€æŸ¥ç´¢å¼•æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™é‡æ–°æ„å»ºï¼Œå¦‚æœå­˜åœ¨ï¼Œå®ƒå°†ä½¿ç”¨ `load_index_from_storage` æ–¹æ³•ä»å·²æœ‰çš„ç´¢å¼•æ–‡ä»¶ä¸­åŠ è½½ç´¢å¼•ï¼Œè€Œä¸æ˜¯é‡æ–°æ„å»ºã€‚

```python
# This block of code is optional to check
# if an index file exist, then it will load it
# if not, it will rebuild it

import os
from llama_index import VectorStoreIndex, StorageContext, load_index_from_storage
from llama_index import load_index_from_storage

if not os.path.exists("./sentence_index"):
    sentence_index = VectorStoreIndex.from_documents(
        [document], service_context=sentence_context
    )

    sentence_index.storage_context.persist(persist_dir="./sentence_index")
else:
    sentence_index = load_index_from_storage(
        StorageContext.from_defaults(persist_dir="./sentence_index"),
        service_context=sentence_context
    )
```

### åˆ›å»ºåå¤„ç†
ä½¿ç”¨ `MetadataReplacementPostProcessor` ç±»åˆ›å»ºäº†ä¸€ä¸ªåå¤„ç†å™¨å®ä¾‹ï¼Œè®¾ç½®äº†ç›®æ ‡å…ƒæ•°æ®é”®ä¸º `window`ã€‚è¯¥åå¤„ç†å™¨çš„ä½œç”¨æ˜¯æ›¿æ¢ç›®æ ‡å…ƒæ•°æ®é”®çš„å†…å®¹ã€‚

```python
from llama_index.indices.postprocessor import MetadataReplacementPostProcessor

postproc = MetadataReplacementPostProcessor(
    target_metadata_key="window"
)
```

ä½¿ç”¨ `NodeWithScore` ç±»ï¼Œå°†åŸå§‹èŠ‚ç‚¹åˆ—è¡¨ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹ä¸ä¸€ä¸ªåˆ†æ•°å…³è”ï¼Œå½¢æˆå¸¦åˆ†æ•°çš„èŠ‚ç‚¹åˆ—è¡¨ã€‚  
ä½¿ç”¨ `deepcopy` å‡½æ•°åˆ›å»ºäº†åŸå§‹èŠ‚ç‚¹åˆ—è¡¨çš„æ·±åº¦æ‹·è´ï¼Œä»¥ä¾¿åç»­æ¯”è¾ƒã€‚

```python
from llama_index.schema import NodeWithScore
from copy import deepcopy

scored_nodes = [NodeWithScore(node=x, score=1.0) for x in nodes]
nodes_old = [deepcopy(n) for n in nodes]
```

æ‰“å°ï¼š

```python
print(nodes_old[1].text)
```

<details class="lake-collapse"><summary id="u34f20b75"><span class="ne-text">outputï¼š</span></summary><pre data-language="json" id="M0c39" class="ne-codeblock language-json"><code>å§å°</code></pre></details>
ä½¿ç”¨åå¤„ç†å™¨çš„ `postprocess_nodes` æ–¹æ³•ï¼Œæ›¿æ¢äº†å¸¦åˆ†æ•°çš„èŠ‚ç‚¹åˆ—è¡¨ä¸­ç›®æ ‡å…ƒæ•°æ®é”®çš„å†…å®¹ã€‚

```python
replaced_nodes = postproc.postprocess_nodes(scored_nodes)
```

```python
print(replaced_nodes[1].text)
```

```python
ä½ å¥½.  å§å°.  çŒ«ç‹—.  è€é¼ 
```

### å¢è®¾é‡æ–°æ’åºå—
ä½¿ç”¨ `SentenceTransformerRerank` ç±»åˆ›å»ºäº†ä¸€ä¸ªåå¤„ç†å™¨å®ä¾‹ï¼Œè®¾ç½®äº†å‚æ•° `top_n` ä¸º 2ï¼Œä»¥åŠä½¿ç”¨çš„æ¨¡å‹ä¸º "`BAAI/bge-reranker-base`"ã€‚

```python
from llama_index.indices.postprocessor import SentenceTransformerRerank

# BAAI/bge-reranker-base
# link: https://huggingface.co/BAAI/bge-reranker-base
rerank = SentenceTransformerRerank(
    top_n=2, model="BAAI/bge-reranker-base"
)
```

åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«æŸ¥è¯¢æ–‡æœ¬çš„ `QueryBundle` å¯¹è±¡ï¼Œè¯¥æŸ¥è¯¢æ–‡æœ¬ä¸º "æˆ‘æƒ³è¦åªç‹—."ã€‚  
åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªå¸¦åˆ†æ•°çš„èŠ‚ç‚¹çš„åˆ—è¡¨ï¼Œè¿™äº›èŠ‚ç‚¹åˆ†åˆ«è¡¨ç¤ºåŒ…å« "è¿™æ˜¯åªçŒ«" å’Œ "è¿™æ˜¯åªç‹—" æ–‡æœ¬çš„æ–‡æœ¬èŠ‚ç‚¹ï¼Œåˆ†æ•°åˆ†åˆ«ä¸º 0.6 å’Œ 0.4ã€‚

```python
from llama_index import QueryBundle
from llama_index.schema import TextNode, NodeWithScore

query = QueryBundle("æˆ‘æƒ³è¦åªç‹—.")

scored_nodes = [
    NodeWithScore(node=TextNode(text="è¿™æ˜¯åªçŒ«"), score=0.6),
    NodeWithScore(node=TextNode(text="è¿™æ˜¯åªç‹—"), score=0.4),
]
```

ä½¿ç”¨ `SentenceTransformerRerank` ç±»çš„ `postprocess_nodes` æ–¹æ³•ï¼Œå¯¹å¸¦åˆ†æ•°çš„èŠ‚ç‚¹åˆ—è¡¨è¿›è¡Œé‡æ–°æ’åï¼Œè€ƒè™‘åˆ°æŸ¥è¯¢æ–‡æœ¬ã€‚é‡æ–°æ’åçš„èŠ‚ç‚¹å°†åŸºäºé¢„è®­ç»ƒçš„å¥å­è½¬æ¢æ¨¡å‹ã€‚

```python
reranked_nodes = rerank.postprocess_nodes(
    scored_nodes, query_bundle=query
)
```

è¾“å‡ºäº†é‡æ–°æ’ååçš„èŠ‚ç‚¹åˆ—è¡¨ä¸­çš„æ–‡æœ¬å’Œåˆ†æ•°ã€‚è¿™é‡Œå±•ç¤ºäº†å¥å­è½¬æ¢æ¨¡å‹å¯¹èŠ‚ç‚¹é‡æ–°æ’åçš„æ•ˆæœã€‚

```python
print([(x.text, x.score) for x in reranked_nodes])
```

<details class="lake-collapse"><summary id="uffedace1"><span class="ne-text">outputï¼š</span></summary><pre data-language="json" id="Rc77E" class="ne-codeblock language-json"><code>[('è¿™æ˜¯åªç‹—', 0.9660425), ('è¿™æ˜¯åªçŒ«', 0.06396222)]</code></pre></details>
### è¿è¡Œç´¢å¼•å¼•æ“
ä½¿ç”¨ `as_query_engine` æ–¹æ³•å°† `sentence_index` è½¬æ¢ä¸ºæŸ¥è¯¢å¼•æ“å¯¹è±¡ `sentence_window_engine`ã€‚  
åœ¨è¿™é‡Œï¼Œè®¾ç½®äº†ç›¸ä¼¼æ€§`ï¼ˆsimilarityï¼‰`çš„ `top k` ä¸º 6ï¼Œå¹¶ä¼ å…¥äº† `node_postprocessors` å‚æ•°ï¼Œå…¶ä¸­åŒ…å«äº†ä¹‹å‰åˆ›å»ºçš„ `postproc` å’Œ `rerank` åå¤„ç†å™¨ã€‚

```python
sentence_window_engine = sentence_index.as_query_engine(
    similarity_top_k=6, node_postprocessors=[postproc, rerank]
)
```

ä½¿ç”¨æŸ¥è¯¢å¼•æ“çš„ `query` æ–¹æ³•æ‰§è¡Œäº†ä¸€ä¸ªæŸ¥è¯¢ï¼ŒæŸ¥è¯¢çš„å†…å®¹æ˜¯ "åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸå»ºåŠŸç«‹ä¸šçš„å…³é”®æ˜¯ä»€ä¹ˆ?"ã€‚æŸ¥è¯¢å¼•æ“å°†ä½¿ç”¨ä¹‹å‰è®¾ç½®çš„åå¤„ç†å™¨è¿›è¡ŒèŠ‚ç‚¹åå¤„ç†ã€‚

```python
window_response = sentence_window_engine.query(
    "åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸå»ºåŠŸç«‹ä¸šçš„å…³é”®æ˜¯ä»€ä¹ˆ?"
)
```

ä½¿ç”¨ `LLAMA` æ¡†æ¶æä¾›çš„ `display_response` å‡½æ•°å±•ç¤ºäº†æŸ¥è¯¢çš„å“åº”ç»“æœã€‚è¿™é€šå¸¸åŒ…æ‹¬ä¸æŸ¥è¯¢åŒ¹é…çš„ä¸€ç»„èŠ‚ç‚¹ï¼Œä»¥åŠå®ƒä»¬çš„æ–‡æœ¬ã€åˆ†æ•°ç­‰ä¿¡æ¯ã€‚  
è¿™ç§æ–¹å¼å¯ä»¥åœ¨`Notebook`ç¯å¢ƒä¸­æ›´å¥½åœ°å¯è§†åŒ–å’Œç†è§£æŸ¥è¯¢çš„ç»“æœã€‚

```python
from llama_index.response.notebook_utils import display_response

display_response(window_response)
```

`**Final Response:**` åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸå»ºåŠŸç«‹ä¸šçš„å…³é”®æ˜¯ç³»ç»Ÿèƒ½å¤Ÿæ­£ç¡®è§£é‡Šå¤–éƒ¨æ•°æ®ï¼Œä»ä¸­å­¦ä¹ ï¼Œå¹¶åˆ©ç”¨è¿™äº›çŸ¥è¯†é€šè¿‡çµæ´»é€‚åº”å®ç°ç‰¹å®šç›®æ ‡å’Œä»»åŠ¡çš„èƒ½åŠ›ã€‚

## åˆå¹¶ä¸Šè¿°æ“ä½œ
<br/>color2
+ `documents`: è¦æ„å»ºç´¢å¼•çš„æ–‡æ¡£åˆ—è¡¨ã€‚
+ `llm`: OpenAI è¯­è¨€æ¨¡å‹å®ä¾‹ã€‚
+ `embed_model`: åµŒå…¥æ¨¡å‹çš„åç§°æˆ–è·¯å¾„ã€‚
+ `sentence_window_size`: å¥å­çª—å£çš„å¤§å°ã€‚
+ `save_dir`: æŒä¹…åŒ–ç´¢å¼•çš„ç›®å½•ã€‚

<br/>

1. åˆ›å»ºä¸€ä¸ªå¥å­çª—å£çš„èŠ‚ç‚¹è§£æå™¨ï¼ˆnode_parserï¼‰ã€‚
2. åˆ›å»ºä¸€ä¸ªåŒ…å«è¯­è¨€æ¨¡å‹å’ŒèŠ‚ç‚¹è§£æå™¨ç­‰ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ ServiceContextã€‚
3. å¦‚æœæŒ‡å®šçš„ç›®å½•ä¸­ä¸å­˜åœ¨ç´¢å¼•ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªåŸºäºæä¾›çš„æ–‡æ¡£çš„ VectorStoreIndex å¹¶å°†å…¶æŒä¹…åŒ–åˆ°æŒ‡å®šç›®å½•ã€‚
4. å¦‚æœç›®å½•ä¸­å·²å­˜åœ¨ç´¢å¼•æ–‡ä»¶ï¼Œåˆ™ä»æ–‡ä»¶ä¸­åŠ è½½ç´¢å¼•ã€‚
5. è¿”å›æ„å»ºçš„å¥å­çª—å£ç´¢å¼•ã€‚

```python
import os
from llama_index import ServiceContext, VectorStoreIndex, StorageContext
from llama_index.node_parser import SentenceWindowNodeParser
from llama_index.indices.postprocessor import MetadataReplacementPostProcessor
from llama_index.indices.postprocessor import SentenceTransformerRerank
from llama_index import load_index_from_storage


def build_sentence_window_index(
    documents,
    llm,
    embed_model="local:BAAI/bge-small-zh-v1.5",
    sentence_window_size=3,
    save_dir="sentence_index",
):
    # create the sentence window node parser w/ default settings
    node_parser = SentenceWindowNodeParser.from_defaults(
        window_size=sentence_window_size,
        window_metadata_key="window",
        original_text_metadata_key="original_text",
    )
    sentence_context = ServiceContext.from_defaults(
        llm=llm,
        embed_model=embed_model,
        node_parser=node_parser,
    )
    if not os.path.exists(save_dir):
        sentence_index = VectorStoreIndex.from_documents(
            documents, service_context=sentence_context
        )
        sentence_index.storage_context.persist(persist_dir=save_dir)
    else:
        sentence_index = load_index_from_storage(
            StorageContext.from_defaults(persist_dir=save_dir),
            service_context=sentence_context,
        )

    return sentence_index
```

<br/>color2
+ `sentence_index`: å·²æ„å»ºçš„å¥å­çª—å£ç´¢å¼•ã€‚
+ `similarity_top_k`: ç›¸ä¼¼æ€§æŸ¥è¯¢çš„ top kã€‚
+ `rerank_top_n`: é‡æ–°æ’åçš„ top nã€‚

<br/>

å®šä¹‰äº†ä¸¤ä¸ªåå¤„ç†å™¨ï¼š`postproc` ç”¨äºæ›¿æ¢å…ƒæ•°æ®é”®ï¼Œ`rerank` ç”¨äºä½¿ç”¨å¥å­è½¬æ¢æ¨¡å‹é‡æ–°æ’åèŠ‚ç‚¹ã€‚  
åˆ›å»ºä¸€ä¸ªæŸ¥è¯¢å¼•æ“ `sentence_window_engine`ï¼Œå°†å¥å­çª—å£ç´¢å¼•è½¬æ¢ä¸ºæŸ¥è¯¢å¼•æ“ï¼Œå¹¶ä½¿ç”¨å®šä¹‰çš„åå¤„ç†å™¨ã€‚  
è¿”å›æ„å»ºçš„æŸ¥è¯¢å¼•æ“ã€‚

```python
def get_sentence_window_query_engine(
    sentence_index, similarity_top_k=6, rerank_top_n=2
):
    # define postprocessors
    postproc = MetadataReplacementPostProcessor(target_metadata_key="window")
    rerank = SentenceTransformerRerank(
        top_n=rerank_top_n, model="BAAI/bge-reranker-base"
    )

    sentence_window_engine = sentence_index.as_query_engine(
        similarity_top_k=similarity_top_k, node_postprocessors=[postproc, rerank]
    )
    return sentence_window_engine
```

è°ƒç”¨ä¹‹å‰å®šä¹‰çš„ `build_sentence_window_index` å‡½æ•°ï¼Œä¼ å…¥æ–‡æ¡£åˆ—è¡¨ã€è¯­è¨€æ¨¡å‹å®ä¾‹å’Œä¿å­˜ç›®å½•ï¼Œä»¥æ„å»ºå¥å­çª—å£ç´¢å¼•ã€‚

```python
from llama_index.llms import OpenAI

index = build_sentence_window_index(
    [document],
    llm=OpenAI(model="gpt-3.5-turbo", temperature=0.1),
    save_dir="./sentence_index",
)
```

è°ƒç”¨ä¹‹å‰å®šä¹‰çš„ `get_sentence_window_query_engine` å‡½æ•°ï¼Œä¼ å…¥æ„å»ºçš„å¥å­çª—å£ç´¢å¼•å’Œç›¸ä¼¼æ€§ `top k`ï¼Œä»¥è·å–å¥å­çª—å£çš„æŸ¥è¯¢å¼•æ“ã€‚  
åœ¨è¿™é‡Œï¼Œ`similarity_top_k` è®¾ç½®ä¸º 6ã€‚

```python
query_engine = get_sentence_window_query_engine(index, similarity_top_k=6)
```

## TruLensè¯„æµ‹
ä»åä¸º 'generated_questions.text' çš„æ–‡ä»¶ä¸­è¯»å–ç”Ÿæˆçš„é—®é¢˜ï¼Œå°†å…¶å­˜å‚¨åœ¨ `eval_questions` åˆ—è¡¨ä¸­ã€‚

```python
eval_questions = []
with open('data/generated_questions.txt', 'r') as file:
    for line in file:
        # Remove newline character and convert to integer
        item = line.strip()
        eval_questions.append(item)
```

å®šä¹‰äº†ä¸€ä¸ªå‡½æ•° `run_evals`ï¼Œè¯¥å‡½æ•°æ¥å—ç”Ÿæˆçš„é—®é¢˜åˆ—è¡¨ã€`TruLens` è®°å½•å™¨å’ŒæŸ¥è¯¢å¼•æ“ä½œä¸ºå‚æ•°ã€‚å¯¹äºæ¯ä¸ªé—®é¢˜ï¼Œä½¿ç”¨ `TruLens` è®°å½•å™¨å¼€å§‹è®°å½•ï¼Œç„¶åä½¿ç”¨æŸ¥è¯¢å¼•æ“æ‰§è¡ŒæŸ¥è¯¢ã€‚

```python
from trulens_eval import Tru

def run_evals(eval_questions, tru_recorder, query_engine):
    for question in eval_questions:
        with tru_recorder as recording:
            response = query_engine.query(question)
```

ä½¿ç”¨ Tru ç±»çš„ `reset_database` æ–¹æ³•é‡ç½® `TruLens` æ•°æ®åº“ã€‚

```python
from utils import get_prebuilt_trulens_recorder

from trulens_eval import Tru

tru = Tru()

tru.reset_database()
```

```python
ğŸ¦‘ Tru initialized with db url sqlite:///default.sqlite .
ğŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of `Tru` to prevent this.
```

###  æ»‘çª—å°ºå¯¸è®¾ç½®ä¸º1
è°ƒç”¨ä¹‹å‰å®šä¹‰çš„å‡½æ•° `build_sentence_window_index` å’Œ `get_sentence_window_query_engine`, åˆ†åˆ«æ„å»ºäº†å¥å­çª—å£ç´¢å¼•å’ŒæŸ¥è¯¢å¼•æ“ã€‚è¿™é‡Œè®¾ç½®äº†çª—å£å¤§å°ä¸º 1ï¼Œå¹¶æŒ‡å®šäº†ä¿å­˜ç›®å½•ä¸º "`sentence_index_1`"ã€‚

```python
sentence_index_1 = build_sentence_window_index(
    documents,
    llm=OpenAI(model="gpt-3.5-turbo", temperature=0.1),
    embed_model="local:BAAI/bge-small-zh-v1.5",  # "local:BAAI/bge-small-en-v1.5" for english
    sentence_window_size=1,
    save_dir="sentence_index_1",
)
sentence_window_engine_1 = get_sentence_window_query_engine(
    sentence_index_1
)
tru_recorder_1 = get_prebuilt_trulens_recorder(
    sentence_window_engine_1,
    app_id='sentence window engine 1'
)
```

è°ƒç”¨ä¹‹å‰å®šä¹‰çš„è¯„ä¼°å‡½æ•° `run_evals`ï¼Œä¼ å…¥ç”Ÿæˆçš„é—®é¢˜åˆ—è¡¨ã€`TruLens` è®°å½•å™¨ `tru_recorder_1` å’Œæ„å»ºçš„æŸ¥è¯¢å¼•æ“ `sentence_window_engine_1`ï¼Œè¿è¡Œè¯„ä¼°ä»»åŠ¡ã€‚

```python
run_evals(eval_questions, tru_recorder_1, sentence_window_engine_1)
```

æŸ¥çœ‹ç»“æœ

```python
records, feedback = tru.get_records_and_feedback(app_ids=[])
```

```python
records.head()
```

![](https://cdn.nlark.com/yuque/0/2025/png/2639475/1736489541901-ee1d1f69-49e3-4a56-8b91-7f671dcd5be6.png)

### æ»‘çª—å°ºå¯¸è®¾ç½®ä¸º 3
è°ƒç”¨ä¹‹å‰å®šä¹‰çš„å‡½æ•° `build_sentence_window_index` å’Œ `get_sentence_window_query_engine`, åˆ†åˆ«æ„å»ºäº†å¥å­çª—å£ç´¢å¼•ã€æŸ¥è¯¢å¼•æ“å’Œ `TruLens` è®°å½•å™¨ã€‚è®¾ç½®äº†çª—å£å¤§å°ä¸º 3ï¼Œå¹¶æŒ‡å®šäº†ä¿å­˜ç›®å½•ä¸º "`sentence_index_3`"ã€‚

```python
sentence_index_3 = build_sentence_window_index(
    documents,
    llm=OpenAI(model="gpt-3.5-turbo", temperature=0.1),
    embed_model="local:BAAI/bge-small-zh-v1.5",  # "local:BAAI/bge-small-en-v1.5" for english
    sentence_window_size=3,
    save_dir="sentence_index_3",
)
sentence_window_engine_3 = get_sentence_window_query_engine(
    sentence_index_3
)

tru_recorder_3 = get_prebuilt_trulens_recorder(
    sentence_window_engine_3,
    app_id='sentence window engine 3'
)
```

è°ƒç”¨ä¹‹å‰å®šä¹‰çš„è¯„ä¼°å‡½æ•° `run_evals`ï¼Œä¼ å…¥ç”Ÿæˆçš„é—®é¢˜åˆ—è¡¨ã€`TruLens` è®°å½•å™¨ `tru_recorder_1` å’Œæ„å»ºçš„æŸ¥è¯¢å¼•æ“ `sentence_window_engine_1`ï¼Œè¿è¡Œè¯„ä¼°ä»»åŠ¡ã€‚

```python
run_evals(eval_questions, tru_recorder_3, sentence_window_engine_3)
```

æŸ¥çœ‹ç»“æœ

```python
records, feedback = tru.get_records_and_feedback(app_ids=[])
```

```python
records.head()
```

![](https://cdn.nlark.com/yuque/0/2025/png/2639475/1736489672171-4ecfd88c-0933-4826-843e-b9752da43ea7.png)

## è‹±æ–‡ç‰ˆ
```python
text_en = "hello. how are you? I am fine!  "

nodes_en = node_parser.get_nodes_from_documents([Document(text=text_en)])
```

```python
print([x.text for x in nodes_en])
```

<details class="lake-collapse"><summary id="ud90c2766"><span class="ne-text">outputï¼š</span></summary><pre data-language="json" id="iOXbm" class="ne-codeblock language-json"><code>['hello. ', 'how are you? ', 'I am fine!  ']</code></pre></details>
```python
print(nodes_en[1].metadata["window"])
```

```python
hello.  how are you?  I am fine!
```

```python

from llama_index import VectorStoreIndex

sentence_index_en = VectorStoreIndex.from_documents(
    [document_en], service_context=sentence_context_en
)
```

```python
sentence_index_en.storage_context.persist(persist_dir="./sentence_index_en")
```

```python
# This block of code is optional to check
# if an index file exist, then it will load it
# if not, it will rebuild it

import os
from llama_index import VectorStoreIndex, StorageContext, load_index_from_storage
from llama_index import load_index_from_storage

if not os.path.exists("./sentence_index_en"):
    sentence_index_en = VectorStoreIndex.from_documents(
        [document_en], service_context=sentence_context_en
    )

    sentence_index_en.storage_context.persist(persist_dir="./sentence_index_en")
else:
    sentence_index_en = load_index_from_storage(
        StorageContext.from_defaults(persist_dir="./sentence_index_en"),
        service_context=sentence_context_en
    )
```

```python
from llama_index.schema import NodeWithScore
from copy import deepcopy

scored_nodes_en = [NodeWithScore(node=x, score=1.0) for x in nodes_en2]
nodes_old_en = [deepcopy(n) for n in nodes_en2]
```

```python
text = "ä½ å¥½. å§å°. çŒ«ç‹—. è€é¼ "
text_en2 = 'hello. bar. cat. dog. mouse.'

nodes = node_parser.get_nodes_from_documents([Document(text=text)])
nodes_en2 = node_parser.get_nodes_from_documents([Document(text=text_en2)])
```



```python
print([x.text for x in nodes])
print([x.text for x in nodes_en])
```

```python
['ä½ å¥½. ', 'å§å°. ', 'çŒ«ç‹—. ', 'è€é¼ ']
['hello. ', 'how are you? ', 'I am fine!  ']
```



```python
print(nodes[0].metadata["window"])
print(nodes_en2[0].metadata["window"])
```

```python
ä½ å¥½.  å§å°.  çŒ«ç‹—. 
hello.  bar.  cat.
```

```python
from llama_index import ServiceContext

sentence_context_en = ServiceContext.from_defaults(
    llm=llm,
    embed_model="local:BAAI/bge-small-en-v1.5",
    node_parser=node_parser,
)
```

