> ËßÜËßâÂ§ßÊ®°ÂûãÔºàVFMÔºâVision Foundation Model
>

+ [Visual ChatGPT](https://github.com/microsoft/visual-chatgpt) ![](https://img.shields.io/github/stars/microsoft/visual-chatgpt?style=social) : Visual ChatGPT connects ChatGPT and a series of Visual Foundation Models to enable sending and receiving images during chatting. "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models". ([arXiv 2023](https://arxiv.org/abs/2303.04671)).
+ [InternImage](https://github.com/OpenGVLab/InternImage) ![](https://img.shields.io/github/stars/OpenGVLab/InternImage?style=social) : "InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions". ([CVPR 2023](https://arxiv.org/abs/2211.05778)).
+ [GLIP](https://github.com/microsoft/GLIP) ![](https://img.shields.io/github/stars/microsoft/GLIP?style=social) : "Grounded Language-Image Pre-training". ([CVPR 2022](https://arxiv.org/abs/2112.03857)).
+ [GLIPv2](https://github.com/microsoft/GLIP) ![](https://img.shields.io/github/stars/microsoft/GLIP?style=social) : "GLIPv2: Unifying Localization and Vision-Language Understanding". ([arXiv 2022](https://arxiv.org/abs/2206.05836)).
+ [DINO](https://github.com/IDEA-Research/DINO) ![](https://img.shields.io/github/stars/IDEA-Research/DINO?style=social) : "DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection". ([ICLR 2023](https://arxiv.org/abs/2203.03605)).
+ [DINOv2](https://github.com/facebookresearch/dinov2) ![](https://img.shields.io/github/stars/facebookresearch/dinov2?style=social) : "DINOv2: Learning Robust Visual Features without Supervision". ([arXiv 2023](https://arxiv.org/abs/2304.07193)).
+ [Grounding DINO](https://github.com/IDEA-Research/GroundingDINO) ![](https://img.shields.io/github/stars/IDEA-Research/GroundingDINO?style=social) : "Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection". ([arXiv 2023](https://arxiv.org/abs/2303.05499)). "Áü•‰πé„Äå‰∏âÂàÜÈíüÁÉ≠Â∫¶„Äç„Ää[ÂçÅÂàÜÈíüËß£ËØªGrounding DINO-Ê†πÊçÆÊñáÂ≠óÊèêÁ§∫Ê£ÄÊµã‰ªªÊÑèÁõÆÊ†á](https://zhuanlan.zhihu.com/p/627646794)„Äã"„ÄÇ
+ [SAM](https://github.com/facebookresearch/segment-anything) ![](https://img.shields.io/github/stars/facebookresearch/segment-anything?style=social) : The repository provides code for running inference with the Segment Anything Model (SAM), links for downloading the trained model checkpoints, and example notebooks that show how to use the model. "Segment Anything". ([arXiv 2023](https://arxiv.org/abs/2304.02643)).
+ [Grounded-SAM](https://github.com/IDEA-Research/Grounded-Segment-Anything) ![](https://img.shields.io/github/stars/IDEA-Research/Grounded-Segment-Anything?style=social) : Marrying Grounding DINO with Segment Anything & Stable Diffusion & Tag2Text & BLIP & Whisper & ChatBot - Automatically Detect , Segment and Generate Anything with Image, Text, and Audio Inputs. We plan to create a very interesting demo by combining [Grounding DINO](https://github.com/IDEA-Research/GroundingDINO) and [Segment Anything](https://github.com/facebookresearch/segment-anything) which aims to detect and segment Anything with text inputs!
+ [SEEM](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once) ![](https://img.shields.io/github/stars/UX-Decoder/Segment-Everything-Everywhere-All-At-Once?style=social) : We introduce SEEM that can Segment Everything Everywhere with Multi-modal prompts all at once. SEEM allows users to easily segment an image using prompts of different types including visual prompts (points, marks, boxes, scribbles and image segments) and language prompts (text and audio), etc. It can also work with any combinations of prompts or generalize to custom prompts! "Segment Everything Everywhere All at Once". ([arXiv 2023](https://arxiv.org/abs/2304.06718)).
+ [SAM3D](https://github.com/DYZhang09/SAM3D) ![](https://img.shields.io/github/stars/DYZhang09/SAM3D?style=social) : "SAM3D: Zero-Shot 3D Object Detection via [Segment Anything](https://github.com/facebookresearch/segment-anything) Model". ([arXiv 2023](https://arxiv.org/abs/2306.02245)).
+ [ImageBind](https://github.com/facebookresearch/ImageBind) ![](https://img.shields.io/github/stars/facebookresearch/ImageBind?style=social) : "ImageBind: One Embedding Space To Bind Them All". ([CVPR 2023](https://arxiv.org/abs/2305.05665)).
+ [Track-Anything](https://github.com/gaomingqi/Track-Anything) ![](https://img.shields.io/github/stars/gaomingqi/Track-Anything?style=social) : Track-Anything is a flexible and interactive tool for video object tracking and segmentation, based on Segment Anything, XMem, and E2FGVI. "Track Anything: Segment Anything Meets Videos". ([arXiv 2023](https://arxiv.org/abs/2304.11968)).
+ [qianqianwang68/omnimotion](https://github.com/qianqianwang68/omnimotion) ![](https://img.shields.io/github/stars/qianqianwang68/omnimotion?style=social) : "Tracking Everything Everywhere All at Once". ([arXiv 2023](https://arxiv.org/abs/2306.05422)).
+ [LLaVA](https://github.com/haotian-liu/LLaVA) ![](https://img.shields.io/github/stars/haotian-liu/LLaVA?style=social) : üåã LLaVA: Large Language and Vision Assistant. Visual instruction tuning towards large language and vision models with GPT-4 level capabilities. [llava.hliu.cc](https://llava.hliu.cc/). "Visual Instruction Tuning". ([arXiv 2023](https://arxiv.org/abs/2304.08485)).
+ [M3I-Pretraining](https://github.com/OpenGVLab/M3I-Pretraining) ![](https://img.shields.io/github/stars/OpenGVLab/M3I-Pretraining?style=social) : "Towards All-in-one Pre-training via Maximizing Multi-modal Mutual Information". ([arXiv 2022](https://arxiv.org/abs/2211.09807)).
+ [BEVFormer](https://github.com/fundamentalvision/BEVFormer) ![](https://img.shields.io/github/stars/fundamentalvision/BEVFormer?style=social) : BEVFormer: a Cutting-edge Baseline for Camera-based Detection. "BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers". ([arXiv 2022](https://arxiv.org/abs/2203.17270)).
+ [Uni-Perceiver](https://github.com/fundamentalvision/Uni-Perceiver) ![](https://img.shields.io/github/stars/fundamentalvision/Uni-Perceiver?style=social) : "Uni-Perceiver: Pre-training Unified Architecture for Generic Perception for Zero-shot and Few-shot Tasks". ([CVPR 2022](https://openaccess.thecvf.com/content/CVPR2022/html/Zhu_Uni-Perceiver_Pre-Training_Unified_Architecture_for_Generic_Perception_for_Zero-Shot_and_CVPR_2022_paper.html)).
+ [AnyLabeling](https://github.com/vietanhdev/anylabeling) ![](https://img.shields.io/github/stars/vietanhdev/anylabeling?style=social) : üåü AnyLabeling üåü. Effortless data labeling with AI support from YOLO and Segment Anything! Effortless data labeling with AI support from YOLO and Segment Anything!
+ [X-AnyLabeling](https://github.com/CVHub520/X-AnyLabeling) ![](https://img.shields.io/github/stars/CVHub520/X-AnyLabeling?style=social) : üí´ X-AnyLabeling üí´. Effortless data labeling with AI support from Segment Anything and other awesome models!
+ [Label Anything](https://github.com/open-mmlab/playground/tree/main/label_anything) ![](https://img.shields.io/github/stars/open-mmlab/playground?style=social) : OpenMMLab PlayGround: Semi-Automated Annotation with Label-Studio and SAM.
+ [RevCol](https://github.com/megvii-research/RevCol) ![](https://img.shields.io/github/stars/megvii-research/RevCol?style=social) : "Reversible Column Networks". ([arXiv 2023](https://arxiv.org/abs/2212.11696)).
+ [Macaw-LLM](https://github.com/lyuchenyang/Macaw-LLM) ![](https://img.shields.io/github/stars/lyuchenyang/Macaw-LLM?style=social) : Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration.
+ [SAM-PT](https://github.com/SysCV/sam-pt) ![](https://img.shields.io/github/stars/SysCV/sam-pt?style=social) : SAM-PT: Extending SAM to zero-shot video segmentation with point-based tracking. "Segment Anything Meets Point Tracking". ([arXiv 2023](https://arxiv.org/abs/2307.01197)).
+ [Video-LLaMA](https://github.com/DAMO-NLP-SG/Video-LLaMA) ![](https://img.shields.io/github/stars/DAMO-NLP-SG/Video-LLaMA?style=social) : "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding". ([arXiv 2023](https://arxiv.org/abs/2306.02858)).
+ [MobileSAM](https://github.com/ChaoningZhang/MobileSAM) ![](https://img.shields.io/github/stars/ChaoningZhang/MobileSAM?style=social) : "Faster Segment Anything: Towards Lightweight SAM for Mobile Applications". ([arXiv 2023](https://arxiv.org/abs/2306.14289)).
+ [BuboGPT](https://github.com/magic-research/bubogpt) ![](https://img.shields.io/github/stars/magic-research/bubogpt?style=social) : "BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs". ([arXiv 2023](https://arxiv.org/abs/2307.08581)).



