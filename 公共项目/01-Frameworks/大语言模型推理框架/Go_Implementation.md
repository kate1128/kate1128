> Â§ßËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜÊ°ÜÊû∂ LLM Inference Framework 
>

# ‚≠êOllama
+ [Ollama](https://github.com/ollama/ollama/) ![](https://img.shields.io/github/stars/ollama/ollama?style=social) : Get up and running with Llama 2, Mistral, Gemma, and other large language models. [ollama.com](https://ollama.com/)

# go-skynet/LocalAI
+ [go-skynet/LocalAI](https://github.com/go-skynet/LocalAI) ![](https://img.shields.io/github/stars/go-skynet/LocalAI?style=social) : ü§ñ Self-hosted, community-driven, local OpenAI-compatible API. Drop-in replacement for OpenAI running LLMs on consumer-grade hardware. Free Open Source OpenAI alternative. No GPU required. LocalAI is an API to run ggml compatible models: llama, gpt4all, rwkv, whisper, vicuna, koala, gpt4all-j, cerebras, falcon, dolly, starcoder, and many other. [localai.io](https://localai.io/)



