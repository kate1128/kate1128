> 大语言模型（LLM）Large Language Model 
>

---有新的尽量补充在这---

# ⭐deekseek
05月28日新增

+ [GitHub - deepseek-ai/DeepSeek-R1](https://github.com/deepseek-ai/DeepSeek-R1)![](https://cdn.nlark.com/yuque/0/2025/png/2639475/1748399041967-f6e17b3a-9aab-4d31-97b6-1f6fe4d35282.png)：deepseek R1

# ⭐Qwen
+ [Qwen（通义千问）](https://github.com/QwenLM/Qwen) ![](https://img.shields.io/github/stars/QwenLM/Qwen?style=social) : The official repo of Qwen (通义千问) chat & pretrained large language model proposed by Alibaba Cloud.
+ [Qwen2](https://github.com/QwenLM/Qwen2) ![](https://img.shields.io/github/stars/QwenLM/Qwen2?style=social) : Qwen2 is the large language model series developed by Qwen team, Alibaba Cloud.
+ 05月28日新增 Qwen3（[https://github.com/QwenLM/Qwen3](https://github.com/QwenLM/Qwen3)）

# ⭐GPT
+ GPT-1 : "Improving Language Understanding by Generative Pre-Training". ([cs.ubc.ca, 2018](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)).
+ [GPT-2](https://github.com/openai/gpt-2) ![](https://img.shields.io/github/stars/openai/gpt-2?style=social) : "Language Models are Unsupervised Multitask Learners". ([OpenAI blog, 2019](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)). [Better language models and their implications](https://openai.com/research/better-language-models).
+ [GPT-3](https://github.com/openai/gpt-3) ![](https://img.shields.io/github/stars/openai/gpt-3?style=social) : "GPT-3: Language Models are Few-Shot Learners". ([arXiv 2020](https://arxiv.org/abs/2005.14165)).
+ InstructGPT : "Training language models to follow instructions with human feedback". ([arXiv 2022](https://arxiv.org/abs/2203.02155)). "Aligning language models to follow instructions". ([OpenAI blog, 2022](https://openai.com/research/instruction-following)).
+ [ChatGPT](https://chat.openai.com/): [Optimizing Language Models for Dialogue](https://openai.com/blog/chatgpt).
+ [GPT-4](https://openai.com/product/gpt-4): GPT-4 is OpenAI’s most advanced system, producing safer and more useful responses. "Sparks of Artificial General Intelligence: Early experiments with GPT-4". ([arXiv 2023](https://arxiv.org/abs/2303.12712)). "GPT-4 Architecture, Infrastructure, Training Dataset, Costs, Vision, MoE". ([SemianAlysis, 2023](https://www.semianalysis.com/p/gpt-4-architecture-infrastructure)).
+ [GPT-4-LLM](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM) ![](https://img.shields.io/github/stars/Instruction-Tuning-with-GPT-4/GPT-4-LLM?style=social) : "Instruction Tuning with GPT-4". ([arXiv 2023](https://arxiv.org/abs/2304.03277)). [instruction-tuning-with-gpt-4.github.io/](https://instruction-tuning-with-gpt-4.github.io/)
+ [MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4) ![](https://img.shields.io/github/stars/Vision-CAIR/MiniGPT-4?style=social) : MiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models. [minigpt-4.github.io](https://minigpt-4.github.io/)
+ [minGPT](https://github.com/karpathy/minGPT) ![](https://img.shields.io/github/stars/karpathy/minGPT?style=social) : A minimal PyTorch re-implementation of the OpenAI GPT (Generative Pretrained Transformer) training.
+ [nanoGPT](https://github.com/karpathy/nanoGPT) ![](https://img.shields.io/github/stars/karpathy/nanoGPT?style=social) : The simplest, fastest repository for training/finetuning medium-sized GPTs.
+ [MicroGPT](https://github.com/muellerberndt/micro-gpt) ![](https://img.shields.io/github/stars/muellerberndt/micro-gpt?style=social) : A simple and effective autonomous agent compatible with GPT-3.5-Turbo and GPT-4. MicroGPT aims to be as compact and reliable as possible.
+ [GPT4All](https://github.com/nomic-ai/gpt4all) ![](https://img.shields.io/github/stars/nomic-ai/gpt4all?style=social) : GPT4All: An ecosystem of open-source on-edge large language models. GTP4All is an ecosystem to train and deploy powerful and customized large language models that run locally on consumer grade CPUs.
+ [WorkGPT](https://github.com/h2oai/h2ogpt) ![](https://img.shields.io/github/stars/h2oai/h2ogpt?style=social) : WorkGPT is an agent framework in a similar fashion to AutoGPT or LangChain.
+ [h2oGPT](https://github.com/team-openpm/workgpt) ![](https://img.shields.io/github/stars/team-openpm/workgpt?style=social) : h2oGPT is a large language model (LLM) fine-tuning framework and chatbot UI with document(s) question-answer capabilities. "h2oGPT: Democratizing Large Language Models". ([arXiv 2023](https://arxiv.org/abs/2306.08161)).
+ [DemoGPT](https://github.com/melih-unsal/DemoGPT) ![](https://img.shields.io/github/stars/melih-unsal/DemoGPT?style=social) : Create 🦜️🔗 LangChain apps by just using prompts with the power of Llama 2 🌟 Star to support our work! | 只需使用句子即可创建 LangChain 应用程序。 给个star支持我们的工作吧！DemoGPT: Auto Gen-AI App Generator with the Power of Llama 2. ⚡ With just a prompt, you can create interactive Streamlit apps via 🦜️🔗 LangChain's transformative capabilities & Llama 2.⚡ [demogpt.io](https://www.demogpt.io/)
+ [GPT-Engineer](https://github.com/AntonOsika/gpt-engineer) ![](https://img.shields.io/github/stars/AntonOsika/gpt-engineer?style=social) : Specify what you want it to build, the AI asks for clarification, and then builds it. GPT Engineer is made to be easy to adapt, extend, and make your agent learn how you want your code to look. It generates an entire codebase based on a prompt.
+ [1595901624/gpt-aggregated-edition](https://github.com/1595901624/gpt-aggregated-edition) ![](https://img.shields.io/github/stars/1595901624/gpt-aggregated-edition?style=social) : 聚合ChatGPT官方版、ChatGPT免费版、文心一言、Poe、chatchat等多平台，支持自定义导入平台。
+ [SpeechGPT](https://github.com/0nutation/SpeechGPT) ![](https://img.shields.io/github/stars/0nutation/SpeechGPT?style=social) : "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities". ([arXiv 2023](https://arxiv.org/abs/2305.11000)).
+ [GPT2-Chinese](https://github.com/Morizeyao/GPT2-Chinese) ![](https://img.shields.io/github/stars/Morizeyao/GPT2-Chinese?style=social) : Chinese version of GPT2 training code, using BERT tokenizer.
+ [gpt-llm-trainer](https://github.com/mshumer/gpt-llm-trainer) ![](https://img.shields.io/github/stars/mshumer/gpt-llm-trainer?style=social) : The goal of this project is to explore an experimental new pipeline to train a high-performing task-specific model. We try to abstract away all the complexity, so it's as easy as possible to go from idea -> performant fully-trained model.

# Llama
+ [Llama 2](https://github.com/facebookresearch/llama) ![](https://img.shields.io/github/stars/facebookresearch/llama?style=social) : Inference code for LLaMA models. "LLaMA: Open and Efficient Foundation Language Models". ([arXiv 2023](https://arxiv.org/abs/2302.13971)). "Llama 2: Open Foundation and Fine-Tuned Chat Models". ([ai.meta.com, 2023-07-18](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)). ([2023-07-18, Llama 2 is here - get it on Hugging Face](https://huggingface.co/blog/llama2)).
+ [Llama 3](https://github.com/meta-llama/llama3) ![](https://img.shields.io/github/stars/meta-llama/llama3?style=social) : The official Meta Llama 3 GitHub site.
+ [Lit-LLaMA](https://github.com/Lightning-AI/lit-llama) ![](https://img.shields.io/github/stars/Lightning-AI/lit-llama?style=social) : ⚡ Lit-LLaMA. Implementation of the LLaMA language model based on nanoGPT. Supports flash attention, Int8 and GPTQ 4bit quantization, LoRA and LLaMA-Adapter fine-tuning, pre-training. Apache 2.0-licensed.
+ [LongLLaMA ](https://github.com/CStanKonrad/long_llama) ![](https://img.shields.io/github/stars/CStanKonrad/long_llama?style=social) : LongLLaMA is a large language model capable of handling long contexts. It is based on OpenLLaMA and fine-tuned with the Focused Transformer (FoT) method.
+ [LLaMA-Adapter](https://github.com/OpenGVLab/LLaMA-Adapter) ![](https://img.shields.io/github/stars/OpenGVLab/LLaMA-Adapter?style=social) : Fine-tuning LLaMA to follow Instructions within 1 Hour and 1.2M Parameters. LLaMA-Adapter: Efficient Fine-tuning of LLaMA 🚀
+ [Llama-2-Onnx](https://github.com/microsoft/Llama-2-Onnx) ![](https://img.shields.io/github/stars/microsoft/Llama-2-Onnx?style=social) : Llama 2 Powered By ONNX.
+ [Chinese LLaMA and Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca) ![](https://img.shields.io/github/stars/ymcui/Chinese-LLaMA-Alpaca?style=social) : 中文LLaMA&Alpaca大语言模型+本地CPU/GPU训练部署 (Chinese LLaMA & Alpaca LLMs)。"Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca". ([arXiv 2023](https://arxiv.org/abs/2304.08177)).
+ [Chinese-LLaMA-Alpaca-2](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2) ![](https://img.shields.io/github/stars/ymcui/Chinese-LLaMA-Alpaca-2?style=social) : 中文 LLaMA-2 & Alpaca-2 大模型二期项目 (Chinese LLaMA-2 & Alpaca-2 LLMs).
+ [FlagAlpha/Llama2-Chinese](https://github.com/FlagAlpha/Llama2-Chinese) ![](https://img.shields.io/github/stars/FlagAlpha/Llama2-Chinese?style=social) : Llama中文社区，最好的中文Llama大模型，完全开源可商用。
+ [michael-wzhu/Chinese-LlaMA2](https://github.com/michael-wzhu/Chinese-LlaMA2) ![](https://img.shields.io/github/stars/michael-wzhu/Chinese-LlaMA2?style=social) : Repo for adapting Meta LlaMA2 in Chinese! META最新发布的LlaMA2的汉化版！ （完全开源可商用）
+ [CrazyBoyM/llama3-Chinese-chat](https://github.com/CrazyBoyM/llama3-Chinese-chat) ![](https://img.shields.io/github/stars/CrazyBoyM/llama3-Chinese-chat?style=social) : Llama3 中文版。

# Claude
+ [Claude](https://www.anthropic.com/product) : Claude is a next-generation AI assistant based on Anthropic’s research into training helpful, honest, and harmless AI systems.

# ChatGLM
+ [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) ![](https://img.shields.io/github/stars/THUDM/ChatGLM-6B?style=social) : ChatGLM-6B: An Open Bilingual Dialogue Language Model | 开源双语对话语言模型。 ChatGLM-6B 是一个开源的、支持中英双语的对话语言模型，基于 [General Language Model (GLM)](https://github.com/THUDM/GLM) 架构，具有 62 亿参数。 "GLM: General Language Model Pretraining with Autoregressive Blank Infilling". ([ACL 2022](https://aclanthology.org/2022.acl-long.26/)).  "GLM-130B: An Open Bilingual Pre-trained Model". ([ICLR 2023](https://openreview.net/forum?id=-Aw0rrrPUF)).
+ [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B) ![](https://img.shields.io/github/stars/THUDM/ChatGLM2-6B?style=social) : ChatGLM2-6B: An Open Bilingual Chat LLM | 开源双语对话语言模型。ChatGLM2-6B 是开源中英双语对话模型 ChatGLM-6B 的第二代版本，在保留了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上，ChatGLM2-6B 引入了更强大的性能、更强大的性能、更高效的推理、更开放的协议。
+ [ChatGLM3](https://github.com/THUDM/ChatGLM3) ![](https://img.shields.io/github/stars/THUDM/ChatGLM3?style=social) : ChatGLM3 series: Open Bilingual Chat LLMs | 开源双语对话语言模型。

# 书生浦语
+ [InternLM（书生·浦语）](https://github.com/InternLM/InternLM) ![](https://img.shields.io/github/stars/InternLM/InternLM?style=social) : Official release of InternLM2 7B and 20B base and chat models. 200K context support. [internlm.intern-ai.org.cn/](https://internlm.intern-ai.org.cn/)

# 百川
+ [Baichuan-7B（百川-7B）](https://github.com/baichuan-inc/Baichuan-7B) ![](https://img.shields.io/github/stars/baichuan-inc/Baichuan-7B?style=social) : A large-scale 7B pretraining language model developed by BaiChuan-Inc. Baichuan-7B 是由百川智能开发的一个开源可商用的大规模预训练语言模型。基于 Transformer 结构，在大约 1.2 万亿 tokens 上训练的 70 亿参数模型，支持中英双语，上下文窗口长度为 4096。在标准的中文和英文 benchmark（C-Eval/MMLU）上均取得同尺寸最好的效果。[huggingface.co/baichuan-inc/baichuan-7B](https://huggingface.co/baichuan-inc/Baichuan-7B)
+ [Baichuan-13B（百川-13B）](https://github.com/baichuan-inc/Baichuan-13B) ![](https://img.shields.io/github/stars/baichuan-inc/Baichuan-13B?style=social) : A 13B large language model developed by Baichuan Intelligent Technology. Baichuan-13B 是由百川智能继 Baichuan-7B 之后开发的包含 130 亿参数的开源可商用的大规模语言模型，在权威的中文和英文 benchmark 上均取得同尺寸最好的效果。本次发布包含有预训练 (Baichuan-13B-Base) 和对齐 (Baichuan-13B-Chat) 两个版本。[huggingface.co/baichuan-inc/Baichuan-13B-Chat](https://huggingface.co/baichuan-inc/Baichuan-13B-Chat)
+ [Baichuan2](https://github.com/baichuan-inc/Baichuan2) ![](https://img.shields.io/github/stars/baichuan-inc/Baichuan2?style=social) : A series of large language models developed by Baichuan Intelligent Technology. Baichuan 2 是百川智能推出的新一代开源大语言模型，采用 2.6 万亿 Tokens 的高质量语料训练。Baichuan 2 在多个权威的中文、英文和多语言的通用、领域 benchmark 上取得同尺寸最佳的效果。本次发布包含有 7B、13B 的 Base 和 Chat 版本，并提供了 Chat 版本的 4bits 量化。[huggingface.co/baichuan-inc](https://huggingface.co/baichuan-inc). "Baichuan 2: Open Large-scale Language Models". ([arXiv 2023](https://arxiv.org/abs/2309.10305)).

# 文心一言
+ [百度-文心大模型](https://wenxin.baidu.com/) : 百度全新一代知识增强大语言模型，文心大模型家族的新成员，能够与人对话互动，回答问题，协助创作，高效便捷地帮助人们获取信息、知识和灵感。
+ [百度智能云-千帆大模型](https://cloud.baidu.com/product/wenxinworkshop) : 百度智能云千帆大模型平台一站式企业级大模型平台，提供先进的生成式AI生产及应用全流程开发工具链。

# 盘古大模型
+ [华为云-盘古大模型](https://www.huaweicloud.com/product/pangu.html) : 盘古大模型致力于深耕行业，打造金融、政务、制造、矿山、气象、铁路等领域行业大模型和能力集，将行业知识know-how与大模型能力相结合，重塑千行百业，成为各组织、企业、个人的专家助手。"Accurate medium-range global weather forecasting with 3D neural networks". ([Nature 2023](https://www.nature.com/articles/s41586-023-06185-3)).

# 日日新
+ [商汤科技-日日新SenseNova](https://techday.sensetime.com/?utm_source=baidu-sem-pc&utm_medium=cpc&utm_campaign=PC-%E6%8A%80%E6%9C%AF%E4%BA%A4%E6%B5%81%E6%97%A5-%E4%BA%A7%E5%93%81%E8%AF%8D-%E6%97%A5%E6%97%A5%E6%96%B0&utm_content=%E6%97%A5%E6%97%A5%E6%96%B0&utm_term=%E6%97%A5%E6%97%A5%E6%96%B0SenseNova&e_creative=73937788324&e_keywordid=594802524403) : 日日新（SenseNova），是商汤科技宣布推出的大模型体系，包括自然语言处理模型“商量”（SenseChat）、文生图模型“秒画”和数字人视频生成平台“如影”（SenseAvatar）等。

# 星火大模型
+ [科大讯飞-星火认知大模型](https://xinghuo.xfyun.cn/) : 新一代认知智能大模型，拥有跨领域知识和语言理解能力，能够基于自然对话方式理解与执行任务。

# 豆包
+ [字节跳动-豆包](https://www.doubao.com/) : 豆包。

# 其他
+ [Gemma](https://github.com/google/gemma_pytorch) ![](https://img.shields.io/github/stars/google/gemma_pytorch?style=social) : The official PyTorch implementation of Google's Gemma models. [ai.google.dev/gemma](https://ai.google.dev/gemma)
+ [Grok-1](https://github.com/xai-org/grok-1) ![](https://img.shields.io/github/stars/xai-org/grok-1?style=social) : This repository contains JAX example code for loading and running the Grok-1 open-weights model.
+ [Whisper](https://github.com/openai/whisper) ![](https://img.shields.io/github/stars/openai/whisper?style=social) : Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multitasking model that can perform multilingual speech recognition, speech translation, and language identification. "Robust Speech Recognition via Large-Scale Weak Supervision". ([arXiv 2022](https://arxiv.org/abs/2212.04356)).
+ [OpenChat](https://github.com/imoneoi/openchat) ![](https://img.shields.io/github/stars/imoneoi/openchat?style=social) : OpenChat: Advancing Open-source Language Models with Imperfect Data. [huggingface.co/openchat/openchat](https://huggingface.co/openchat/openchat)
+ [StableLM](https://github.com/Stability-AI/StableLM) ![](https://img.shields.io/github/stars/Stability-AI/StableLM?style=social) : StableLM: Stability AI Language Models.
+ [JARVIS](https://github.com/microsoft/JARVIS) ![](https://img.shields.io/github/stars/microsoft/JARVIS?style=social) : JARVIS, a system to connect LLMs with ML community. "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace". ([arXiv 2023](https://arxiv.org/abs/2303.17580)).
+ [Dolly](https://github.com/databrickslabs/dolly) ![](https://img.shields.io/github/stars/databrickslabs/dolly?style=social) : Databricks’ Dolly, a large language model trained on the Databricks Machine Learning Platform. [Hello Dolly: Democratizing the magic of ChatGPT with open models](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html)
+ [LMFlow](https://github.com/OptimalScale/LMFlow) ![](https://img.shields.io/github/stars/OptimalScale/LMFlow?style=social) : An extensible, convenient, and efficient toolbox for finetuning large machine learning models, designed to be user-friendly, speedy and reliable, and accessible to the entire community. Large Language Model for All. [optimalscale.github.io/LMFlow/](https://optimalscale.github.io/LMFlow/)
+ [Colossal-AI](https://github.com/hpcaitech/ColossalAI) ![](https://img.shields.io/github/stars/hpcaitech/ColossalAI?style=social) : Making big AI models cheaper, easier, and scalable. [www.colossalai.org](www.colossalai.org). "Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training". ([arXiv 2021](https://arxiv.org/abs/2110.14883)).
+ [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) ![](https://img.shields.io/github/stars/tatsu-lab/stanford_alpaca?style=social) : Stanford Alpaca: An Instruction-following LLaMA Model.
+ [feizc/Visual-LLaMA](https://github.com/feizc/Visual-LLaMA) ![](https://img.shields.io/github/stars/feizc/Visual-LLaMA?style=social) : Open LLaMA Eyes to See the World. This project aims to optimize LLaMA model for visual information understanding like GPT-4 and further explore the potentional of large language model.
+ [Lightning-AI/lightning-colossalai](https://github.com/Lightning-AI/lightning-colossalai) ![](https://img.shields.io/github/stars/Lightning-AI/lightning-colossalai?style=social) : Efficient Large-Scale Distributed Training with [Colossal-AI](https://colossalai.org/) and [Lightning AI](https://lightning.ai/).
+ [ChatALL](https://github.com/sunner/ChatALL) ![](https://img.shields.io/github/stars/sunner/ChatALL?style=social) :  Concurrently chat with ChatGPT, Bing Chat, bard, Alpaca, Vincuna, Claude, ChatGLM, MOSS, iFlytek Spark, ERNIE and more, discover the best answers. [chatall.ai](http://chatall.ai/)
+ [FreedomIntelligence/LLMZoo](https://github.com/FreedomIntelligence/LLMZoo) ![](https://img.shields.io/github/stars/FreedomIntelligence/LLMZoo?style=social) : ⚡LLM Zoo is a project that provides data, models, and evaluation benchmark for large language models.⚡ [Tech Report](https://github.com/FreedomIntelligence/LLMZoo/blob/main/assets/llmzoo.pdf)
+ [shm007g/LLaMA-Cult-and-More](https://github.com/shm007g/LLaMA-Cult-and-More) ![](https://img.shields.io/github/stars/shm007g/LLaMA-Cult-and-More?style=social) : News about 🦙 Cult and other AIGC models.
+ [X-PLUG/mPLUG-Owl](https://github.com/X-PLUG/mPLUG-Owl) ![](https://img.shields.io/github/stars/X-PLUG/mPLUG-Owl?style=social) : mPLUG-Owl🦉: Modularization Empowers Large Language Models with Multimodality.
+ [i-Code](https://github.com/microsoft/i-Code) ![](https://img.shields.io/github/stars/microsoft/i-Code?style=social) : The ambition of the i-Code project is to build integrative and composable multimodal Artificial Intelligence. The "i" stands for integrative multimodal learning. "CoDi: Any-to-Any Generation via Composable Diffusion". ([arXiv 2023](https://arxiv.org/abs/2305.11846)).
+ [Lamini](https://github.com/lamini-ai/lamini) ![](https://img.shields.io/github/stars/lamini-ai/lamini?style=social) : Lamini: The LLM engine for rapidly customizing models 🦙
+ [xorbitsai/inference](https://github.com/xorbitsai/inference) ![](https://img.shields.io/github/stars/xorbitsai/inference?style=social) : Xorbits Inference (Xinference) is a powerful and versatile library designed to serve LLMs, speech recognition models, and multimodal models, even on your laptop. It supports a variety of models compatible with GGML, such as llama, chatglm, baichuan, whisper, vicuna, orac, and many others.
+ [epfLLM/Megatron-LLM](https://github.com/epfLLM/Megatron-LLM) ![](https://img.shields.io/github/stars/epfLLM/Megatron-LLM?style=social) : distributed trainer for LLMs.
+ [AmineDiro/cria](https://github.com/AmineDiro/cria) ![](https://img.shields.io/github/stars/AmineDiro/cria?style=social) : OpenAI compatible API for serving LLAMA-2 model.
+ [MOSS](https://github.com/OpenLMLab/MOSS) ![](https://img.shields.io/github/stars/OpenLMLab/MOSS?style=social) : An open-source tool-augmented conversational language model from Fudan University. MOSS是一个支持中英双语和多种插件的开源对话语言模型，moss-moon系列模型具有160亿参数，在FP16精度下可在单张A100/A800或两张3090显卡运行，在INT4/8精度下可在单张3090显卡运行。MOSS基座语言模型在约七千亿中英文以及代码单词上预训练得到，后续经过对话指令微调、插件增强学习和人类偏好训练具备多轮对话能力及使用多种插件的能力。[txsun1997.github.io/blogs/moss.html](https://txsun1997.github.io/blogs/moss.html)
+ [BayLing（百聆）](https://github.com/ictnlp/BayLing) ![](https://img.shields.io/github/stars/OpenLMLab/MOSS?style=social) : “百聆”是一个具有增强的语言对齐的英语/中文大语言模型，具有优越的英语/中文能力，在多项测试中取得ChatGPT 90%的性能。BayLing is an English/Chinese LLM equipped with advanced language alignment, showing superior capability in English/Chinese generation, instruction following and multi-turn interaction. [nlp.ict.ac.cn/bayling](http://nlp.ict.ac.cn/bayling). "BayLing: Bridging Cross-lingual Alignment and Instruction Following through Interactive Translation for Large Language Models". ([arXiv 2023](https://arxiv.org/abs/2306.10968)).
+ [FlagAI（悟道·天鹰（Aquila））](https://github.com/FlagAI-Open/FlagAI) ![](https://img.shields.io/github/stars/FlagAI-Open/FlagAI?style=social) : FlagAI (Fast LArge-scale General AI models) is a fast, easy-to-use and extensible toolkit for large-scale model. Our goal is to support training, fine-tuning, and deployment of large-scale models on various downstream tasks with multi-modality.
+ [YuLan-Chat（玉兰）](https://github.com/RUC-GSAI/YuLan-Chat/) ![](https://img.shields.io/github/stars/RUC-GSAI/YuLan-Chat?style=social) : YuLan-Chat models are chat-based large language models, which are developed by the researchers in GSAI, Renmin University of China (YuLan, which represents Yulan Magnolia, is the campus flower of Renmin University of China). The newest version is developed by continually-pretraining and instruction-tuning [LLaMA-2](https://github.com/facebookresearch/llama) with high-quality English and Chinese data. YuLan-Chat系列模型是中国人民大学高瓴人工智能学院师生共同开发的支持聊天的大语言模型（名字"玉兰"取自中国人民大学校花）。 最新版本基于LLaMA-2进行了中英文双语的继续预训练和指令微调。
+ [Yi-1.5](https://github.com/01-ai/Yi-1.5) ![](https://img.shields.io/github/stars/01-ai/Yi-1.5?style=social) : Yi-1.5 is an upgraded version of Yi, delivering stronger performance in coding, math, reasoning, and instruction-following capability.
+ [智海-录问](https://github.com/zhihaiLLM/wisdomInterrogatory) ![](https://img.shields.io/github/stars/zhihaiLLM/wisdomInterrogatory?style=social) : 智海-录问(wisdomInterrogatory)是由浙江大学、阿里巴巴达摩院以及华院计算三家单位共同设计研发的法律大模型。核心思想：以“普法共享和司法效能提升”为目标，从推动法律智能化体系入司法实践、数字化案例建设、虚拟法律咨询服务赋能等方面提供支持，形成数字化和智能化的司法基座能力。
+ [活字](https://github.com/HIT-SCIR/huozi) ![](https://img.shields.io/github/stars/HIT-SCIR/huozi?style=social) : 活字是由哈工大自然语言处理研究所多位老师和学生参与开发的一个开源可商用的大规模预训练语言模型。 该模型基于 Bloom 结构的70 亿参数模型，支持中英双语，上下文窗口长度为 2048。 在标准的中文和英文基准以及主观评测上均取得同尺寸中优异的结果。
+ [MiLM-6B](https://github.com/XiaoMi/MiLM-6B) ![](https://img.shields.io/github/stars/XiaoMi/MiLM-6B?style=social) : MiLM-6B 是由小米开发的一个大规模预训练语言模型，参数规模为64亿。在 C-Eval 和 CMMLU 上均取得同尺寸最好的效果。
+ [CPM-Bee](https://github.com/OpenBMB/CPM-Bee) ![](https://img.shields.io/github/stars/OpenBMB/CPM-Bee?style=social) : CPM-Bee是一个完全开源、允许商用的百亿参数中英文基座模型，也是[CPM-Live](https://live.openbmb.org/)训练的第二个里程碑。
+ [PandaLM](https://github.com/WeOpenML/PandaLM) ![](https://img.shields.io/github/stars/WeOpenML/PandaLM?style=social) : PandaLM: Reproducible and Automated Language Model Assessment.
+ [Chinese-Tiny-LLM](https://github.com/Chinese-Tiny-LLM/Chinese-Tiny-LLM) ![](https://img.shields.io/github/stars/Chinese-Tiny-LLM/Chinese-Tiny-LLM?style=social) : "Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model". ([arXiv 2024](https://arxiv.org/abs/2404.04167)).
+ [潘多拉 (Pandora)](https://github.com/pengzhile/pandora) ![](https://img.shields.io/github/stars/pengzhile/pandora?style=social) : 潘多拉，一个让你呼吸顺畅的ChatGPT。Pandora, a ChatGPT that helps you breathe smoothly.



