> å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰Large Language Model 
>

---æœ‰æ–°çš„å°½é‡è¡¥å……åœ¨è¿™---

# â­deekseek
05æœˆ28æ—¥æ–°å¢

+ [GitHub - deepseek-ai/DeepSeek-R1](https://github.com/deepseek-ai/DeepSeek-R1)![](https://cdn.nlark.com/yuque/0/2025/png/2639475/1748399041967-f6e17b3a-9aab-4d31-97b6-1f6fe4d35282.png)ï¼šdeepseek R1

# â­Qwen
+ [Qwenï¼ˆé€šä¹‰åƒé—®ï¼‰](https://github.com/QwenLM/Qwen) ![](https://img.shields.io/github/stars/QwenLM/Qwen?style=social) : The official repo of Qwen (é€šä¹‰åƒé—®) chat & pretrained large language model proposed by Alibaba Cloud.
+ [Qwen2](https://github.com/QwenLM/Qwen2) ![](https://img.shields.io/github/stars/QwenLM/Qwen2?style=social) : Qwen2 is the large language model series developed by Qwen team, Alibaba Cloud.
+ 05æœˆ28æ—¥æ–°å¢ Qwen3ï¼ˆ[https://github.com/QwenLM/Qwen3](https://github.com/QwenLM/Qwen3)ï¼‰

# â­GPT
+ GPT-1 : "Improving Language Understanding by Generative Pre-Training". ([cs.ubc.ca, 2018](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)).
+ [GPT-2](https://github.com/openai/gpt-2) ![](https://img.shields.io/github/stars/openai/gpt-2?style=social) : "Language Models are Unsupervised Multitask Learners". ([OpenAI blog, 2019](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)). [Better language models and their implications](https://openai.com/research/better-language-models).
+ [GPT-3](https://github.com/openai/gpt-3) ![](https://img.shields.io/github/stars/openai/gpt-3?style=social) : "GPT-3: Language Models are Few-Shot Learners". ([arXiv 2020](https://arxiv.org/abs/2005.14165)).
+ InstructGPT : "Training language models to follow instructions with human feedback". ([arXiv 2022](https://arxiv.org/abs/2203.02155)). "Aligning language models to follow instructions". ([OpenAI blog, 2022](https://openai.com/research/instruction-following)).
+ [ChatGPT](https://chat.openai.com/): [Optimizing Language Models for Dialogue](https://openai.com/blog/chatgpt).
+ [GPT-4](https://openai.com/product/gpt-4): GPT-4 is OpenAIâ€™s most advanced system, producing safer and more useful responses. "Sparks of Artificial General Intelligence: Early experiments with GPT-4". ([arXiv 2023](https://arxiv.org/abs/2303.12712)). "GPT-4 Architecture, Infrastructure, Training Dataset, Costs, Vision, MoE". ([SemianAlysis, 2023](https://www.semianalysis.com/p/gpt-4-architecture-infrastructure)).
+ [GPT-4-LLM](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM) ![](https://img.shields.io/github/stars/Instruction-Tuning-with-GPT-4/GPT-4-LLM?style=social) : "Instruction Tuning with GPT-4". ([arXiv 2023](https://arxiv.org/abs/2304.03277)). [instruction-tuning-with-gpt-4.github.io/](https://instruction-tuning-with-gpt-4.github.io/)
+ [MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4) ![](https://img.shields.io/github/stars/Vision-CAIR/MiniGPT-4?style=social) : MiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models. [minigpt-4.github.io](https://minigpt-4.github.io/)
+ [minGPT](https://github.com/karpathy/minGPT) ![](https://img.shields.io/github/stars/karpathy/minGPT?style=social) : A minimal PyTorch re-implementation of the OpenAI GPT (Generative Pretrained Transformer) training.
+ [nanoGPT](https://github.com/karpathy/nanoGPT) ![](https://img.shields.io/github/stars/karpathy/nanoGPT?style=social) : The simplest, fastest repository for training/finetuning medium-sized GPTs.
+ [MicroGPT](https://github.com/muellerberndt/micro-gpt) ![](https://img.shields.io/github/stars/muellerberndt/micro-gpt?style=social) : A simple and effective autonomous agent compatible with GPT-3.5-Turbo and GPT-4. MicroGPT aims to be as compact and reliable as possible.
+ [GPT4All](https://github.com/nomic-ai/gpt4all) ![](https://img.shields.io/github/stars/nomic-ai/gpt4all?style=social) : GPT4All: An ecosystem of open-source on-edge large language models. GTP4All is an ecosystem to train and deploy powerful and customized large language models that run locally on consumer grade CPUs.
+ [WorkGPT](https://github.com/h2oai/h2ogpt) ![](https://img.shields.io/github/stars/h2oai/h2ogpt?style=social) : WorkGPT is an agent framework in a similar fashion to AutoGPT or LangChain.
+ [h2oGPT](https://github.com/team-openpm/workgpt) ![](https://img.shields.io/github/stars/team-openpm/workgpt?style=social) : h2oGPT is a large language model (LLM) fine-tuning framework and chatbot UI with document(s) question-answer capabilities. "h2oGPT: Democratizing Large Language Models". ([arXiv 2023](https://arxiv.org/abs/2306.08161)).
+ [DemoGPT](https://github.com/melih-unsal/DemoGPT) ![](https://img.shields.io/github/stars/melih-unsal/DemoGPT?style=social) : Create ğŸ¦œï¸ğŸ”— LangChain apps by just using prompts with the power of Llama 2 ğŸŒŸ Star to support our work! | åªéœ€ä½¿ç”¨å¥å­å³å¯åˆ›å»º LangChain åº”ç”¨ç¨‹åºã€‚ ç»™ä¸ªstaræ”¯æŒæˆ‘ä»¬çš„å·¥ä½œå§ï¼DemoGPT: Auto Gen-AI App Generator with the Power of Llama 2. âš¡ With just a prompt, you can create interactive Streamlit apps via ğŸ¦œï¸ğŸ”— LangChain's transformative capabilities & Llama 2.âš¡ [demogpt.io](https://www.demogpt.io/)
+ [GPT-Engineer](https://github.com/AntonOsika/gpt-engineer) ![](https://img.shields.io/github/stars/AntonOsika/gpt-engineer?style=social) : Specify what you want it to build, the AI asks for clarification, and then builds it. GPT Engineer is made to be easy to adapt, extend, and make your agent learn how you want your code to look. It generates an entire codebase based on a prompt.
+ [1595901624/gpt-aggregated-edition](https://github.com/1595901624/gpt-aggregated-edition) ![](https://img.shields.io/github/stars/1595901624/gpt-aggregated-edition?style=social) : èšåˆChatGPTå®˜æ–¹ç‰ˆã€ChatGPTå…è´¹ç‰ˆã€æ–‡å¿ƒä¸€è¨€ã€Poeã€chatchatç­‰å¤šå¹³å°ï¼Œæ”¯æŒè‡ªå®šä¹‰å¯¼å…¥å¹³å°ã€‚
+ [SpeechGPT](https://github.com/0nutation/SpeechGPT) ![](https://img.shields.io/github/stars/0nutation/SpeechGPT?style=social) : "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities". ([arXiv 2023](https://arxiv.org/abs/2305.11000)).
+ [GPT2-Chinese](https://github.com/Morizeyao/GPT2-Chinese) ![](https://img.shields.io/github/stars/Morizeyao/GPT2-Chinese?style=social) : Chinese version of GPT2 training code, using BERT tokenizer.
+ [gpt-llm-trainer](https://github.com/mshumer/gpt-llm-trainer) ![](https://img.shields.io/github/stars/mshumer/gpt-llm-trainer?style=social) : The goal of this project is to explore an experimental new pipeline to train a high-performing task-specific model. We try to abstract away all the complexity, so it's as easy as possible to go from idea -> performant fully-trained model.

# Llama
+ [Llama 2](https://github.com/facebookresearch/llama) ![](https://img.shields.io/github/stars/facebookresearch/llama?style=social) : Inference code for LLaMA models. "LLaMA: Open and Efficient Foundation Language Models". ([arXiv 2023](https://arxiv.org/abs/2302.13971)). "Llama 2: Open Foundation and Fine-Tuned Chat Models". ([ai.meta.com, 2023-07-18](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)). ([2023-07-18, Llama 2 is here - get it on Hugging Face](https://huggingface.co/blog/llama2)).
+ [Llama 3](https://github.com/meta-llama/llama3) ![](https://img.shields.io/github/stars/meta-llama/llama3?style=social) : The official Meta Llama 3 GitHub site.
+ [Lit-LLaMA](https://github.com/Lightning-AI/lit-llama) ![](https://img.shields.io/github/stars/Lightning-AI/lit-llama?style=social) : âš¡ Lit-LLaMA. Implementation of the LLaMA language model based on nanoGPT. Supports flash attention, Int8 and GPTQ 4bit quantization, LoRA and LLaMA-Adapter fine-tuning, pre-training. Apache 2.0-licensed.
+ [LongLLaMA ](https://github.com/CStanKonrad/long_llama) ![](https://img.shields.io/github/stars/CStanKonrad/long_llama?style=social) : LongLLaMA is a large language model capable of handling long contexts. It is based on OpenLLaMA and fine-tuned with the Focused Transformer (FoT) method.
+ [LLaMA-Adapter](https://github.com/OpenGVLab/LLaMA-Adapter) ![](https://img.shields.io/github/stars/OpenGVLab/LLaMA-Adapter?style=social) : Fine-tuning LLaMA to follow Instructions within 1 Hour and 1.2M Parameters. LLaMA-Adapter: Efficient Fine-tuning of LLaMA ğŸš€
+ [Llama-2-Onnx](https://github.com/microsoft/Llama-2-Onnx) ![](https://img.shields.io/github/stars/microsoft/Llama-2-Onnx?style=social) : Llama 2 Powered By ONNX.
+ [Chinese LLaMA and Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca) ![](https://img.shields.io/github/stars/ymcui/Chinese-LLaMA-Alpaca?style=social) : ä¸­æ–‡LLaMA&Alpacaå¤§è¯­è¨€æ¨¡å‹+æœ¬åœ°CPU/GPUè®­ç»ƒéƒ¨ç½² (Chinese LLaMA & Alpaca LLMs)ã€‚"Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca". ([arXiv 2023](https://arxiv.org/abs/2304.08177)).
+ [Chinese-LLaMA-Alpaca-2](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2) ![](https://img.shields.io/github/stars/ymcui/Chinese-LLaMA-Alpaca-2?style=social) : ä¸­æ–‡ LLaMA-2 & Alpaca-2 å¤§æ¨¡å‹äºŒæœŸé¡¹ç›® (Chinese LLaMA-2 & Alpaca-2 LLMs).
+ [FlagAlpha/Llama2-Chinese](https://github.com/FlagAlpha/Llama2-Chinese) ![](https://img.shields.io/github/stars/FlagAlpha/Llama2-Chinese?style=social) : Llamaä¸­æ–‡ç¤¾åŒºï¼Œæœ€å¥½çš„ä¸­æ–‡Llamaå¤§æ¨¡å‹ï¼Œå®Œå…¨å¼€æºå¯å•†ç”¨ã€‚
+ [michael-wzhu/Chinese-LlaMA2](https://github.com/michael-wzhu/Chinese-LlaMA2) ![](https://img.shields.io/github/stars/michael-wzhu/Chinese-LlaMA2?style=social) : Repo for adapting Meta LlaMA2 in Chinese! METAæœ€æ–°å‘å¸ƒçš„LlaMA2çš„æ±‰åŒ–ç‰ˆï¼ ï¼ˆå®Œå…¨å¼€æºå¯å•†ç”¨ï¼‰
+ [CrazyBoyM/llama3-Chinese-chat](https://github.com/CrazyBoyM/llama3-Chinese-chat) ![](https://img.shields.io/github/stars/CrazyBoyM/llama3-Chinese-chat?style=social) : Llama3 ä¸­æ–‡ç‰ˆã€‚

# Claude
+ [Claude](https://www.anthropic.com/product) : Claude is a next-generation AI assistant based on Anthropicâ€™s research into training helpful, honest, and harmless AI systems.

# ChatGLM
+ [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) ![](https://img.shields.io/github/stars/THUDM/ChatGLM-6B?style=social) : ChatGLM-6B: An Open Bilingual Dialogue Language Model | å¼€æºåŒè¯­å¯¹è¯è¯­è¨€æ¨¡å‹ã€‚ ChatGLM-6B æ˜¯ä¸€ä¸ªå¼€æºçš„ã€æ”¯æŒä¸­è‹±åŒè¯­çš„å¯¹è¯è¯­è¨€æ¨¡å‹ï¼ŒåŸºäº [General Language Model (GLM)](https://github.com/THUDM/GLM) æ¶æ„ï¼Œå…·æœ‰ 62 äº¿å‚æ•°ã€‚ "GLM: General Language Model Pretraining with Autoregressive Blank Infilling". ([ACL 2022](https://aclanthology.org/2022.acl-long.26/)).  "GLM-130B: An Open Bilingual Pre-trained Model". ([ICLR 2023](https://openreview.net/forum?id=-Aw0rrrPUF)).
+ [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B) ![](https://img.shields.io/github/stars/THUDM/ChatGLM2-6B?style=social) : ChatGLM2-6B: An Open Bilingual Chat LLM | å¼€æºåŒè¯­å¯¹è¯è¯­è¨€æ¨¡å‹ã€‚ChatGLM2-6B æ˜¯å¼€æºä¸­è‹±åŒè¯­å¯¹è¯æ¨¡å‹ ChatGLM-6B çš„ç¬¬äºŒä»£ç‰ˆæœ¬ï¼Œåœ¨ä¿ç•™äº†åˆä»£æ¨¡å‹å¯¹è¯æµç•…ã€éƒ¨ç½²é—¨æ§›è¾ƒä½ç­‰ä¼—å¤šä¼˜ç§€ç‰¹æ€§çš„åŸºç¡€ä¹‹ä¸Šï¼ŒChatGLM2-6B å¼•å…¥äº†æ›´å¼ºå¤§çš„æ€§èƒ½ã€æ›´å¼ºå¤§çš„æ€§èƒ½ã€æ›´é«˜æ•ˆçš„æ¨ç†ã€æ›´å¼€æ”¾çš„åè®®ã€‚
+ [ChatGLM3](https://github.com/THUDM/ChatGLM3) ![](https://img.shields.io/github/stars/THUDM/ChatGLM3?style=social) : ChatGLM3 series: Open Bilingual Chat LLMs | å¼€æºåŒè¯­å¯¹è¯è¯­è¨€æ¨¡å‹ã€‚

# ä¹¦ç”Ÿæµ¦è¯­
+ [InternLMï¼ˆä¹¦ç”ŸÂ·æµ¦è¯­ï¼‰](https://github.com/InternLM/InternLM) ![](https://img.shields.io/github/stars/InternLM/InternLM?style=social) : Official release of InternLM2 7B and 20B base and chat models. 200K context support. [internlm.intern-ai.org.cn/](https://internlm.intern-ai.org.cn/)

# ç™¾å·
+ [Baichuan-7Bï¼ˆç™¾å·-7Bï¼‰](https://github.com/baichuan-inc/Baichuan-7B) ![](https://img.shields.io/github/stars/baichuan-inc/Baichuan-7B?style=social) : A large-scale 7B pretraining language model developed by BaiChuan-Inc. Baichuan-7B æ˜¯ç”±ç™¾å·æ™ºèƒ½å¼€å‘çš„ä¸€ä¸ªå¼€æºå¯å•†ç”¨çš„å¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚åŸºäº Transformer ç»“æ„ï¼Œåœ¨å¤§çº¦ 1.2 ä¸‡äº¿ tokens ä¸Šè®­ç»ƒçš„ 70 äº¿å‚æ•°æ¨¡å‹ï¼Œæ”¯æŒä¸­è‹±åŒè¯­ï¼Œä¸Šä¸‹æ–‡çª—å£é•¿åº¦ä¸º 4096ã€‚åœ¨æ ‡å‡†çš„ä¸­æ–‡å’Œè‹±æ–‡ benchmarkï¼ˆC-Eval/MMLUï¼‰ä¸Šå‡å–å¾—åŒå°ºå¯¸æœ€å¥½çš„æ•ˆæœã€‚[huggingface.co/baichuan-inc/baichuan-7B](https://huggingface.co/baichuan-inc/Baichuan-7B)
+ [Baichuan-13Bï¼ˆç™¾å·-13Bï¼‰](https://github.com/baichuan-inc/Baichuan-13B) ![](https://img.shields.io/github/stars/baichuan-inc/Baichuan-13B?style=social) : A 13B large language model developed by Baichuan Intelligent Technology. Baichuan-13B æ˜¯ç”±ç™¾å·æ™ºèƒ½ç»§ Baichuan-7B ä¹‹åå¼€å‘çš„åŒ…å« 130 äº¿å‚æ•°çš„å¼€æºå¯å•†ç”¨çš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼Œåœ¨æƒå¨çš„ä¸­æ–‡å’Œè‹±æ–‡ benchmark ä¸Šå‡å–å¾—åŒå°ºå¯¸æœ€å¥½çš„æ•ˆæœã€‚æœ¬æ¬¡å‘å¸ƒåŒ…å«æœ‰é¢„è®­ç»ƒ (Baichuan-13B-Base) å’Œå¯¹é½ (Baichuan-13B-Chat) ä¸¤ä¸ªç‰ˆæœ¬ã€‚[huggingface.co/baichuan-inc/Baichuan-13B-Chat](https://huggingface.co/baichuan-inc/Baichuan-13B-Chat)
+ [Baichuan2](https://github.com/baichuan-inc/Baichuan2) ![](https://img.shields.io/github/stars/baichuan-inc/Baichuan2?style=social) : A series of large language models developed by Baichuan Intelligent Technology. Baichuan 2 æ˜¯ç™¾å·æ™ºèƒ½æ¨å‡ºçš„æ–°ä¸€ä»£å¼€æºå¤§è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨ 2.6 ä¸‡äº¿ Tokens çš„é«˜è´¨é‡è¯­æ–™è®­ç»ƒã€‚Baichuan 2 åœ¨å¤šä¸ªæƒå¨çš„ä¸­æ–‡ã€è‹±æ–‡å’Œå¤šè¯­è¨€çš„é€šç”¨ã€é¢†åŸŸ benchmark ä¸Šå–å¾—åŒå°ºå¯¸æœ€ä½³çš„æ•ˆæœã€‚æœ¬æ¬¡å‘å¸ƒåŒ…å«æœ‰ 7Bã€13B çš„ Base å’Œ Chat ç‰ˆæœ¬ï¼Œå¹¶æä¾›äº† Chat ç‰ˆæœ¬çš„ 4bits é‡åŒ–ã€‚[huggingface.co/baichuan-inc](https://huggingface.co/baichuan-inc). "Baichuan 2: Open Large-scale Language Models". ([arXiv 2023](https://arxiv.org/abs/2309.10305)).

# æ–‡å¿ƒä¸€è¨€
+ [ç™¾åº¦-æ–‡å¿ƒå¤§æ¨¡å‹](https://wenxin.baidu.com/) : ç™¾åº¦å…¨æ–°ä¸€ä»£çŸ¥è¯†å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ï¼Œæ–‡å¿ƒå¤§æ¨¡å‹å®¶æ—çš„æ–°æˆå‘˜ï¼Œèƒ½å¤Ÿä¸äººå¯¹è¯äº’åŠ¨ï¼Œå›ç­”é—®é¢˜ï¼ŒååŠ©åˆ›ä½œï¼Œé«˜æ•ˆä¾¿æ·åœ°å¸®åŠ©äººä»¬è·å–ä¿¡æ¯ã€çŸ¥è¯†å’Œçµæ„Ÿã€‚
+ [ç™¾åº¦æ™ºèƒ½äº‘-åƒå¸†å¤§æ¨¡å‹](https://cloud.baidu.com/product/wenxinworkshop) : ç™¾åº¦æ™ºèƒ½äº‘åƒå¸†å¤§æ¨¡å‹å¹³å°ä¸€ç«™å¼ä¼ä¸šçº§å¤§æ¨¡å‹å¹³å°ï¼Œæä¾›å…ˆè¿›çš„ç”Ÿæˆå¼AIç”Ÿäº§åŠåº”ç”¨å…¨æµç¨‹å¼€å‘å·¥å…·é“¾ã€‚

# ç›˜å¤å¤§æ¨¡å‹
+ [åä¸ºäº‘-ç›˜å¤å¤§æ¨¡å‹](https://www.huaweicloud.com/product/pangu.html) : ç›˜å¤å¤§æ¨¡å‹è‡´åŠ›äºæ·±è€•è¡Œä¸šï¼Œæ‰“é€ é‡‘èã€æ”¿åŠ¡ã€åˆ¶é€ ã€çŸ¿å±±ã€æ°”è±¡ã€é“è·¯ç­‰é¢†åŸŸè¡Œä¸šå¤§æ¨¡å‹å’Œèƒ½åŠ›é›†ï¼Œå°†è¡Œä¸šçŸ¥è¯†know-howä¸å¤§æ¨¡å‹èƒ½åŠ›ç›¸ç»“åˆï¼Œé‡å¡‘åƒè¡Œç™¾ä¸šï¼Œæˆä¸ºå„ç»„ç»‡ã€ä¼ä¸šã€ä¸ªäººçš„ä¸“å®¶åŠ©æ‰‹ã€‚"Accurate medium-range global weather forecasting with 3D neural networks". ([Nature 2023](https://www.nature.com/articles/s41586-023-06185-3)).

# æ—¥æ—¥æ–°
+ [å•†æ±¤ç§‘æŠ€-æ—¥æ—¥æ–°SenseNova](https://techday.sensetime.com/?utm_source=baidu-sem-pc&utm_medium=cpc&utm_campaign=PC-%E6%8A%80%E6%9C%AF%E4%BA%A4%E6%B5%81%E6%97%A5-%E4%BA%A7%E5%93%81%E8%AF%8D-%E6%97%A5%E6%97%A5%E6%96%B0&utm_content=%E6%97%A5%E6%97%A5%E6%96%B0&utm_term=%E6%97%A5%E6%97%A5%E6%96%B0SenseNova&e_creative=73937788324&e_keywordid=594802524403) : æ—¥æ—¥æ–°ï¼ˆSenseNovaï¼‰ï¼Œæ˜¯å•†æ±¤ç§‘æŠ€å®£å¸ƒæ¨å‡ºçš„å¤§æ¨¡å‹ä½“ç³»ï¼ŒåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹â€œå•†é‡â€ï¼ˆSenseChatï¼‰ã€æ–‡ç”Ÿå›¾æ¨¡å‹â€œç§’ç”»â€å’Œæ•°å­—äººè§†é¢‘ç”Ÿæˆå¹³å°â€œå¦‚å½±â€ï¼ˆSenseAvatarï¼‰ç­‰ã€‚

# æ˜Ÿç«å¤§æ¨¡å‹
+ [ç§‘å¤§è®¯é£-æ˜Ÿç«è®¤çŸ¥å¤§æ¨¡å‹](https://xinghuo.xfyun.cn/) : æ–°ä¸€ä»£è®¤çŸ¥æ™ºèƒ½å¤§æ¨¡å‹ï¼Œæ‹¥æœ‰è·¨é¢†åŸŸçŸ¥è¯†å’Œè¯­è¨€ç†è§£èƒ½åŠ›ï¼Œèƒ½å¤ŸåŸºäºè‡ªç„¶å¯¹è¯æ–¹å¼ç†è§£ä¸æ‰§è¡Œä»»åŠ¡ã€‚

# è±†åŒ…
+ [å­—èŠ‚è·³åŠ¨-è±†åŒ…](https://www.doubao.com/) : è±†åŒ…ã€‚

# å…¶ä»–
+ [Gemma](https://github.com/google/gemma_pytorch) ![](https://img.shields.io/github/stars/google/gemma_pytorch?style=social) : The official PyTorch implementation of Google's Gemma models. [ai.google.dev/gemma](https://ai.google.dev/gemma)
+ [Grok-1](https://github.com/xai-org/grok-1) ![](https://img.shields.io/github/stars/xai-org/grok-1?style=social) : This repository contains JAX example code for loading and running the Grok-1 open-weights model.
+ [Whisper](https://github.com/openai/whisper) ![](https://img.shields.io/github/stars/openai/whisper?style=social) : Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multitasking model that can perform multilingual speech recognition, speech translation, and language identification. "Robust Speech Recognition via Large-Scale Weak Supervision". ([arXiv 2022](https://arxiv.org/abs/2212.04356)).
+ [OpenChat](https://github.com/imoneoi/openchat) ![](https://img.shields.io/github/stars/imoneoi/openchat?style=social) : OpenChat: Advancing Open-source Language Models with Imperfect Data. [huggingface.co/openchat/openchat](https://huggingface.co/openchat/openchat)
+ [StableLM](https://github.com/Stability-AI/StableLM) ![](https://img.shields.io/github/stars/Stability-AI/StableLM?style=social) : StableLM: Stability AI Language Models.
+ [JARVIS](https://github.com/microsoft/JARVIS) ![](https://img.shields.io/github/stars/microsoft/JARVIS?style=social) : JARVIS, a system to connect LLMs with ML community. "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace". ([arXiv 2023](https://arxiv.org/abs/2303.17580)).
+ [Dolly](https://github.com/databrickslabs/dolly) ![](https://img.shields.io/github/stars/databrickslabs/dolly?style=social) : Databricksâ€™ Dolly, a large language model trained on the Databricks Machine Learning Platform. [Hello Dolly: Democratizing the magic of ChatGPT with open models](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html)
+ [LMFlow](https://github.com/OptimalScale/LMFlow) ![](https://img.shields.io/github/stars/OptimalScale/LMFlow?style=social) : An extensible, convenient, and efficient toolbox for finetuning large machine learning models, designed to be user-friendly, speedy and reliable, and accessible to the entire community. Large Language Model for All. [optimalscale.github.io/LMFlow/](https://optimalscale.github.io/LMFlow/)
+ [Colossal-AI](https://github.com/hpcaitech/ColossalAI) ![](https://img.shields.io/github/stars/hpcaitech/ColossalAI?style=social) : Making big AI models cheaper, easier, and scalable. [www.colossalai.org](www.colossalai.org). "Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training". ([arXiv 2021](https://arxiv.org/abs/2110.14883)).
+ [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) ![](https://img.shields.io/github/stars/tatsu-lab/stanford_alpaca?style=social) : Stanford Alpaca: An Instruction-following LLaMA Model.
+ [feizc/Visual-LLaMA](https://github.com/feizc/Visual-LLaMA) ![](https://img.shields.io/github/stars/feizc/Visual-LLaMA?style=social) : Open LLaMA Eyes to See the World. This project aims to optimize LLaMA model for visual information understanding like GPT-4 and further explore the potentional of large language model.
+ [Lightning-AI/lightning-colossalai](https://github.com/Lightning-AI/lightning-colossalai) ![](https://img.shields.io/github/stars/Lightning-AI/lightning-colossalai?style=social) : Efficient Large-Scale Distributed Training with [Colossal-AI](https://colossalai.org/) and [Lightning AI](https://lightning.ai/).
+ [ChatALL](https://github.com/sunner/ChatALL) ![](https://img.shields.io/github/stars/sunner/ChatALL?style=social) :  Concurrently chat with ChatGPT, Bing Chat, bard, Alpaca, Vincuna, Claude, ChatGLM, MOSS, iFlytek Spark, ERNIE and more, discover the best answers. [chatall.ai](http://chatall.ai/)
+ [FreedomIntelligence/LLMZoo](https://github.com/FreedomIntelligence/LLMZoo) ![](https://img.shields.io/github/stars/FreedomIntelligence/LLMZoo?style=social) : âš¡LLM Zoo is a project that provides data, models, and evaluation benchmark for large language models.âš¡ [Tech Report](https://github.com/FreedomIntelligence/LLMZoo/blob/main/assets/llmzoo.pdf)
+ [shm007g/LLaMA-Cult-and-More](https://github.com/shm007g/LLaMA-Cult-and-More) ![](https://img.shields.io/github/stars/shm007g/LLaMA-Cult-and-More?style=social) : News about ğŸ¦™ Cult and other AIGC models.
+ [X-PLUG/mPLUG-Owl](https://github.com/X-PLUG/mPLUG-Owl) ![](https://img.shields.io/github/stars/X-PLUG/mPLUG-Owl?style=social) : mPLUG-OwlğŸ¦‰: Modularization Empowers Large Language Models with Multimodality.
+ [i-Code](https://github.com/microsoft/i-Code) ![](https://img.shields.io/github/stars/microsoft/i-Code?style=social) : The ambition of the i-Code project is to build integrative and composable multimodal Artificial Intelligence. The "i" stands for integrative multimodal learning. "CoDi: Any-to-Any Generation via Composable Diffusion". ([arXiv 2023](https://arxiv.org/abs/2305.11846)).
+ [Lamini](https://github.com/lamini-ai/lamini) ![](https://img.shields.io/github/stars/lamini-ai/lamini?style=social) : Lamini: The LLM engine for rapidly customizing models ğŸ¦™
+ [xorbitsai/inference](https://github.com/xorbitsai/inference) ![](https://img.shields.io/github/stars/xorbitsai/inference?style=social) : Xorbits Inference (Xinference) is a powerful and versatile library designed to serve LLMs, speech recognition models, and multimodal models, even on your laptop. It supports a variety of models compatible with GGML, such as llama, chatglm, baichuan, whisper, vicuna, orac, and many others.
+ [epfLLM/Megatron-LLM](https://github.com/epfLLM/Megatron-LLM) ![](https://img.shields.io/github/stars/epfLLM/Megatron-LLM?style=social) : distributed trainer for LLMs.
+ [AmineDiro/cria](https://github.com/AmineDiro/cria) ![](https://img.shields.io/github/stars/AmineDiro/cria?style=social) : OpenAI compatible API for serving LLAMA-2 model.
+ [MOSS](https://github.com/OpenLMLab/MOSS) ![](https://img.shields.io/github/stars/OpenLMLab/MOSS?style=social) : An open-source tool-augmented conversational language model from Fudan University. MOSSæ˜¯ä¸€ä¸ªæ”¯æŒä¸­è‹±åŒè¯­å’Œå¤šç§æ’ä»¶çš„å¼€æºå¯¹è¯è¯­è¨€æ¨¡å‹ï¼Œmoss-moonç³»åˆ—æ¨¡å‹å…·æœ‰160äº¿å‚æ•°ï¼Œåœ¨FP16ç²¾åº¦ä¸‹å¯åœ¨å•å¼ A100/A800æˆ–ä¸¤å¼ 3090æ˜¾å¡è¿è¡Œï¼Œåœ¨INT4/8ç²¾åº¦ä¸‹å¯åœ¨å•å¼ 3090æ˜¾å¡è¿è¡Œã€‚MOSSåŸºåº§è¯­è¨€æ¨¡å‹åœ¨çº¦ä¸ƒåƒäº¿ä¸­è‹±æ–‡ä»¥åŠä»£ç å•è¯ä¸Šé¢„è®­ç»ƒå¾—åˆ°ï¼Œåç»­ç»è¿‡å¯¹è¯æŒ‡ä»¤å¾®è°ƒã€æ’ä»¶å¢å¼ºå­¦ä¹ å’Œäººç±»åå¥½è®­ç»ƒå…·å¤‡å¤šè½®å¯¹è¯èƒ½åŠ›åŠä½¿ç”¨å¤šç§æ’ä»¶çš„èƒ½åŠ›ã€‚[txsun1997.github.io/blogs/moss.html](https://txsun1997.github.io/blogs/moss.html)
+ [BayLingï¼ˆç™¾è†ï¼‰](https://github.com/ictnlp/BayLing) ![](https://img.shields.io/github/stars/OpenLMLab/MOSS?style=social) : â€œç™¾è†â€æ˜¯ä¸€ä¸ªå…·æœ‰å¢å¼ºçš„è¯­è¨€å¯¹é½çš„è‹±è¯­/ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰ä¼˜è¶Šçš„è‹±è¯­/ä¸­æ–‡èƒ½åŠ›ï¼Œåœ¨å¤šé¡¹æµ‹è¯•ä¸­å–å¾—ChatGPT 90%çš„æ€§èƒ½ã€‚BayLing is an English/Chinese LLM equipped with advanced language alignment, showing superior capability in English/Chinese generation, instruction following and multi-turn interaction. [nlp.ict.ac.cn/bayling](http://nlp.ict.ac.cn/bayling). "BayLing: Bridging Cross-lingual Alignment and Instruction Following through Interactive Translation for Large Language Models". ([arXiv 2023](https://arxiv.org/abs/2306.10968)).
+ [FlagAIï¼ˆæ‚Ÿé“Â·å¤©é¹°ï¼ˆAquilaï¼‰ï¼‰](https://github.com/FlagAI-Open/FlagAI) ![](https://img.shields.io/github/stars/FlagAI-Open/FlagAI?style=social) : FlagAI (Fast LArge-scale General AI models) is a fast, easy-to-use and extensible toolkit for large-scale model. Our goal is to support training, fine-tuning, and deployment of large-scale models on various downstream tasks with multi-modality.
+ [YuLan-Chatï¼ˆç‰å…°ï¼‰](https://github.com/RUC-GSAI/YuLan-Chat/) ![](https://img.shields.io/github/stars/RUC-GSAI/YuLan-Chat?style=social) : YuLan-Chat models are chat-based large language models, which are developed by the researchers in GSAI, Renmin University of China (YuLan, which represents Yulan Magnolia, is the campus flower of Renmin University of China). The newest version is developed by continually-pretraining and instruction-tuning [LLaMA-2](https://github.com/facebookresearch/llama) with high-quality English and Chinese data. YuLan-Chatç³»åˆ—æ¨¡å‹æ˜¯ä¸­å›½äººæ°‘å¤§å­¦é«˜ç“´äººå·¥æ™ºèƒ½å­¦é™¢å¸ˆç”Ÿå…±åŒå¼€å‘çš„æ”¯æŒèŠå¤©çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆåå­—"ç‰å…°"å–è‡ªä¸­å›½äººæ°‘å¤§å­¦æ ¡èŠ±ï¼‰ã€‚ æœ€æ–°ç‰ˆæœ¬åŸºäºLLaMA-2è¿›è¡Œäº†ä¸­è‹±æ–‡åŒè¯­çš„ç»§ç»­é¢„è®­ç»ƒå’ŒæŒ‡ä»¤å¾®è°ƒã€‚
+ [Yi-1.5](https://github.com/01-ai/Yi-1.5) ![](https://img.shields.io/github/stars/01-ai/Yi-1.5?style=social) : Yi-1.5 is an upgraded version of Yi, delivering stronger performance in coding, math, reasoning, and instruction-following capability.
+ [æ™ºæµ·-å½•é—®](https://github.com/zhihaiLLM/wisdomInterrogatory) ![](https://img.shields.io/github/stars/zhihaiLLM/wisdomInterrogatory?style=social) : æ™ºæµ·-å½•é—®(wisdomInterrogatory)æ˜¯ç”±æµ™æ±Ÿå¤§å­¦ã€é˜¿é‡Œå·´å·´è¾¾æ‘©é™¢ä»¥åŠåé™¢è®¡ç®—ä¸‰å®¶å•ä½å…±åŒè®¾è®¡ç ”å‘çš„æ³•å¾‹å¤§æ¨¡å‹ã€‚æ ¸å¿ƒæ€æƒ³ï¼šä»¥â€œæ™®æ³•å…±äº«å’Œå¸æ³•æ•ˆèƒ½æå‡â€ä¸ºç›®æ ‡ï¼Œä»æ¨åŠ¨æ³•å¾‹æ™ºèƒ½åŒ–ä½“ç³»å…¥å¸æ³•å®è·µã€æ•°å­—åŒ–æ¡ˆä¾‹å»ºè®¾ã€è™šæ‹Ÿæ³•å¾‹å’¨è¯¢æœåŠ¡èµ‹èƒ½ç­‰æ–¹é¢æä¾›æ”¯æŒï¼Œå½¢æˆæ•°å­—åŒ–å’Œæ™ºèƒ½åŒ–çš„å¸æ³•åŸºåº§èƒ½åŠ›ã€‚
+ [æ´»å­—](https://github.com/HIT-SCIR/huozi) ![](https://img.shields.io/github/stars/HIT-SCIR/huozi?style=social) : æ´»å­—æ˜¯ç”±å“ˆå·¥å¤§è‡ªç„¶è¯­è¨€å¤„ç†ç ”ç©¶æ‰€å¤šä½è€å¸ˆå’Œå­¦ç”Ÿå‚ä¸å¼€å‘çš„ä¸€ä¸ªå¼€æºå¯å•†ç”¨çš„å¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚ è¯¥æ¨¡å‹åŸºäº Bloom ç»“æ„çš„70 äº¿å‚æ•°æ¨¡å‹ï¼Œæ”¯æŒä¸­è‹±åŒè¯­ï¼Œä¸Šä¸‹æ–‡çª—å£é•¿åº¦ä¸º 2048ã€‚ åœ¨æ ‡å‡†çš„ä¸­æ–‡å’Œè‹±æ–‡åŸºå‡†ä»¥åŠä¸»è§‚è¯„æµ‹ä¸Šå‡å–å¾—åŒå°ºå¯¸ä¸­ä¼˜å¼‚çš„ç»“æœã€‚
+ [MiLM-6B](https://github.com/XiaoMi/MiLM-6B) ![](https://img.shields.io/github/stars/XiaoMi/MiLM-6B?style=social) : MiLM-6B æ˜¯ç”±å°ç±³å¼€å‘çš„ä¸€ä¸ªå¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œå‚æ•°è§„æ¨¡ä¸º64äº¿ã€‚åœ¨ C-Eval å’Œ CMMLU ä¸Šå‡å–å¾—åŒå°ºå¯¸æœ€å¥½çš„æ•ˆæœã€‚
+ [CPM-Bee](https://github.com/OpenBMB/CPM-Bee) ![](https://img.shields.io/github/stars/OpenBMB/CPM-Bee?style=social) : CPM-Beeæ˜¯ä¸€ä¸ªå®Œå…¨å¼€æºã€å…è®¸å•†ç”¨çš„ç™¾äº¿å‚æ•°ä¸­è‹±æ–‡åŸºåº§æ¨¡å‹ï¼Œä¹Ÿæ˜¯[CPM-Live](https://live.openbmb.org/)è®­ç»ƒçš„ç¬¬äºŒä¸ªé‡Œç¨‹ç¢‘ã€‚
+ [PandaLM](https://github.com/WeOpenML/PandaLM) ![](https://img.shields.io/github/stars/WeOpenML/PandaLM?style=social) : PandaLM: Reproducible and Automated Language Model Assessment.
+ [Chinese-Tiny-LLM](https://github.com/Chinese-Tiny-LLM/Chinese-Tiny-LLM) ![](https://img.shields.io/github/stars/Chinese-Tiny-LLM/Chinese-Tiny-LLM?style=social) : "Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model". ([arXiv 2024](https://arxiv.org/abs/2404.04167)).
+ [æ½˜å¤šæ‹‰ (Pandora)](https://github.com/pengzhile/pandora) ![](https://img.shields.io/github/stars/pengzhile/pandora?style=social) : æ½˜å¤šæ‹‰ï¼Œä¸€ä¸ªè®©ä½ å‘¼å¸é¡ºç•…çš„ChatGPTã€‚Pandora, a ChatGPT that helps you breathe smoothly.



