# 什么是 `token`
<br/>color2
对于自然语言，因为它的输入是一段文本，在中文里就是一个一个字，或一个一个词，行业内把这个字或词叫`Token`。

<br/>

# 什么是 `Token`化
## 举例说明 01
如果要使用模型，拿到一段文本的第一件事就是把它`Token`化，可以按字、按词，或其他方式，比如每两个字一组（`Bi-Gram`）。

+ **给定文本**：相信 AI 可以让世界变得更美好。
+ **按字**`**Token**`**化**：我/们/相/信/A/I/可/以/让/世/界/变/得/更/美/好/。
+ **按词**`**Token**`**化**：/相信/AI/可以/让/世界/变得/更/美好/。
+ **按**`**Bi-Gram Token**`**化**：/们相/相信/信A/AI/I可/可以/以让/让世/世界/界变/变得/得更/更美/美好/好。


⚠️应该怎么选择`Token`化的方式？

每种不同的方法都有自己的优点和不足，在大模型之前，按词的方式比较常见。但在有了大模型之后，基本都是按字来了。

<br/>


⚠️要怎么表示这些`Token`？

计算机只能处理数字，所以要想办法把这些`Token`给变成计算机认识的数字才行。就是把所有字作为一个字典，序号就代表它自己。

<br/>

以上面的句子为例，假设词表就包含上面那些字，那么词表就可以用一个`txt`文件存储，内容如下：

```plain
我
们
相
信
A
I
可
以  
让  
世  
界  
变  
得  
更  
美  
好  
。
```

一行一个字，每个字作为一个Token，此时，`0=我`，`1=们`，……，以此类推。拿中文来说，这个词表可能只要几千行，即使包含各种特殊符号、生僻字，也就2万个多点，假设词表大小为`N`。


⚠️如何用这些数字来表示一段文本？

最简单的方法就是用它的 id 直接串起来，但这种表示方法的特征是一维的，也就是说只能表示一个特征。这种方法不太符合实际情况，效果也不理想。

所以，研究人员就想到另一种表示方法：`**One-Hot**`**编码**。其实，将文本变为数字表示的过程本质上就是一种编码过程。

<br/>

`**One-Hot**` 的意思是，对每一个字都有`N`（词表大小）个特征，除了该字的 id 位置值为 1，其余都为 0。

所以把整个词表表示为下面的形式：

> 我 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  
们 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0  
相 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0  
信 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0  
……下面省略
>

此时，对每一个`Token`（字），它的表示就变成了一个一维向量，比如：`「我」：[1,0,...0]`这个向量的长度就是词表的大小N，它被称为「`我`」这个字的`One-Hot`表示。

对于一段文本，一般会将每个`Token`的表示结合起来，结合方式可以采用求和或平均。**这样，对于任意长度的任意文本，都能将其表示为固定大小的向量**，非常方便进行各种矩阵或张量（三维以上的数组）计算，这对深度学习至关重要。

## 举例说明 02
举个例子，比如有这么一句话：`让世界更美好`。现在使用刚刚的方法将其表示为一个向量，采用平均的方式。

### 列出每个字的向量
首先，列出每个字的向量：

> 让 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0  
世 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0  
界 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0  
更 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0  
美 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0  
好 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0  
。 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
>

### 针对每一列取平均
然后针对每一列取平均，结果为：`0 0 0 0 0 0 0 1/7 1/7 1/7 0 0 1/7 1/7 1/7 1/7`。对任意两句话，只要其中包含的字不完全一样，最终得到的向量表示也不会完全一样。

## 产生的问题
当然，在实际使用时，往往不会这么简单的用`1/0`来表示，因为每个字在句子中的作用是不一样的，所以一般会给不同的`Token`赋予不同的权重。最常见的方法是使用在句子中出现的频率，那些高频的被认为是重要的（不是「的」「更」这样的虚词）。更多可以参考[浅析文本分类 —— 情感分析与自然语言处理 | Yam](https://yam.gift/2021/10/27/NLP/2021-10-27-Senta/)。



<br/>color2
这种方法不错，在深度学习之前很长一段时间里都是这样的，不过它有两个很大的问题：

1. **数据维度太高**

太高的维度会导致**向量在空间中聚集在一个非常狭窄的角落**，模型难以训练。

<br/>

> 比如说 3000 个字是 3000 维
>

<br/>color2
2. **数据稀疏**，**向量之间缺乏语义上的交互**（语义鸿沟）

<br/>

> 意思就很多位置是 0，很少位置是 1
>

<br/>color2
比如「我爱吃苹果」和「我爱用苹果」，前者是水果，后者是手机，怎么判断出来的呢？根据上下文。但由于现在这种表示方式，导致**上下文之间是孤立的**，所以模型学不到这个知识点。还有类似「我喜欢你」和「你喜欢我」这样会得到同样的表示，但其实是不同的意思。

<br/>

