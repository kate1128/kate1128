Few-shot（少样本提示） 提示方法存在一些缺陷。对于一些相对简单且无需逻辑推理的问题，LLM 可能通过检索其参数化知识来得出答案，从而表现出色。

然而，**对于一些不太复杂但需要推理的问题**，例如简单的算术应用题，LLM 往往表现不佳。因此，**思维链（Chain-of-Thought，CoT）** 方法应运而生。

## 思维链提示过程
思维链（Chain-of-Thought，CoT）的本质是一种**离散式提示学习**。OpenAI 在 NeurIPS 2022 发表的一篇论文 [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://openreview.net/forum?id=_VjQlMeSB_J) 探索了如何生成思维链：**一系列中间推理步骤**，显著提高了 LLM 执行复杂推理的能力。该论文特别展示了，通过一种称为 “**思维链提示**” 的简单方法，这种推理能力可以在足够大的语言模型中自然地出现。这种方法在提示中提供了一些**思维链的示例**，如下图示意：

[Chain of Thought](https://www.yuque.com/qiaokate/su87gb/xdfm49579ay9goaa?inner=wGK4v)

对于特定的任务，我们需要设计一到多个思维链的示例以进行思维链推理，这实际上是上下文学习的应用。如下图所示，这是一些算术、常识和符号推理基准测试的输入、思维链、输出三元组的示例。

![](attachment:2189f35a-ad96-4723-a9e9-04568b824cb6.png)![](https://cdn.nlark.com/yuque/0/2025/png/2639475/1739936129658-26460a0a-f531-4cd3-8f22-e56255dc9478.png)

<details class="lake-collapse"><summary id="udebaf29c"><span class="ne-text" style="color: var(--jp-content-font-color1)">这里再提供 3 个简单的中文示例：</span></summary><p id="ueec6632a" class="ne-p"><span class="ne-text">问：音乐会原定于 1943 年 6 月 1 日举行，但推迟了一天到今天。10 天前的日期（MM/DD/YYYY）是多少？<br /></span><span class="ne-text">答：06/01/1943 后一天是 06/02/1943，所以今天是 06/02/1943。今天的前 10 天是 1943 年 5 月 23 日。所以答案是 05/23/1943。<br /><br /></span><span class="ne-text">问：输入 1 到 500 的数字需要击键多少次？答案选项：(a) 1156 (b) 1392 (c) 1480 (d) 1562 (e) 1788<br /></span><span class="ne-text">答：1 到 9 有 9 个一位数。10 到 99 有 90 个两位数。100 到 500 有 401 个三位数。9 + 90(2) + 401(3) = 1392。答案是（b）。<br /><br /></span><span class="ne-text">问：把 &quot;Lady Gaga&quot; 中单词的最后一个字母连接起来。<br /></span><span class="ne-text">答：&quot;Lady&quot; 的最后一个字母是 &quot;y&quot;。&quot;Gaga&quot; 的最后一个字母是 &quot;a&quot;。连接起来就是 &quot;ya&quot;。所以答案是 &quot;ya&quot;。</span></p></details>
思维链提示使 LLM 能够处理复杂的算术、常识和符号推理任务。在三个大语言模型（LLMs）上的实验结果表明，思维链提示可以显著提高这些任务的成绩。例如，在 GSM8K 数学单词问题基准测试中，仅用 8 个思维链示例提示 PaLM 540B，就达到了最先进的准确度，甚至超过了带有验证器的经过微调的 GPT-3。这种经验上的收获可能是惊人的。此外，模型通过思维链提示过程获得的性能提升可能与模型的大小成正比。



