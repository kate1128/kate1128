> 参考文档：[https://www.zhihu.com/question/627484732/answer/3261671478](https://www.zhihu.com/question/627484732/answer/3261671478)
>
> 华为昇腾：[https://www.hiascend.com/document/detail/zh/canncommercial/80RC1/devaids/auxiliarydevtool/modelslim_0101.html](https://www.hiascend.com/document/detail/zh/canncommercial/80RC1/devaids/auxiliarydevtool/modelslim_0101.html)
>
> 前提就是大模型太大了，需要一些大模型压缩技术来降低模型的部署成本，提升模型推理性能。
>
> 模型压缩办法有：
>
> 1. 剪枝
> 2. 知识蒸馏
> 3. 量化
>

在机器学习和深度学习中，**量化**（Quantization）是指将浮点数（通常是 32 位浮点数）转换为低精度格式（如 8 位整数）的过程。这通常用于减小模型的存储大小，降低计算需求，提升推理速度，并减少模型在硬件上的能耗。

量化在许多应用中至关重要，特别是当我们需要将深度学习模型部署到资源有限的设备上，如移动设备、嵌入式系统和边缘计算设备时。它可以显著提升模型的执行效率，并使得模型可以在低功耗的硬件上运行。

##  为什么要做量化？
量化的主要目标是：

1. **减少内存占用**：通过将模型权重和激活值从高精度的浮点数（通常是32位）转换为低精度的数据类型（如8位整数），可以显著减少存储需求。
2. **提高计算效率**：低精度操作通常比高精度操作更快，尤其在专用硬件（如GPU、TPU或量化支持的ASIC）上。
3. **节省功耗**：低精度计算和存储的功耗通常低于高精度计算，尤其在移动设备或嵌入式设备上，这对于电池续航非常重要。

## 量化的种类
量化可以在不同的层面进行，可以细分为以下几种类型：

### 权重量化（Weight Quantization）
+ 只对模型的权重进行量化。将模型的权重从32位浮动点数转换为8位整数。这是最常见的量化方式。

   **示例**：

+ 一个32位的浮动点数权重可能会变成一个8位的整数，这样就节省了内存和计算资源。

### 激活量化（Activation Quantization）
+ 对模型的激活值（即每层神经网络的输出）进行量化。

   **示例**：

+ 在神经网络的前向传播过程中，激活值从32位浮动点数转换为8位整数。

### 全量化（Full-precision Quantization）
+ 在这种方法中，不仅对权重进行量化，还对激活进行量化。即整个网络的输入、权重和输出都被量化。

### 对称量化（Symmetric Quantization）与非对称量化（Asymmetric Quantization）
+ **对称量化**：量化使用对称的尺度因子，即输入数据和输出数据的范围是对称的。例如，在量化过程中，正负数的最大值范围相同。
+ **非对称量化**：量化时使用不同的尺度因子和偏移量，这通常适用于某些数据分布不对称的情况。

### 逐层量化与逐通道量化（Layer-wise and Channel-wise Quantization）
+ **逐层量化**：每一层的量化使用相同的尺度因子。
+ **逐通道量化**：每个通道（尤其是卷积层的不同通道）使用独立的量化尺度。

### 动态量化（Dynamic Quantization）
+ 动态量化是指在推理时对模型的权重或激活进行量化，而不是在训练过程中进行量化。常见的应用包括对于预训练模型的量化。

### 静态量化（Static Quantization）
+ 静态量化是在模型训练结束后进行量化，并且量化参数在推理时保持固定。

## 量化的步骤
量化过程通常分为以下几个步骤：

1. **选择量化目标**：
    - 确定需要量化的部分，通常包括权重和激活。
2. **计算量化参数**：
    - 计算每一层或每个权重的最小值和最大值，以确定如何映射到目标的量化精度（如8位整数）。
3. **执行量化**：
    - 使用选定的精度（如8位整数）将浮点数转换为低精度表示。
4. **微调（可选）**：
    - 在量化后进行微调（Quantization-Aware Training, QAT），帮助模型适应量化后的精度损失，进一步提升精度。
5. **验证**：
    - 在量化后的模型上运行推理，确保精度损失在可接受范围内。

## 量化的优缺点
### 优点：
+ **内存占用减少**：量化能显著减小模型的存储需求，使得大规模神经网络能够在资源受限的设备上运行。
+ **推理速度加快**：低精度的计算通常比高精度计算速度更快，尤其是在支持量化硬件的环境下。
+ **低功耗**：量化计算和存储消耗的电量较低，对于移动设备尤为重要。

### 缺点：
+ **精度损失**：量化过程通常会导致模型精度的下降，尤其是在较大的量化精度（如从32位到8位）下。
+ **实现复杂性**：某些硬件平台需要专门的量化支持，并且实现量化可能会增加开发的复杂性。
+ **训练难度**：如果不使用量化感知训练（QAT），量化过程可能导致较大的精度损失。

## 量化感知训练（QAT）
量化感知训练（Quantization-Aware Training, QAT）是一种在训练阶段模拟量化的技术。通过这种方法，在训练过程中引入量化的“噪声”，让模型逐渐适应低精度计算。这样，即使在量化后，模型的精度损失也可以最小化。

QAT的流程通常包括以下步骤：

1. 模拟量化操作，按量化精度（如8位）计算损失函数。
2. 通过反向传播进行训练，使模型能够适应量化后的权重和激活。
3. 在训练结束时，将模型权重保存为低精度格式。

## 量化的实际应用
量化广泛应用于各种深度学习模型的加速和压缩，特别是在资源受限的设备上。例如：

+ **移动设备**：手机、平板等设备上部署深度学习模型，量化可以帮助减少存储需求并加速推理。
+ **嵌入式系统**：嵌入式设备通常处理能力有限，量化能帮助提高模型的运行效率。
+ **边缘计算**：在边缘设备上进行推理时，量化有助于降低计算和存储需求。

许多深度学习框架，如 TensorFlow、PyTorch 和 ONNX，都提供了量化工具和API，帮助用户在训练或部署时应用量化。

## 总结
量化是深度学习模型优化的重要技术之一，尤其在边缘设备和移动设备上进行推理时，能够大幅减少内存和计算开销，提高执行速度。通过量化，模型可以在低功耗、高效的硬件上运行，同时保持较高的准确性。量化感知训练（QAT）则进一步减少了精度损失，使得低精度计算更易于在实际应用中取得成功。

