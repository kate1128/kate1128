**Vision Foundation Modelï¼ˆè§†è§‰åŸºç¡€æ¨¡å‹ï¼Œç®€ç§° VFMï¼‰** æ˜¯ä¸€ç§å¤§å‹é¢„è®­ç»ƒçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œä¸“é—¨ç”¨äºå¤„ç†ä¸è§†è§‰ç›¸å…³çš„ä»»åŠ¡ã€‚è¿™äº›æ¨¡å‹é€šå¸¸æ˜¯é€šè¿‡å¤§è§„æ¨¡çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆè§†è§‰ä¿¡æ¯ï¼Œå¹¿æ³›åº”ç”¨äºè®¡ç®—æœºè§†è§‰é¢†åŸŸã€‚

### 1. **æ¦‚å¿µ**
Vision Foundation Modelï¼ˆVFMï¼‰æ˜¯è§†è§‰é¢†åŸŸä¸­çš„ä¸€ä¸ªæ¦‚å¿µï¼Œæ—¨åœ¨é€šè¿‡å¤§è§„æ¨¡çš„æ•°æ®é¢„è®­ç»ƒï¼Œå­¦ä¹ å¤šç§è§†è§‰ä»»åŠ¡çš„é€šç”¨è¡¨ç¤ºã€‚

è¿™äº›æ¨¡å‹ä¸åªé’ˆå¯¹å•ä¸€ä»»åŠ¡ï¼ˆå¦‚å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒç”Ÿæˆç­‰ï¼‰ï¼Œè€Œæ˜¯èƒ½å¤Ÿåœ¨å¤šä¸ªè§†è§‰ä»»åŠ¡ä¸­æä¾›å¼ºå¤§çš„è¿ç§»èƒ½åŠ›ã€‚ç±»ä¼¼äº **è¯­è¨€æ¨¡å‹**ï¼ˆå¦‚ GPT ç³»åˆ—ï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ä½œç”¨ï¼ŒVFM åœ¨è§†è§‰é¢†åŸŸæä¾›äº†ä¸€ç§å¼ºå¤§çš„åŸºç¡€èƒ½åŠ›ï¼Œèƒ½å¤Ÿé€‚åº”å¹¶å¤„ç†å¤šç§è§†è§‰åº”ç”¨ã€‚

### 2. **æ ¸å¿ƒç‰¹å¾**
+ **å¤šä»»åŠ¡å­¦ä¹ **ï¼šVFM èƒ½å¤Ÿåœ¨å¤šä¸ªè§†è§‰ä»»åŠ¡ä¸­å…±äº«çŸ¥è¯†ï¼Œå¦‚å›¾åƒåˆ†ç±»ã€ç‰©ä½“æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²ã€å›¾åƒç”Ÿæˆã€å›¾åƒ-æ–‡æœ¬å…³è”ç­‰ã€‚
+ **å¤§è§„æ¨¡é¢„è®­ç»ƒ**ï¼šä¸è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç±»ä¼¼ï¼ŒVFM åœ¨å¤§è§„æ¨¡çš„å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œé€šå¸¸åŒ…å«æ•°ç™¾ä¸‡ç”šè‡³æ•°åäº¿å¼ å›¾åƒï¼Œå…·æœ‰å¼ºå¤§çš„ç‰¹å¾å­¦ä¹ å’Œæ³›åŒ–èƒ½åŠ›ã€‚
+ **è·¨æ¨¡æ€èƒ½åŠ›**ï¼šè®¸å¤š VFM è¿˜å…·æœ‰è·¨æ¨¡æ€çš„èƒ½åŠ›ï¼Œå³èƒ½å¤Ÿå¤„ç†å’Œç†è§£å›¾åƒä¸æ–‡æœ¬ä¹‹é—´çš„å…³ç³»ã€‚æ¯”å¦‚ï¼Œæ¨¡å‹ä¸ä»…å¯ä»¥ç†è§£å›¾åƒå†…å®¹ï¼Œè¿˜èƒ½ç”Ÿæˆå›¾åƒçš„æè¿°ï¼Œæˆ–æ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆç›¸åº”çš„å›¾åƒã€‚

### 3. **ä¸å…¶ä»–æ¨¡å‹çš„å…³ç³»**
+ **åŸºç¡€æ¨¡å‹ï¼ˆFoundation Modelï¼‰**ï¼šVFM æ˜¯â€œåŸºç¡€æ¨¡å‹â€ç±»åˆ«çš„ä¸€éƒ¨åˆ†ï¼Œç±»ä¼¼äºåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ä½¿ç”¨çš„ **GPTï¼ˆGenerative Pretrained Transformerï¼‰**ã€**BERTï¼ˆBidirectional Encoder Representations from Transformersï¼‰** ç­‰æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹é€šå¸¸æ˜¯â€œé€šç”¨çš„â€ï¼Œå¯ä»¥è¿ç§»åˆ°å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ï¼ˆä¾‹å¦‚ï¼Œç‰¹å®šçš„å›¾åƒè¯†åˆ«ã€ç›®æ ‡æ£€æµ‹ç­‰ä»»åŠ¡ï¼‰ä¸­ï¼Œè€Œä¸éœ€è¦é‡æ–°è®­ç»ƒã€‚
+ **è§†è§‰å˜æ¢å™¨ï¼ˆVision Transformersï¼ŒViTï¼‰**ï¼šVision Transformer æ˜¯ä¸€ç§åŸºäº Transformer æ¶æ„çš„è§†è§‰æ¨¡å‹ï¼Œåœ¨å¤§è§„æ¨¡è§†è§‰ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚VFM çš„ä¸€äº›å®ä¾‹å¯èƒ½åŸºäº Transformer æˆ–ç±»ä¼¼çš„æ·±åº¦å­¦ä¹ æ¶æ„ã€‚

### 4. **åº”ç”¨**
VFM åœ¨å¤šä¸ªé¢†åŸŸçš„è§†è§‰ä»»åŠ¡ä¸­éƒ½èƒ½å‘æŒ¥ä½œç”¨ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š

+ **å›¾åƒåˆ†ç±»**ï¼šå°†å›¾åƒå½’ç±»åˆ°é¢„å®šä¹‰çš„ç±»åˆ«ä¸­ã€‚
+ **ç›®æ ‡æ£€æµ‹**ï¼šè¯†åˆ«å¹¶å®šä½å›¾åƒä¸­çš„ç›®æ ‡å¯¹è±¡ã€‚
+ **è¯­ä¹‰åˆ†å‰²**ï¼šå°†å›¾åƒåˆ†å‰²æˆå¤šä¸ªåŒºåŸŸï¼Œæ¯ä¸ªåŒºåŸŸä»£è¡¨ä¸åŒçš„ç‰©ä½“æˆ–èƒŒæ™¯ã€‚
+ **è§†è§‰é—®ç­”**ï¼šç»“åˆå›¾åƒå’Œæ–‡æœ¬è¿›è¡Œé—®ç­”ï¼Œæ¨¡å‹æ ¹æ®å›¾åƒå†…å®¹å›ç­”æ–‡æœ¬é—®é¢˜ã€‚
+ **å›¾åƒç”Ÿæˆä¸è½¬æ¢**ï¼šä¾‹å¦‚ï¼Œé€šè¿‡æ–‡æœ¬æè¿°ç”Ÿæˆå›¾åƒï¼ˆæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼‰æˆ–è€…å›¾åƒé£æ ¼è½¬æ¢ï¼ˆå¦‚å°†ä¸€å¼ å›¾åƒè½¬æ¢æˆä¸åŒçš„è‰ºæœ¯é£æ ¼ï¼‰ã€‚

### 5. **å…¸å‹å®ä¾‹**
+ **CLIP (Contrastive Language-Image Pre-training)**ï¼šç”± OpenAI å¼€å‘ï¼ŒCLIP æ˜¯ä¸€ä¸ªå…¸å‹çš„è·¨æ¨¡æ€ VFMï¼Œèƒ½å¤ŸåŒæ—¶ç†è§£å›¾åƒå’Œæ–‡æœ¬ï¼Œå¹¶åœ¨å¤šä¸ªä»»åŠ¡ä¸­æä¾›å¼ºå¤§çš„è¿ç§»èƒ½åŠ›ã€‚
+ **DALLÂ·E**ï¼šä¹Ÿæ˜¯ OpenAI å¼€å‘çš„ä¸€ä¸ªåŸºäº VFM çš„æ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆå›¾åƒï¼Œå±•ç¤ºäº†è§†è§‰åŸºç¡€æ¨¡å‹åœ¨ç”Ÿæˆä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚
+ **DeepMind's Perceiver**ï¼šå¦ä¸€ç§åŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡è§†è§‰è¾“å…¥ç”Ÿæˆæœ‰æ•ˆçš„è¡¨ç¤ºï¼Œæ”¯æŒå¤šç§è§†è§‰å’Œæ„ŸçŸ¥ä»»åŠ¡ã€‚

### 6. **ä¼˜åŠ¿ä¸æŒ‘æˆ˜**
**ä¼˜åŠ¿ï¼š**

+ **è¿ç§»å­¦ä¹ **ï¼šVFM å¯ä»¥åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åè¿ç§»åˆ°ç‰¹å®šçš„ä»»åŠ¡ä¸Šï¼Œæ— éœ€ä»å¤´å¼€å§‹è®­ç»ƒã€‚
+ **å¤šæ¨¡æ€èƒ½åŠ›**ï¼šé€šè¿‡åŒæ—¶å¤„ç†å›¾åƒå’Œæ–‡æœ¬ï¼ŒVFM èƒ½å¤Ÿå®ç°æ›´å¤æ‚çš„ä»»åŠ¡ï¼Œå¦‚å›¾åƒæè¿°ã€å›¾åƒé—®ç­”ç­‰ã€‚
+ **å¯æ‰©å±•æ€§**ï¼šVFM èƒ½å¤Ÿåœ¨ä¸åŒè§„æ¨¡çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œéšç€æ•°æ®é‡å’Œè®¡ç®—èµ„æºçš„å¢åŠ ï¼Œæ€§èƒ½ä¸æ–­æå‡ã€‚

**æŒ‘æˆ˜ï¼š**

+ **è®¡ç®—èµ„æºéœ€æ±‚é«˜**ï¼šè®­ç»ƒå’Œå¾®è°ƒè¿™äº›å¤§è§„æ¨¡æ¨¡å‹éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œå­˜å‚¨ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†æ—¶ã€‚
+ **æ•°æ®å’Œåè§**ï¼šVFM ä¾èµ–äºå¤§è§„æ¨¡çš„æ ‡æ³¨æ•°æ®é›†ï¼Œå¯èƒ½ä¼šå—åˆ°æ•°æ®åè§çš„å½±å“ï¼Œå¯¼è‡´æ¨¡å‹åœ¨æŸäº›ç¾¤ä½“æˆ–åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ã€‚
+ **ä»»åŠ¡ç‰¹å®šè°ƒæ•´**ï¼šå°½ç®¡åŸºç¡€æ¨¡å‹å…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†åœ¨ä¸€äº›ç‰¹å®šä»»åŠ¡ä¸­ï¼Œæ¨¡å‹ä»ç„¶éœ€è¦è¿›è¡Œå¾®è°ƒæ‰èƒ½è¾¾åˆ°æœ€ä¼˜æ€§èƒ½ã€‚

###  æ€»ç»“
Vision Foundation Model æ˜¯ä¸€ä¸ªé€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒè€Œå½¢æˆçš„é€šç”¨è§†è§‰æ¨¡å‹ï¼Œèƒ½å¤Ÿä¸ºå¤šç§è§†è§‰ä»»åŠ¡æä¾›å¼ºå¤§çš„èƒ½åŠ›ã€‚å®ƒåœ¨è®¡ç®—æœºè§†è§‰ä¸­çš„æ½œåŠ›å·¨å¤§ï¼Œä¸ä»…èƒ½å¤Ÿåœ¨å¤šç§ä»»åŠ¡ä¸­æä¾›é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œè¿˜å…·å¤‡è·¨æ¨¡æ€çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿå¤„ç†å›¾åƒä¸æ–‡æœ¬çš„ç»“åˆé—®é¢˜ã€‚éšç€æŠ€æœ¯çš„å‘å±•ï¼ŒVFM å¾ˆå¯èƒ½æˆä¸ºæœªæ¥è§†è§‰æŠ€æœ¯çš„æ ¸å¿ƒåŸºç¡€æ¨¡å‹ä¹‹ä¸€ã€‚

### ï¼ˆVFMï¼‰Vision Foundation Model
+ [Visual ChatGPT](https://github.com/microsoft/visual-chatgpt) ![](https://img.shields.io/github/stars/microsoft/visual-chatgpt?style=social) : Visual ChatGPT connects ChatGPT and a series of Visual Foundation Models to enable sending and receiving images during chatting. "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models". ([arXiv 2023](https://arxiv.org/abs/2303.04671)).
+ [InternImage](https://github.com/OpenGVLab/InternImage) ![](https://img.shields.io/github/stars/OpenGVLab/InternImage?style=social) : "InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions". ([CVPR 2023](https://arxiv.org/abs/2211.05778)).
+ [GLIP](https://github.com/microsoft/GLIP) ![](https://img.shields.io/github/stars/microsoft/GLIP?style=social) : "Grounded Language-Image Pre-training". ([CVPR 2022](https://arxiv.org/abs/2112.03857)).
+ [GLIPv2](https://github.com/microsoft/GLIP) ![](https://img.shields.io/github/stars/microsoft/GLIP?style=social) : "GLIPv2: Unifying Localization and Vision-Language Understanding". ([arXiv 2022](https://arxiv.org/abs/2206.05836)).
+ [DINO](https://github.com/IDEA-Research/DINO) ![](https://img.shields.io/github/stars/IDEA-Research/DINO?style=social) : "DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection". ([ICLR 2023](https://arxiv.org/abs/2203.03605)).
+ [DINOv2](https://github.com/facebookresearch/dinov2) ![](https://img.shields.io/github/stars/facebookresearch/dinov2?style=social) : "DINOv2: Learning Robust Visual Features without Supervision". ([arXiv 2023](https://arxiv.org/abs/2304.07193)).
+ [Grounding DINO](https://github.com/IDEA-Research/GroundingDINO) ![](https://img.shields.io/github/stars/IDEA-Research/GroundingDINO?style=social) : "Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection". ([arXiv 2023](https://arxiv.org/abs/2303.05499)). "çŸ¥ä¹ã€Œä¸‰åˆ†é’Ÿçƒ­åº¦ã€ã€Š[ååˆ†é’Ÿè§£è¯»Grounding DINO-æ ¹æ®æ–‡å­—æç¤ºæ£€æµ‹ä»»æ„ç›®æ ‡](https://zhuanlan.zhihu.com/p/627646794)ã€‹"ã€‚
+ [SAM](https://github.com/facebookresearch/segment-anything) ![](https://img.shields.io/github/stars/facebookresearch/segment-anything?style=social) : The repository provides code for running inference with the Segment Anything Model (SAM), links for downloading the trained model checkpoints, and example notebooks that show how to use the model. "Segment Anything". ([arXiv 2023](https://arxiv.org/abs/2304.02643)).
+ [Grounded-SAM](https://github.com/IDEA-Research/Grounded-Segment-Anything) ![](https://img.shields.io/github/stars/IDEA-Research/Grounded-Segment-Anything?style=social) : Marrying Grounding DINO with Segment Anything & Stable Diffusion & Tag2Text & BLIP & Whisper & ChatBot - Automatically Detect , Segment and Generate Anything with Image, Text, and Audio Inputs. We plan to create a very interesting demo by combining [Grounding DINO](https://github.com/IDEA-Research/GroundingDINO) and [Segment Anything](https://github.com/facebookresearch/segment-anything) which aims to detect and segment Anything with text inputs!
+ [SEEM](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once) ![](https://img.shields.io/github/stars/UX-Decoder/Segment-Everything-Everywhere-All-At-Once?style=social) : We introduce SEEM that can Segment Everything Everywhere with Multi-modal prompts all at once. SEEM allows users to easily segment an image using prompts of different types including visual prompts (points, marks, boxes, scribbles and image segments) and language prompts (text and audio), etc. It can also work with any combinations of prompts or generalize to custom prompts! "Segment Everything Everywhere All at Once". ([arXiv 2023](https://arxiv.org/abs/2304.06718)).
+ [SAM3D](https://github.com/DYZhang09/SAM3D) ![](https://img.shields.io/github/stars/DYZhang09/SAM3D?style=social) : "SAM3D: Zero-Shot 3D Object Detection via [Segment Anything](https://github.com/facebookresearch/segment-anything) Model". ([arXiv 2023](https://arxiv.org/abs/2306.02245)).
+ [ImageBind](https://github.com/facebookresearch/ImageBind) ![](https://img.shields.io/github/stars/facebookresearch/ImageBind?style=social) : "ImageBind: One Embedding Space To Bind Them All". ([CVPR 2023](https://arxiv.org/abs/2305.05665)).
+ [Track-Anything](https://github.com/gaomingqi/Track-Anything) ![](https://img.shields.io/github/stars/gaomingqi/Track-Anything?style=social) : Track-Anything is a flexible and interactive tool for video object tracking and segmentation, based on Segment Anything, XMem, and E2FGVI. "Track Anything: Segment Anything Meets Videos". ([arXiv 2023](https://arxiv.org/abs/2304.11968)).
+ [qianqianwang68/omnimotion](https://github.com/qianqianwang68/omnimotion) ![](https://img.shields.io/github/stars/qianqianwang68/omnimotion?style=social) : "Tracking Everything Everywhere All at Once". ([arXiv 2023](https://arxiv.org/abs/2306.05422)).
+ [LLaVA](https://github.com/haotian-liu/LLaVA) ![](https://img.shields.io/github/stars/haotian-liu/LLaVA?style=social) : ğŸŒ‹ LLaVA: Large Language and Vision Assistant. Visual instruction tuning towards large language and vision models with GPT-4 level capabilities. [llava.hliu.cc](https://llava.hliu.cc/). "Visual Instruction Tuning". ([arXiv 2023](https://arxiv.org/abs/2304.08485)).
+ [M3I-Pretraining](https://github.com/OpenGVLab/M3I-Pretraining) ![](https://img.shields.io/github/stars/OpenGVLab/M3I-Pretraining?style=social) : "Towards All-in-one Pre-training via Maximizing Multi-modal Mutual Information". ([arXiv 2022](https://arxiv.org/abs/2211.09807)).
+ [BEVFormer](https://github.com/fundamentalvision/BEVFormer) ![](https://img.shields.io/github/stars/fundamentalvision/BEVFormer?style=social) : BEVFormer: a Cutting-edge Baseline for Camera-based Detection. "BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers". ([arXiv 2022](https://arxiv.org/abs/2203.17270)).
+ [Uni-Perceiver](https://github.com/fundamentalvision/Uni-Perceiver) ![](https://img.shields.io/github/stars/fundamentalvision/Uni-Perceiver?style=social) : "Uni-Perceiver: Pre-training Unified Architecture for Generic Perception for Zero-shot and Few-shot Tasks". ([CVPR 2022](https://openaccess.thecvf.com/content/CVPR2022/html/Zhu_Uni-Perceiver_Pre-Training_Unified_Architecture_for_Generic_Perception_for_Zero-Shot_and_CVPR_2022_paper.html)).
+ [AnyLabeling](https://github.com/vietanhdev/anylabeling) ![](https://img.shields.io/github/stars/vietanhdev/anylabeling?style=social) : ğŸŒŸ AnyLabeling ğŸŒŸ. Effortless data labeling with AI support from YOLO and Segment Anything! Effortless data labeling with AI support from YOLO and Segment Anything!
+ [X-AnyLabeling](https://github.com/CVHub520/X-AnyLabeling) ![](https://img.shields.io/github/stars/CVHub520/X-AnyLabeling?style=social) : ğŸ’« X-AnyLabeling ğŸ’«. Effortless data labeling with AI support from Segment Anything and other awesome models!
+ [Label Anything](https://github.com/open-mmlab/playground/tree/main/label_anything) ![](https://img.shields.io/github/stars/open-mmlab/playground?style=social) : OpenMMLab PlayGround: Semi-Automated Annotation with Label-Studio and SAM.
+ [RevCol](https://github.com/megvii-research/RevCol) ![](https://img.shields.io/github/stars/megvii-research/RevCol?style=social) : "Reversible Column Networks". ([arXiv 2023](https://arxiv.org/abs/2212.11696)).
+ [Macaw-LLM](https://github.com/lyuchenyang/Macaw-LLM) ![](https://img.shields.io/github/stars/lyuchenyang/Macaw-LLM?style=social) : Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration.
+ [SAM-PT](https://github.com/SysCV/sam-pt) ![](https://img.shields.io/github/stars/SysCV/sam-pt?style=social) : SAM-PT: Extending SAM to zero-shot video segmentation with point-based tracking. "Segment Anything Meets Point Tracking". ([arXiv 2023](https://arxiv.org/abs/2307.01197)).
+ [Video-LLaMA](https://github.com/DAMO-NLP-SG/Video-LLaMA) ![](https://img.shields.io/github/stars/DAMO-NLP-SG/Video-LLaMA?style=social) : "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding". ([arXiv 2023](https://arxiv.org/abs/2306.02858)).
+ [MobileSAM](https://github.com/ChaoningZhang/MobileSAM) ![](https://img.shields.io/github/stars/ChaoningZhang/MobileSAM?style=social) : "Faster Segment Anything: Towards Lightweight SAM for Mobile Applications". ([arXiv 2023](https://arxiv.org/abs/2306.14289)).
+ [BuboGPT](https://github.com/magic-research/bubogpt) ![](https://img.shields.io/github/stars/magic-research/bubogpt?style=social) : "BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs". ([arXiv 2023](https://arxiv.org/abs/2307.08581)).



