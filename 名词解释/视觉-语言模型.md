> [https://www.nvidia.cn/glossary/vision-language-models/](https://www.nvidia.cn/glossary/vision-language-models/)
>

<font style="color:rgb(26, 26, 26);">视觉语言模型 (VLM) 是一种多模态、生成式 AI 模型，能够理解和处理视频、图像和文本。</font>

## <font style="color:rgb(0, 0, 0);">什么是视觉语言模型？</font>
<font style="color:rgb(26, 26, 26);">视觉语言模型是通过将大语言模型 (LLM) 与视觉编码器相结合构建的多模态 AI 系统，使 LLM 具有“看”的能力。</font>

<font style="color:rgb(26, 26, 26);">凭借这种能力，VLM 可以处理并提供对提示中的视频、图像和文本输入的高级理解，以生成文本响应。</font>

![](https://cdn.nlark.com/yuque/0/2025/png/2639475/1751858862484-edb77a38-56ba-4932-8f5f-93eb38b837e6.png)

<font style="color:rgb(26, 26, 26);">图 1：视觉语言模型用例</font>

<font style="color:rgb(26, 26, 26);">与传统的</font>[计算机视觉](https://www.nvidia.cn/glossary/computer-vision/)<font style="color:rgb(26, 26, 26);">模型不同，VLM 不受固定类别集或特定任务 (如分类或检测) 约束。在大量文本和图像/视频字幕对的语料上进行重新训练，VLM 可以用自然语言进行指导，并用于处理许多典型的视觉任务以及新的生成式 AI 任务，例如摘要和视觉问答。</font>

## <font style="color:rgb(0, 0, 0);">为何视觉语言模型很重要？</font>
<font style="color:rgb(26, 26, 26);">为了理解 VLM 的重要性，了解之前的计算机视觉 (CV) 模型的工作原理会很有帮助。传统的基于卷积神经网络 (</font>[CNN](https://www.nvidia.cn/glossary/convolutional-neural-network/)<font style="color:rgb(26, 26, 26);">) 的 CV 模型是在有限类别的 (数据) 集 (合) 上针对特定任务进行训练的。例如：</font>

+ <font style="color:rgb(0, 0, 0);">识别图像中是否包含猫或狗的分类模型</font>
+ <font style="color:rgb(0, 0, 0);">读取图像中的文本，但不对文档的格式或任何视觉数据进行解读的光学字符检测和识别 CV 模型</font>

<font style="color:rgb(26, 26, 26);">以前的 CV 模型是为了特定目的而进行训练的，无法超越其开发和训练的任务或类别集。如果用例发生根本变化或需要向模型添加新类别，开发人员则须收集和标记大量图像并重新训练模型。这是一个昂贵且耗时的过程。此外，CV 模型没有任何自然语言理解。</font>

<font style="color:rgb(26, 26, 26);">VLM 结合基础模型 (如</font><font style="color:rgb(26, 26, 26);"> </font>[CLIP](https://github.com/openai/CLIP)<font style="color:rgb(26, 26, 26);">) 和 LLM 的功能，拥有视觉和语言能力，从而带来了一类新能力。开箱即用，VLM 在各种视觉任务 (如视觉问答、分类和光学字符识别) 上具有强大的零样本性能。它们也非常灵活，不仅可以用于一组固定类别集，而且可以通过简单地更改文本提示用于几乎任何用例。</font>

<font style="color:rgb(26, 26, 26);">使用 VLM 和与 LLM 交互非常类似。用户提供可以与图像交错的文本提示。然后根据输入来生成文本输出。输入提示是开放式的，允许用户向 VLM 发出回答问题、总结、解释内容或使用图像进行推理的指令。用户可以与 VLM 进行多轮对话，并能够在对话上下文中添加图像。VLM 还可以集成到视觉智能体中，从而自主执行视觉任务。</font>

## <font style="color:rgb(0, 0, 0);">视觉语言模型如何工作？</font>
<font style="color:rgb(26, 26, 26);">大多数 VLM 架构由三部分构成：</font>

+ <font style="color:rgb(0, 0, 0);">视觉编码器</font>
+ <font style="color:rgb(0, 0, 0);">投影器 (Projector)</font>
+ <font style="color:rgb(0, 0, 0);">LLM</font>

<font style="color:rgb(26, 26, 26);">视觉编码器通常是一个基于 transformer 架构的 CLIP 模型，该模型已在数百万个图像-文本对进行了训练，具有图像与文本的关联能力。投影器 (Projector) 由一组网络层构成，将视觉编码器的输出转换为 LLM 可以理解的方式，一般解读为图像标记 (tokens)。投影器 (Projector) 可以是如 LLLaVA 与 VILA 中的简单线性层，或者是如 Llama 3.2 Vision 中使用的交叉注意力层更复杂的结构。</font>

<font style="color:rgb(26, 26, 26);">任何现有的 LLM 都可以用来构建 VLM。有数百种结合了各种 LLM 与视觉编码器的 VLM 变体。</font>

![](https://cdn.nlark.com/yuque/0/2025/svg/2639475/1751858862761-2902fff0-f608-401f-af40-9449fdd5dc63.svg)

<font style="color:rgb(26, 26, 26);">图 2：视觉语言模型的通用三部分架构</font>

## <font style="color:rgb(0, 0, 0);">如何训练视觉语言模型？</font>
<font style="color:rgb(26, 26, 26);">VLM 的训练分为几个阶段，包括预训练，之后是监督式微调。或者，参数有效微调 (PEFT) 也可以作为最后阶段在自定义数据上构建特定领域 VLM（的训练方法）。</font>

<font style="color:rgb(26, 26, 26);">预训练阶段将视觉编码器 (encoder)、投影器 (projector) 和 LLM 对齐，使其在解释文本和图像输入时基本上使用相同的语言。这是使用包含图像——标题对与交错图像-文本数据的大量文本及图像语料来完成的。一旦通过预训练将三部分对齐，VLM 就会通过监督微调阶段来帮助了解如何响应用户提示。</font>

<font style="color:rgb(26, 26, 26);">这一阶段使用的数据是示例提示与文本和/或图像输入以及模型的预期响应的混合。例如，这些数据可以是提示模型描述图像或统计该帧内所有目标数量，以及预期正确的响应。经过这一轮训练，VLM 将了解如何最好地解读图像并响应用户提示。</font>

![](https://cdn.nlark.com/yuque/0/2025/svg/2639475/1751858862972-a36bbac6-8ec2-48b4-9d3b-65a3a2e072cf.svg)

<font style="color:rgb(26, 26, 26);">图 3：VLM 训练通常针对模型的特定部分，分几个阶段完成</font>

<font style="color:rgb(26, 26, 26);">VLM 一旦训练完成，可以以与 LLM 相同的方式，即提供提示的方式使用，该提示还可以在文本中穿插图像。然后，VLM 将根据输入生成文本响应。VLM 通常使用 OpenAI 风格的 REST API 接口进行部署，以便于与模型交互。</font>

<font style="color:rgb(26, 26, 26);">目前正在研究更先进的技术来增强视觉能力：</font>

+ <font style="color:rgb(0, 0, 0);">整合视觉编码器来处理图像输入</font>
+ <font style="color:rgb(0, 0, 0);">将高分辨率图像输入分割为更小的图块进行处理</font>
+ <font style="color:rgb(0, 0, 0);">增加上下文长度，以改善长视频理解能力</font>

<font style="color:rgb(26, 26, 26);">所有这些进展都在提升 VLM 的能力，从仅仅理解单一图像输入发展为能够比较与对比图像、准确阅读文本、理解长视频并具有强大空间理解能力的高性能模型。</font>

## <font style="color:rgb(0, 0, 0);">视觉语言模型如何进行基准测试？</font>
<font style="color:rgb(26, 26, 26);">目前存在的常见基准测试，如</font><font style="color:rgb(26, 26, 26);"> </font>[MMMU](https://mmmu-benchmark.github.io/)<font style="color:rgb(26, 26, 26);">、</font>[Video-MME](https://video-mme.github.io/home_page.html)<font style="color:rgb(26, 26, 26);">、</font>[MathVista](https://mathvista.github.io/)<font style="color:rgb(26, 26, 26);">、</font>[ChartQA](https://github.com/vis-nlp/ChartQA)<font style="color:rgb(26, 26, 26);"> </font><font style="color:rgb(26, 26, 26);">和</font><font style="color:rgb(26, 26, 26);"> </font>[DocVQA](https://www.docvqa.org/)<font style="color:rgb(26, 26, 26);">, 用于确定视觉语言模型在各种任务上的表现，例如：</font>

+ <font style="color:rgb(0, 0, 0);">视觉问答</font>
+ <font style="color:rgb(0, 0, 0);">逻辑和推理</font>
+ <font style="color:rgb(0, 0, 0);">文档理解</font>
+ <font style="color:rgb(0, 0, 0);">多图像比较</font>
+ <font style="color:rgb(0, 0, 0);">视频理解</font>

<font style="color:rgb(26, 26, 26);">大多数基准测试由一组图像和几个相关问题组成，通常以多选题的形式呈现。多选题是一致性基准测试和比较 VLM 的最简单方法。这些问题测试 VLM 的感知、知识和推理能力。在运行这些基准测试时，VLM 会收到图像、问题以及它必须做出选择的多选题答案。</font>

![](https://cdn.nlark.com/yuque/0/2025/png/2639475/1751858864806-a48939e3-7138-4d33-9b2c-65303fee86d8.png)

<font style="color:rgb(26, 26, 26);">图4：VLMs (视觉语言类模型) 使用 MMMU 基准测试的多选题示例</font>

<font style="color:rgb(26, 26, 26);">来源：(</font>[MMMU](https://mmmu-benchmark.github.io/)<font style="color:rgb(26, 26, 26);">)</font>

<font style="color:rgb(26, 26, 26);">VLM 的准确度是指一组多选题中做出正确选项的数量。一些基准还包括数字问题，其中 VLM 必须执行特定的计算，并且在答案的一定百分比范围内才被视为正确。这些问题和图像通常来源于学术资料，如大学教材。</font>

## <font style="color:rgb(0, 0, 0);">如何使用视觉语言模型？</font>
<font style="color:rgb(26, 26, 26);">VLM 凭借其灵活性和自然语言理解能力，正迅速成为所有视觉相关任务类型的首选工具。可以通过自然语言轻松指示 VLM 执行各种各样的任务：</font>

1. <font style="color:rgb(0, 0, 0);">视觉问答</font>
2. <font style="color:rgb(0, 0, 0);">图像和视频总结</font>
3. <font style="color:rgb(0, 0, 0);">文本解析和手写文档</font>

<font style="color:rgb(26, 26, 26);">以前需要大量经过特殊训练的模型的应用程序现在只需一个 VLM 即可完成。</font>

<font style="color:rgb(26, 26, 26);">VLM 尤其擅长总结图像内容，并且可以根据内容提示执行特定任务。以教育用例为例 — — 可以向 VLM 提供一张手写数学问题的图像，它可以使用其光学字符识别和推理能力来解读该问题并生成如何解决问题的分步指南。VLM 不仅能够理解图像的内容，还可进行推理并执行特定任务。</font>

![]()

<font style="color:rgb(26, 26, 26);">图 5：视频分析 AI 智能体将视频和图像数据转换为真实世界的见解</font>

<font style="color:rgb(26, 26, 26);">每天都会产生大量的视频，因此审查各行各业制作的大量视频并从中提取见解是不可行的。VLM 可以集成到更大的系统中，以构建在提示时具有检测特定事件能力的视频分析 AI 智能体。这些系统可用于检测仓库中发生故障的机器人，或在货架变空时发出缺货警报。其总体理解超越了单纯的检测，还可以用来生成自动报告。例如，智能交通系统可以检测、分析并生成交通危险报告，如倒下的树木、停滞的车辆或发生碰撞。</font>

<font style="color:rgb(26, 26, 26);">VLM 可与图数据库等技术一起使用来理解长视频。这有助于其捕捉视频中复杂的物体和活动。此类系统可用于总结仓库中的操作以发现瓶颈和低效环节，或为足球、篮球或足球比赛制作体育解说。</font>

## <font style="color:rgb(0, 0, 0);">视觉语言模型面临哪些挑战？</font>
<font style="color:rgb(26, 26, 26);">视觉语言模型正在迅速成熟，但它们仍然存在一些局限性，特别是在空间理解和长上下文视频理解方面。</font>

<font style="color:rgb(26, 26, 26);">多数 VLM 采用基于 CLIP 的模型作为视觉编码器，输入图像大小被限制为 224x224 或 336x336。这种较小的输入图像导致小物体和细节很难被检测到。例如，视频的高清 1080x1920 帧必须压缩或裁剪为更小的输入分辨率，导致很难保留小物体或精细的细节。为了解决这个问题，VLM 开始使用平铺方法，将大图像分解成更小的块，然后输入到模型中。目前还在进行研究，探索使用更高分辨率的图像编码器。</font>

<font style="color:rgb(26, 26, 26);">VLM 也难以提供物体的精确位置。基于 CLIP 的视觉编码器的训练数据主要由图像的简短文本描述 (如标题) 组成。这些描述不包括详细的、细粒度的物体位置，这种限制会影响 CLIP 的空间理解。采用其作为视觉编码器的 VLM 继承了这一限制。新的方法正在探索集成多个视觉编码器来克服这些限制</font><font style="color:rgb(26, 26, 26);"> </font>[2408.15998 (arxiv.org)](https://arxiv.org/pdf/2408.15998)<font style="color:rgb(26, 26, 26);">。</font>

<font style="color:rgb(26, 26, 26);">长视频理解是一项挑战，因为需要考虑长达数小时的视频中的视觉信息才能正确分析或回答问题。与 LLM 一样，VLM 具有有限的上下文长度含义——只能涵盖视频中的一定数量的帧来回答问题。目前正在研究增加上下文长度和在更多基于视频的数据上训练 VLM 的方法，例如 LongVILA</font><font style="color:rgb(26, 26, 26);"> </font>[2408.10188 (arxiv.org)](https://www.arxiv.org/pdf/2408.10188)<font style="color:rgb(26, 26, 26);">。</font>

<font style="color:rgb(26, 26, 26);">对于非常具体的用例 (例如，在特定产品线中发现制造缺陷) 而言，VLM 可能没有看到足够的数据。这些限制可以通过在特定领域的数据上微调 VLM 来克服，或者使用带有上下文学习的多图像 VLM 来提供示例，这些示例可以在不显式训练模型的情况下传授模型新的信息。使用 PEFT 对特定领域数据进行模型训练是另一种可用于提高 VLM 在自定义数据上准确性的技术。</font>

## <font style="color:rgb(0, 0, 0);">如何开始使用视觉语言模型？</font>
<font style="color:rgb(26, 26, 26);">NVIDIA 提供了一些工具来简化视觉语言模型的构建和部署：</font>

+ [NVIDIA NIM](https://build.nvidia.com/explore/vision)<font style="color:rgb(0, 0, 0);">™</font><font style="color:rgb(0, 0, 0);">：NVIDIA NIM 是一组推理微服务，包括行业标准 API、领域特定代码、优化推理引擎和企业运行时。点击</font>[此处](https://build.nvidia.com/explore/vision)<font style="color:rgb(0, 0, 0);">查看当前可用的 VLM NIM。 我们创建了</font><font style="color:rgb(0, 0, 0);"> </font>[NIM 参考工作流](https://github.com/NVIDIA/metropolis-nim-workflows/tree/main)<font style="color:rgb(0, 0, 0);">，帮助您快速上手。</font>
+ [NVIDIA AI Blueprint](https://www.nvidia.cn/ai-data-science/ai-workflows/)<font style="color:rgb(0, 0, 0);">：NVIDIA AI Blueprint 是生成式 AI 用例的参考工作流程，使用 NVIDIA NIM 微服务构建，作为 NVIDIA AI 企业平台的一部分。</font>[用于视频搜索和摘要的 NVIDIA AI Blueprint](https://build.nvidia.com/nvidia/video-search-and-)<font style="color:rgb(0, 0, 0);"> 可帮助您构建和定制交互式视频分析 AI 智能体，该智能体能够使用视觉 VLM、LLM 和 </font>[RAG](https://www.nvidia.cn/glossary/retrieval-augmented-generation/)<font style="color:rgb(0, 0, 0);"> 理解大量实时或存档视频中的活动。</font>

