> 1. <font style="color:rgb(51, 51, 51);">Zero-shot-CoT：在 prompt 的后面加上 Let's think step by step（一步一步地思考）</font>[Zero-shot-CoT](https://www.yuque.com/qiaokate/su87gb/iabo95p0525z9gin)
> 2. LtM 分解子问题解决：[LtM 分解子问题解决](https://www.yuque.com/qiaokate/su87gb/cgxs9alodbbgqoqs)
> 3. 给出多种方案（Self Consistency)：[给出多种方案（Self Consistency)](https://www.yuque.com/qiaokate/su87gb/hs19ui2f4i56tq1z)
> 4. 是否需要提出子问题（Self-Ask）：[是否需要提出子问题（Self-Ask）](https://www.yuque.com/qiaokate/su87gb/fuwf6l37l2soy5l7)
> 5. 思维链（Chain-of-Thought，CoT）**和自我一致性**[Chain of Thought](https://www.yuque.com/qiaokate/su87gb/xdfm49579ay9goaa)**&**[给出多种方案（Self Consistency)](https://www.yuque.com/qiaokate/su87gb/hs19ui2f4i56tq1z)
> 6. **提示优化工具**[优化系统指令模板](https://www.yuque.com/qiaokate/su87gb/zqdld3m4e5ss0nb1)
> 7. **协同推理和行动 ReAct**[协同推理和行动 ReAct](https://www.yuque.com/qiaokate/su87gb/cw0igik4r0b6lcuq)
> 8. **情绪激励提示**[情绪激励提示](https://www.yuque.com/qiaokate/su87gb/mrg454cx47aye5v0)
> 9. **后退式提示 Step-Back Prompting**[后退式提示 Step-Back Prompting](https://www.yuque.com/qiaokate/su87gb/db3tgbuti4cgw262)
> 10. **程序辅助 LLM：PAL**[程序 PALChain 辅助 LLM](https://www.yuque.com/qiaokate/su87gb/kgu30yy9gn71qn98)
> 11. **思维骨架提示 Skeleton of Thought（SoT）**[思维骨架提示SoT](https://www.yuque.com/qiaokate/su87gb/be7qr1xk1ogpslbo)
>

##  扩展包括
+ **思维链和自我一致性**。思维链（Chain-of-Thought，CoT）的本质是一种离散式提示学习。通过生成一系列中间推理步骤，以提高 LLMs 执行复杂推理的能力。当获取思维链提示的示例较少或存在困难时，零样本思维链可以发挥作用。自我一致性扩展了思维链的构建，它生成多个思维链，并通过多数投票选择最终答案。 自我一致性策略利用了这样一种理念，即一个复杂的推理问题通常可以通过多种不同的思维方式得出同一个正确答案。
+ **提示优化工具**。利用 LLM 优化输入提示。我们通常需要遵循一些原则和策略来创建能产生高质量回答的有效提示。实际上，我们也可以利用这些原则和策略来有效地优化提示。
+ **协同推理和行动 ReAct**。`ReAct = Reason + Action = 推理 + 行动`。ReAct 方法通过引导 LLM 生成与特定任务相关的推理文本，从而触发搜索或调用工具的动作，以提高问答任务的性能。
+ **情绪激励提示**。LLMs 可能通过理解心理情绪刺激来提升在某些问答任务上的性能。情绪刺激类提示简单、易用，在许多情况下都值得尝试和关注。
+ **后退式提示 Step-Back Prompting**。一种名为 「后退式提示」的简单提示技术可以使 LLMs 能够进行抽象，从包含特定细节的实例中抽象出高级概念和关键原理。使用获得的高级概念和关键原理来指导推理，LLMs 显着提高了它们遵循正确推理路径的能力。
+ **程序辅助 LLM**。`PAL`：`**P**rogram-**a**ided **L**anguage Models` 使用 Python 程序作为中间推理步骤，将求解和计算的任务外包给一个外部的 Python 解释器，而不是完全依赖语言模型自身。
+ **思维骨架提示**。`Skeleton of Thought（SoT）`并不是按顺序生成答案，而是并行地生成答案的不同部分。更具体地说，当问题给定时，SoT 会首先引导 LLM 构建思维骨架（多个思维要点），然后进行批量解码或并行 API 调用以并行扩展多个要点，最后汇总输出结果以得出最终答案。

##  其他未包含的
+ 生成知识提示。论文：[ACL 2022 - Generated Knowledge Prompting for Commonsense Reasoning](https://arxiv.org/abs/2110.08387)。这种方法从语言模型中生成知识，并在回答问题时将这些知识作为额外的输入。这显示了 LLM 可以作为改进常识推理的灵活的外部知识来源。没有纳入的原因：后退式提示（Step-Back Prompting）是一种更先进的方法，其工作原理与生成背景知识和基础原理来促进推理类似。
+ 链式提示。链式提示（[Prompt Chaining](https://docs.anthropic.com/claude/docs/prompt-chaining)）是一种将任务分解为多个子任务的方法。在确定子任务后，将子任务的提示词提供给 LLM，得到的结果将作为新的提示词的一部分。没有纳入的原因：将任务分解为明确定义的子任务有时可能会很困难，而且制作好的提示链的示例的难度也会因问题的不同而不同。此外，如果对早期提示的回应是错误的，那么可能会在后续提示中引发混乱。
+ 已有一些自动提示工程的实现方法，例如：[APE](https://openreview.net/forum?id=92gvk82DE-)、[AutoPrompt](https://aclanthology.org/2020.emnlp-main.346.pdf)、[PromptBreeder](https://arxiv.org/pdf/2309.16797.pdf)、[PromptAgent](https://arxiv.org/abs/2310.16427) 等。没有纳入的原因：并没有想象中的那样通用、有效和易用，需要针对特定任务进行搜索和优化，目前仍在研究中。
+ [Active Prompting](https://arxiv.org/pdf/2302.12246.pdf)。一种利用思维链主动提示 LLM 的方法。第一步是选择是否使用少量的 CoT 示例来查询 LLM。然后，对一组训练问题生成 k 个可能的答案，并基于这 k 个答案计算不确定度（使用不一致性）。接着，选择最不确定的问题由人类进行注释，然后使用新的注释范例来推断每个问题。没有纳入的原因：基于思维链推理有一点改进，有时需要人类设计的 CoT 推理进行注释，这比较麻烦。
+ [最少到最多提示（Least to Most Prompting，LtM）](https://openreview.net/forum?id=WZH7099tgfM)。它将思维链提示过程（CoT Prompting）进一步发展，与思维链提示过程类似，需要解决的问题被分解成一组建立在彼此之上的子问题。在第二步中，这些子问题被逐个解决。与思维链不同的是，先前子问题的解决方案被输入到提示中，以尝试解决下一个问题。没有纳入的原因：与 CoT 类似。
+ [方向性刺激提示](https://arxiv.org/pdf/2302.11520.pdf)。一种引导黑盒 LLMs 朝着特定预期输出方向前进的新颖框架。这种方法并不直接调整 LLM，而是通过训练一个可调节的策略 LM，为每个输入实例生成辅助的定向刺激提示。这些提示可以作为细微的、针对特定实例的提示和线索，引导 LLM 生成所需的结果，例如在生成的摘要中包含特定的关键词。没有纳入的原因：实现复杂，不易使用。
+ 一些很复杂的提示策略，如 [ToT（Tree of Thoughts）](https://arxiv.org/abs/2305.10601)、[PoT（Program of Thoughts）](https://openreview.net/forum?id=YfZ4ZPt8zd)、[GoT（Graph of Thoughts）](https://arxiv.org/abs/2308.09687)、[AoT （Algorithm of Thoughts）](https://arxiv.org/abs/2308.10379)等。没有纳入的原因：目前这些方法并不具备广泛的适用性，且实现过程相当复杂。它们主要被用于研究如何提升推理能力，并在某些基准测试中刷新最高成绩。对于 GPT-3.5 Turbo 和 GPT-4 已经擅长的许多现有任务，如创意写作、知识问答、文本摘要、情感分析和机器翻译等，诸如 ToT 之类的刻意搜索可能不是必需的。

## 环境配置
+ `Anaconda `
+ `Python 3.8.10`（Python 的版本要求 Python 3.7+）
+ 主要依赖库：`openai==1.10.0`、`langchain==0.1.5`、`langchain-experimental==0.0.50`、`langchain-openai==0.0.5`、`numexpr==2.8.6`、`google-search-results==2.4.2`
+ LLM：`gpt-3.5-turbo-0125`

## 前提
+ 了解与 LLMs 相关的基础概念，如提示学习、提示工程、指令遵循、零样本/少样本提示、上下文学习和检索增强生成等。
+ 了解 OpenAI API 的相关知识，包括密钥、关键参数、消息类型等，并能熟练使用 Python openai 库。

