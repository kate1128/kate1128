> 简单来说就是 如果 prompt 提示词设计合理的话可以调动出来模型的推理能力
>
> 说明学习提示词设计是很有必要的，尽可能发挥模型的能力
>
> 有一些常见的提示词设计方案，先有个概念：
>
> 1. **<font style="color:rgb(51, 51, 51);">Zero-shot-CoT</font>**
>
> <font style="color:rgb(51, 51, 51);">在 prompt 的后面加上 Let's think step by step（一步一步地思考）</font>
>
> 2. **Chain of Thought**
>
> 展示一个相似问题的推理过程，告诉ChatGPT应该这么来（推荐使用👍）
>
> 3. **最少到最多提示过程 (Least to Most prompting, LtM) **
>
> 将问题分解为子问题逐个解决
>
> 4. **Self-Ask **
>
> 在问题拆解之前，先询问LLM这个问题是否需要提出子问题，对具有挑战性的问题进行拆解，一步一步解决，最后给出的答案
>
> 5. **自洽性（Self-consistency）**
>
> 是对 CoT 的一个补充，它不仅仅生成一个思维链，而是生成多个思维链，然后取多数答案作为最终答案。
>

# 什么是推理（Reasoning）？
在做出选择或处理问题时，推理是通过使用基于新的或现有信息的逻辑来理性地评价事物的能力。推理能在决定最佳方案或最能满足的目标的方案之前，平衡两个或多个方案的优点和缺点。它还能帮解决困难、处理不确定性、核实索赔，并仔细评估情况，以确保做出的决定符合的最佳利益。

[Types of Reasoning](https://unacademy.com/content/cat/study-material/data-interpretation-and-logical-reasoning/types-of-reasoning/)

<details class="lake-collapse"><summary id="ufa180e47"><span class="ne-text">场景举例</span></summary><p id="u01077210" class="ne-p" style="text-align: left; text-indent: 2em"><span class="ne-text" style="color: var(--jp-content-font-color1)">比如：最近打算买一台电脑？ 决定之前，首先会考虑的预算，查看预算范围的产品；其次如果追求颜值，接下来会考虑电脑的外观，如果追求性价比，会更关注电脑的硬件，比如CPU、GPU、内存等；此外还有的需求（学习or工作），时间紧迫性等等，综合考虑多个因素，最终得出一个最合适的方案。这是人类的一项宝贵的技能，在人类的生活中拥有广泛的应用。</span></p></details>
![画板](https://cdn.nlark.com/yuque/0/2025/jpeg/2639475/1736134840021-7444d0ee-a793-4d52-8478-1e29b5d196c9.jpeg)

本图来源于[Logical Reasoning](https://www.fibonicci.com/logical-reasoning/)

# 推理类型
常见的推理类型主要包含3种，即亚里士多德在公元前的《前分析篇》中列举了推理的三种类型，为**演绎、归纳以及溯回**。

后来，皮尔斯在此基础上提出了“**溯因推理**”

## 演绎推理（Deductive Reasoning）
[Logical Reasoning](https://www.fibonicci.com/logical-reasoning/)

<br/>color2
一般来说，演绎推理是指使用一组给定的事实或数据，通过逻辑推理来推导出其他事实。演绎推理，也称为**三段论**，通常的理解包括**两个前提，一个大的和一个小的**，然后**一个逻辑结论**，可以用来证明这些新的事实是真实的。

<br/>

例如，经典的例子：

> 大前提：人类都是凡人  
小前提：苏格拉底是人  
结论：苏格拉底是凡人
>

在 "苏格拉底是人"（小前提）的具体情况下，对 "所有的人都是凡人"（大前提）的一般规则应用演绎法，可以得出结论："苏格拉底是凡人"。

## 归纳推理（Inductive Reasoning）
[Logical Reasoning](https://www.fibonicci.com/logical-reasoning/)

<br/>color2
归纳推理是寻找一种模式或趋势，然后对其进行概括。

当对信息进行归纳和推断时，并**不确定这一趋势是否会继续下去，但假设它会继续下去**。因此，并不确定基于归纳推理的结论会是100%的真实。

<br/>

<details class="lake-collapse"><summary id="u1781f1e3"><span class="ne-text">场景举例</span></summary><ol class="ne-ol"><li id="u6ffb69dc" data-lake-index-type="0"><span class="ne-text" style="color: var(--jp-content-font-color1)">一个著名的假说是： &quot;天鹅都是白的&quot;</span></li></ol><p id="u73ebc3fe" class="ne-p" style="text-align: left; text-indent: 2em"><span class="ne-text" style="color: var(--jp-content-font-color1)">这个结论是在没有观察到任何黑天鹅的情况下从大量的观察中得出的，因此在逻辑上假设黑天鹅不存在。所以，归纳推理是一种有风险的逻辑推理形式，因为从天鹅的例子来看，如果发现了一只黑天鹅，那么结论就很容易不正确了。</span></p><ol start="2" class="ne-ol"><li id="u45a08fe6" data-lake-index-type="0" style="text-align: left"><span class="ne-text" style="color: var(--jp-content-font-color1)">在实际能力测试中，另一个常见的归纳推理的例子是数字序列。试着确定模式，归纳和推断，找到该序列的下一个数字。</span></li></ol><div class="ne-quote"><p id="u106649ec" class="ne-p" style="text-align: left; text-indent: 2em"><span class="ne-text" style="color: var(--jp-content-font-color1)">&quot;6, 9, 12, 15, ?&quot;</span></p></div><p id="u0d874d7e" class="ne-p" style="text-align: left; text-indent: 2em"><span class="ne-text" style="color: var(--jp-content-font-color1)">这个趋势的逻辑答案似乎是18，但不可能100%确定，也许这个数字代表的是天或小时或一些意想不到的怪事，这可能导致推断出的结果不同。</span></p></details>
## 溯因推理（Abductive reasoning）
[Logical Reasoning](https://www.fibonicci.com/logical-reasoning/)

<br/>color2
溯因推理与归纳推理有些类似。它最早是由“猜测”一词引入的，因为这里**得出的结论是基于概率的**。在归纳推理中，人们假定最合理的结论也是正确的。

<br/>

例如：

> 大前提：罐子里装满了黄色的弹珠  
小前提：鲍勃手里有一颗黄色的弹珠  
结论：鲍勃手中的黄色弹珠是从罐子里拿出来的。
>

通过溯因推理，鲍勃从罐子里拿走黄色弹珠的可能性是合理的，然而这纯粹是基于推测。黄色弹珠可能是任何人送给鲍勃的，也可能是鲍勃在商店里买的黄色弹珠。因此，从“装满黄色大理石的罐子”的观察中推断出鲍勃拿走了黄色大理石，可能会导致一个错误的结论。

![](https://raw.githubusercontent.com/datawhalechina/hugging-llm/113172197ccc31c52602c7e4fc1651b7b4b1da7f/content/chapter5/images/major-exteriors-of-brain.png)

本图来源于：[What part of the brain is used in reasoning?from How The Brain Learns](https://perpustakaan.gunungsitolikota.go.id/uploaded_files/temporary/DigitalCollection/ZDdhMTYzZDY2OWJjYmU3OWRlYTk3MDVhZDllYjQ5MjhmNDFmNmMxNQ==.pdf)

#  ChatGPT 的使用
> 也可以不用 chatgpt，用其他模型替换测试，先折起来
>

参考：[LLM API 的使用](https://www.yuque.com/qiaokate/su87gb/spwbayudkxvmfd8x#KaODd)

<br/>color2
`Chatgpt`一个对话的`API`调用包含两个必要的输入：

+ `model`: 使用的模型名称（例如：`gpt-3.5-turbo, gpt-4, gpt-4-0314，gpt-4, gpt-4-0314`）
+ `message`: 一个消息对象的列表，每个对象有两个必要的字段：
    - `role`: 信使的角色，包括：`system、user，or assistant`)
    - `content`: 信息的内容，例如: 给我写一首美丽的诗

<br/>

其中，消息也可以包含一个可选的名称字段，它给信使一个名字。例如：`example-user、Alice、BlackbeardBot`。名称中不得包含空格。

通常情况下，对话会以一个告诉助手如何行事的系统消息开始，然后是用户和助手的消息交替出现，但可以不遵循这种格式。

```python
from openai import OpenAI

client = OpenAI(
    base_url='https://xiaoai.plus/v1',
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)

# Example OpenAI Python library request
MODEL = "gpt-3.5-turbo"

response =  client.chat.completions.create(
    model=MODEL,
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Knock knock."},
        {"role": "assistant", "content": "Who's there?"},
        {"role": "user", "content": "Orange."},
    ],
    temperature=0,
)

response
```

测试一下，显示这个

<details class="lake-collapse"><summary id="ud90c2766"><span class="ne-text" style="color: var(--jp-content-font-color1)">output：</span></summary><pre data-language="json" id="iOXbm" class="ne-codeblock language-json"><code>ChatCompletion(id='chatcmpl-yagqlvRBo9sQEyZrHyHZEFoOIggqO', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Orange who?', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1736231646, model='gpt-3.5-turbo', object='chat.completion', service_tier=None, system_fingerprint='fp_808245b034', usage=CompletionUsage(completion_tokens=3, prompt_tokens=35, total_tokens=38, completion_tokens_details=None, prompt_tokens_details=None))</code></pre></details>
<br/>color2
响应对象包括下面几个字段：

+ `id`: 请求的ID
+ `object`: 返回的对象的类型（如chat.completion）
+ `created`: 请求的时间戳
+ `model`: 用于生成回复的模型的全名
+ `usage`: 用于生成回复的token数量，包括提示、完成和总数。
+ `choices`: 完成对象的列表(只有一个，除非设置n大于1)
    - `message`: 由模型生成的消息对象，包括角色和内容
    - `finish_reason`: 模型停止生成文本的原因（要么是停止，要么是长度，如果达到`max_tokens`的限制）
    - `index`: 选择列表中的完成度的索引

<br/>

# 测试ChatGPT的推理能力
针对常见的推理任务，对ChatGPT进行测试

## 演绎推理（Deductive Reasoning）
```python
from openai import OpenAI

client = OpenAI(
    base_url='https://xiaoai.plus/v1',
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)

# Example OpenAI Python library request
MODEL = "gpt-3.5-turbo"

response =  client.chat.completions.create(
    model=MODEL,
    messages=[
      {"role": "user", "content": "大前提：人类都是凡人 \n \
                                     小前提：苏格拉底是人 \n \
                                     结论："},
    ],
    temperature=0,
)

response.choices[0].message.content
```

<details class="lake-collapse"><summary id="ue375d2e8"><span class="ne-text" style="color: var(--jp-content-font-color1)">output：</span></summary><pre data-language="json" id="Z4bEz" class="ne-codeblock language-json"><code>苏格拉底是凡人。</code></pre></details>
## 归纳推理（Inductive Reasoning）
1. 西瓜是甜的，香瓜是甜的，所以叫“瓜”的蔬果都应该

```python
from openai import OpenAI

client = OpenAI(
    base_url='https://xiaoai.plus/v1',
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)

# Example OpenAI Python library request
MODEL = "gpt-3.5-turbo"

response =  client.chat.completions.create(
    model=MODEL,
    messages=[
        {"role": "user", "content": "西瓜是甜的，香瓜是甜的，所以叫“瓜”的蔬果都应该 \n \
                                     结论："},
    ],
    temperature=0,
)

response.choices[0].message.content
```

<details class="lake-collapse"><summary id="u07a26851"><span class="ne-text" style="color: var(--jp-content-font-color1)">output：</span></summary><pre data-language="json" id="vUtCD" class="ne-codeblock language-json"><code>都是甜的。</code></pre></details>
2. `6, 9, 12, 15, ? `

```python
from openai import OpenAI

client = OpenAI(
    base_url='https://xiaoai.plus/v1',
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)

# Example OpenAI Python library request
MODEL = "gpt-3.5-turbo"

response =  client.chat.completions.create(
    model=MODEL,
    messages=[
        {"role": "user", "content": "6, 9, 12, 15, ? \n \
                                     结论："},
    ],
    temperature=0,
)

response.choices[0].message.content
```

<details class="lake-collapse"><summary id="u9c156252"><span class="ne-text" style="color: var(--jp-content-font-color1)">output：</span></summary><pre data-language="json" id="xz73U" class="ne-codeblock language-json"><code>18</code></pre></details>
## 溯因推理（Abductive reasoning）
1. 大前提：罐子里装满了黄色的弹珠，小前提：鲍勃手里有一颗黄色的弹珠 ，问题：鲍勃手里的弹珠来自哪里？

```python
from openai import OpenAI

client = OpenAI(
    base_url='https://xiaoai.plus/v1',
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)

# Example OpenAI Python library request
MODEL = "gpt-3.5-turbo"

response =  client.chat.completions.create(
    model=MODEL,
    messages=[
        {"role": "user", "content": "大前提：罐子里装满了黄色的弹珠  \n \
                                    小前提：鲍勃手里有一颗黄色的弹珠  \n \
                                    问题：鲍勃手里的弹珠来自哪里？"},
    ],
    temperature=0,
)

response.choices[0].message.content
```

<details class="lake-collapse"><summary id="udd90bee0"><span class="ne-text" style="color: var(--jp-content-font-color1)">output：</span></summary><pre data-language="json" id="Wqcfo" class="ne-codeblock language-json"><code>'结论：鲍勃手里的弹珠来自罐子里。因为大前提说明罐子里装满了黄色的弹珠，而小前提表明鲍勃手里有一颗黄色的弹珠，所以可以推断鲍勃手里的弹珠来自罐子里。'</code></pre></details>
2. 如果电源线路接触不好，那么日光灯熄灭；日光灯熄灭，所以:

```python
response = openai.ChatCompletion.create(
    model=MODEL,
    messages=[
        {"role": "user", "content": "如果电源线路接触不好，那么日光灯熄灭；日光灯熄灭，\n \
                                     所以: "},
    ],
    temperature=0,
)

print(response['choices'][0]['message']['content'])
```

<details class="lake-collapse"><summary id="u1450ed5a"><span class="ne-text">output：</span></summary><pre data-language="json" id="I8Fax" class="ne-codeblock language-json"><code>'要检查电源线路是否接触良好，以确保日光灯正常工作。'</code></pre></details>
## 三者之间的关系
皮尔斯如何使用“豆子实例”来阐述三者的关系

### 演绎推理
```python
from openai import OpenAI

client = OpenAI(
    base_url='https://xiaoai.plus/v1',
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)

# Example OpenAI Python library request
MODEL = "gpt-3.5-turbo"

response =  client.chat.completions.create(
    model=MODEL,
    messages=[
         {"role": "user", "content": "这个袋子中的豆子都是白色的。\n \
                                     这些豆子来自这个袋子。\n \
                                     这些豆子是"},
    ],
    temperature=0,
)

response.choices[0].message.content
```

<details class="lake-collapse"><summary id="u34b93d69"><span class="ne-text">output：</span></summary><pre data-language="json" id="bcZu8" class="ne-codeblock language-json"><code>白色的。</code></pre></details>
### 归纳推理
```python
from openai import OpenAI

client = OpenAI(
    base_url='https://xiaoai.plus/v1',
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)

# Example OpenAI Python library request
MODEL = "gpt-3.5-turbo"

response =  client.chat.completions.create(
    model=MODEL,
    messages=[
        {"role": "user", "content": "这些豆子来自这个袋子。\n \
                                     这些豆子是白色的。\n \
                                     这个袋子中的豆子都是"},
    ],
    temperature=0,
)

response.choices[0].message.content
```

<details class="lake-collapse"><summary id="ufbfdb580"><span class="ne-text">output：</span></summary><pre data-language="json" id="FSafw" class="ne-codeblock language-json"><code>白色的。</code></pre></details>


### 溯因推理
1. 这个袋子中的豆子都是白色的。这些豆子是白色的。这些豆子来自哪里？

```python
from openai import OpenAI

client = OpenAI(
    base_url='https://xiaoai.plus/v1',
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)

# Example OpenAI Python library request
MODEL = "gpt-3.5-turbo"

response =  client.chat.completions.create(
    model=MODEL,
    messages=[
        {"role": "user", "content": "这个袋子中的豆子都是白色的。\n \
                                     这些豆子是白色的。\n \
                                     这些豆子来自哪里？"},
    ],
    temperature=0,
)

response.choices[0].message.content
```

<details class="lake-collapse"><summary id="u1279f7f9"><span class="ne-text">output：</span></summary><pre data-language="json" id="gVWw1" class="ne-codeblock language-json"><code>'这些豆子很可能是大豆，通常生长在温带地区，如美国、巴西、阿根廷等地。不过具体来自哪里还需要进一步了解。'</code></pre></details>


2. 这个袋子中的豆子都是白色的。这些豆子是白色的。这些豆子来自哪里？ 请进行溯因推理

```python
from openai import OpenAI

client = OpenAI(
    base_url='https://xiaoai.plus/v1',
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)

# Example OpenAI Python library request
MODEL = "gpt-3.5-turbo"

response =  client.chat.completions.create(
    model=MODEL,
    messages=[
        {"role": "user", "content": "这个袋子中的豆子都是白色的。\n \
                                     这些豆子是白色的。\n \
                                     这些豆子来自哪里？ 请进行溯因推理"},
    ],
    temperature=0,
)

response.choices[0].message.content
```

<details class="lake-collapse"><summary id="u8f5f45a9"><span class="ne-text">output：</span></summary><pre data-language="json" id="sBsNd" class="ne-codeblock language-json"><code>'这些豆子来自袋子里，因为句子提到了“这个袋子中的豆子都是白色的”。所以这些白色豆子来自这个袋子中。'</code></pre></details>
ChatGPT可以对演绎、归纳类型的问题有很好的感知，而在溯因推理这块似乎很符合一个正常人的思维。皮尔斯曾指出：“只要是关于科学的观念，就都利用外展（溯因）得而得出的，而且这些观念都可以利用演绎来进行验证”。哪一天，AI拥有了真正完整的推理能力，那才是真正想象中的AI。

# 调用ChatPT的推理能力
人们在生活中，面对简单的问题，很多时候仅仅凭直觉可以应对，比如`1+1=2`，但当面对一个很复杂的事情时，过于棘手甚至会让转不过弯来，这个时候就得静一下来，仔细思考一下，如何解决。


⚠️如何让`ChatGPT`有思考呢？

1. 问题结束，加上让它思考的魔法语句，比如：`"Let's think step by step."` , 让`ChatGPT`思考起来
2. 给一个或者几个例子，让`ChatGPT`类比学习一下
3. 让`ChatGPT`使用多种思路回答，最后综合一下，优中选优

<br/>

![](https://raw.githubusercontent.com/datawhalechina/hugging-llm/113172197ccc31c52602c7e4fc1651b7b4b1da7f/content/chapter5/images/think.jpg)

如何调用模型的推理能力，就是要设计合理的提示词：

<br/>color2
1. **<font style="color:rgb(51, 51, 51);">Zero-shot-CoT</font>**

<font style="color:rgb(51, 51, 51);">在 prompt 的后面加上 Let's think step by step（一步一步地思考）</font>

2. **Chain of Thought**

展示一个相似问题的推理过程，告诉ChatGPT应该这么来（推荐使用👍）

3. **最少到最多提示过程 (Least to Most prompting, LtM) **

将问题分解为子问题逐个解决

4. **Self-Ask **

在问题拆解之前，先询问LLM这个问题是否需要提出子问题，对具有挑战性的问题进行拆解，一步一步解决，最后给出的答案

5. **自洽性（Self-consistency）**

是对 CoT 的一个补充，它不仅仅生成一个思维链，而是生成多个思维链，然后取多数答案作为最终答案。

<br/>

# ChatGPT、GPT-4推理对比
以下对ChatGPT和GPT-4的推理能力的评测来源于[Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4](https://arxiv.org/pdf/2304.03439v1.pdf)。该论文分析了多个逻辑推理数据集，包括LogiQA和ReClor等主流数据集，以及ARLSAT等新发布的数据集。

实验结果显示，ChatGPT和GPT-4在大多数逻辑推理数据集上的表现优于传统的微调方法，表明这两个模型能够更好地进行逻辑推理。GPT-4相比拥有更好的表现, 但是在处理新发布的和分布外（OOD）数据集时，性能明显下降。对于ChatGPT和GPT-4来说，逻辑推理仍然具有挑战性，特别是在OOD和自然语言推理数据集上。

![](https://raw.githubusercontent.com/datawhalechina/hugging-llm/113172197ccc31c52602c7e4fc1651b7b4b1da7f/content/chapter5/images/RT.png)

本图来源于[Source:Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4by Hanmeng Liu et al. (2022)](https://arxiv.org/abs/2304.03439)

此外，LogiQA 2.0 ood数据集上，该论文作者团队挑选了20个之前GPT-4回答错误的问题，通过使用Zero-shot COT的方法，即加上“Let's think step by step”，引导模型进行更多的推理，正确回答了20个问题中的4个。所以无论是ChatGPT、还是强大如GPT-4在仍然需要更好引导下（比如使用COT的方法），可以进一步提升其准确性和泛化能力。

# 相关文献
+ 【1】[Techniques to Improve Reliability](https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md)
+ 【2】[Learn Prompting](https://learnprompting.org/)
+ 【3】[ChatGPT“智能”测试：ChatGPT 对逻辑学基本概念的“理解掌握”程度](https://blog.sciencenet.cn/blog-2371919-1376282.html)
+ 【4】[https://yam.gift/2023/01/31/NLP/2023-01-31-ChatGPT-Prompt-Example/](https://yam.gift/2023/01/31/NLP/2023-01-31-ChatGPT-Prompt-Example/)
+ 【5】[Large Language Models are Zero-Shot Reasoners](https://arxiv.org/pdf/2205.11916.pdf)
+ 【6】[Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/pdf/2201.11903.pdf)
+ 【7】[Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/pdf/2203.11171.pdf)
+ 【8】[Measuring and Narrowing the Compositionality Gap in Language Models](https://arxiv.org/pdf/2210.03350.pdf)
+ 【9】[Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4](https://arxiv.org/pdf/2304.03439v1.pdf)
+ 【10】[Least-to-Most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/pdf/2205.10625.pdf)
+ 【11】[增强ChatGPT回答的逻辑性](https://mp.weixin.qq.com/s?__biz=MzI1MTE3MTIwOQ==&mid=2247483750&idx=1&sn=ef559cfaadda6947e99ea0a16e36a69d&chksm=e9f65980de81d09616e4be6d7a46a39c1f878812b247b8a5a4dea7eceade827ea9491b10211d#rd)

