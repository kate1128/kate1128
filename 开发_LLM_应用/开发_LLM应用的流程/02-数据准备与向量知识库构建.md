本项目实现原理如下图所示

[Langchain-Chatchat/docs/img/langchain+chatglm.png at master · chatchat-space/Langchain-Chatchat](https://github.com/chatchat-space/Langchain-Chatchat/blob/master/docs/img/langchain%2Bchatglm.png)

1. 加载本地文档 
2. 读取文本
3. 文本分割
4. 文本向量化
5. question 向量化
6. 在文本向量中匹配出与问句向量最相似的 top k 个 
7. 匹配出的文本作为上下文和问题一起添加到 Prompt 中 
8. 提交给 LLM 生成回答

![](https://cdn.nlark.com/yuque/0/2024/png/2639475/1735282903726-22d25ff2-b54f-4259-a0a6-8af35a16233b.png)

### 收集和整理用户提供的文档
用户常用文档格式有 PDF、TXT、MD 等，首先，我们可以使用 LangChain 的文档加载器模块方便地加载用户提供的文档，或者使用一些成熟的 Python 包进行读取。

由于目前大模型使用 token 的限制，我们需要对读取的文本进行切分，将较长的文本切分为较小的文本，这时一段文本就是一个单位的知识。

### 将文档词向量化
使用 **文本嵌入(Embeddings)技术 **对分割后的文档进行向量化，使语义相似的文本片段具有接近的向量表示。然后，存入向量数据库，完成 **索引(index)** 的创建。

利用向量数据库对各文档片段进行索引，可以实现快速检索。

### 将向量化后的文档导入 Chroma 知识库，建立知识库索引
Langchain 集成了超过 30 个不同的向量数据库。Chroma 数据库轻量级且数据存储在内存中，这使得它非常容易启动和开始使用。

将用户知识库内容经过 Embedding 存入向量数据库，然后用户每一次提问也会经过 Embedding，利用向量相关性算法（例如余弦算法）找到最匹配的几个知识库片段，将这些知识库片段作为上下文，与用户问题一起作为 Prompt 提交给 LLM 回答。



