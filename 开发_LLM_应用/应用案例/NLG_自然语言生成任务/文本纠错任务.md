## 什么是文本纠错？
在日常生活中，不管是微信聊天、微博推文甚至是出版书籍中，我们都或多或少地会发现文本中的错别字现象。

这些错别字可能源于语音输入时的口音偏差，如“飞机”被识别成了“灰机”；也可能是拼音输入时误触了临近键位或者选错了结果，如“飞机”被识别成了“得急”、“肥鸡”；亦或是手写输入时写成了形近字，如“战栗”被识别为了“战粟”……

常见的错误类型包括：

+ 拼写错误：中文课程->中文磕碜；明天会议->明天会易
+ 语法错误：他昨天去参加会议了->他昨天将要去参加会议
+ 标点符号错误：您好，请多指教！->您好,请多指教???
+ 知识性错误：上海黄浦区->上海黄埔区
+ 重复性错误：您好，请问您今天有空吗？->您好，请问您今天有空吗吗吗吗吗吗
+ 遗漏性错误：他昨天去参加会议了->他昨天去参加了
+ 语序性错误：他昨天去参加会议了->他昨天去会议参加了
+ 多语言错误：他昨天去参加会议了->他昨天去参加huiyi了
+ ……

总之，文本错误可能是千奇百怪的。对于人类而言，凭借着常识与上下文，实现语义理解尚不是什么难事，有时只是些许影响阅读体验；而对于一些特定的文本下游任务，如命名实体识别或意图识别，一条不加处理的错误输入文本可能会导致南辕北辙的识别结果。|

文本纠错任务指的是通过自然语言处理技术对文本中出现的错误进行检测和纠正的过程。目前已经成为自然语言处理领域中的一个重要分支，被广泛地应用于搜索引擎、机器翻译、智能客服等各种领域。纵然由于文本错误的多样性，我们往往难以将所有错误通通识别并纠正成功，但是如果能尽可能多且正确地识别文本中的错误，能够大大降低人工审核的成本，也不失为一桩美事~

## 常见的文本纠错技术
常见的文本纠错技术主要有以下几种：

1. 基于规则的文本纠错技术
2. 基于语言模型的文本纠错技术
3. 基于MLM的文本纠错技术
4. 基于NLG的文本纠错技术

### 基于规则的文本纠错技术
这种文本纠错技术是通过实现定义的规则来检查文本中的拼写、语法、标点符号等常见错误，假如“金字塔”常被误写为“金子塔”，则在数据库中加入两者的映射关系。由于这种传统方法需要大量的人工工作以及专家对于语言的深刻理解，因此难以处理海量文本或较为复杂的语言错误。

### 基于语言模型的文本纠错技术
基于语言模型的文本纠错技术包括错误检测和错误纠正，这种方法同样比较简单粗暴，方法速度快，扩展性强，效果一般。常见的模型有Kenlm。

+ 错误检测：使用`jieba`中文分词器对句子进行切词，然后结合字粒度和词粒度两方面的疑似错误结果，形成疑似错误位置候选集。
+ 错误纠正：遍历所有的候选集并使用音似、形似词典替换错误位置的词，然后通过语言模型计算句子困惑度，最后比较并排序所有候选集结果，得到最优纠正词。

### 基于MLM的文本纠错技术
我们知道，BERT在预训练阶段使用了Masked Language Model掩码语言模型（MLM）及Next Sentence Prediction下一句预测（NSP）两个任务，其中MLM任务中有15%*10%的Token会被替换为随机的其他词汇，迫使模型更多地依赖于上下文信息去预测Mask词汇，在一定程度上赋予了模型纠错能力。

因此，我们将BERT的MLM任务做一下简单的修改，将输入设计为错误的词汇，输出为正确的词汇，做一下简单的fine tune，即可轻松实现文本纠错功能。

例如，ACL2020的Soft-Masked BERT模型（[论文笔记](https://www.cnblogs.com/peng-yuan/p/15412346.html)），设计了一个二重网络来进行文本纠错，其中“错误检测网络”通过Bi-GRU识别每个字符错误的概率，“错误纠正网络”倾向将错误概率更高的词Mask掉，并预测出真实词汇。

### 基于NLG的文本纠错技术
上述提到的Mask方法只能用于输入与输出等长的情况，但是实际应用中往往会出现两者不等长的情况，如错字或多字。一种可能的解决办法是，在原有的BERT模型后嵌入一层Transformer Decoder，即将“文本纠错”任务等价于“将错误的文本翻译成正确的文本”，此时我们没法保证输出文本与原始文本中正确的部分一定能保持完全一致，可能在语义不变的情况下，会生成了一种新的表达方式。

### 一个文本纠错工具集：pycorrector
pycorrector是一个文本纠错工具集，内置了KenLM、MacBERT、Transformer等多种文本纠错模型。

+ pycorrector的项目地址：[https://github.com/shibing624/pycorrector](https://github.com/shibing624/pycorrector)
+ 一个基于MacBERT的线上Demo：[https://huggingface.co/spaces/shibing624/pycorrector](https://huggingface.co/spaces/shibing624/pycorrector)

pycorrector不仅可以通过“import pycorrector”调用，也提供了Huggingface的预训练模型调用方式，以下是一个基于Huggingface的MacBERT4CSC调用样例。

```python
from transformers import BertTokenizer, BertForMaskedLM

# 载入模型
tokenizer = BertTokenizer.from_pretrained("shibing624/macbert4csc-base-chinese")
model = BertForMaskedLM.from_pretrained("shibing624/macbert4csc-base-chinese")

text = "大家好,一起来参加DataWhale的《ChatGPT使用指南》组队学习课乘吧！"
input_ids = tokenizer([text], padding=True, return_tensors='pt')

# 生成结果文本
with torch.no_grad():
    outputs = model(**input_ids)
output_ids = torch.argmax(outputs.logits, dim=-1)
output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True).replace(' ', '')

print("原始文本: ", text)
print("纠错文本: ", output_text)
```

<details class="lake-collapse"><summary id="ud90c2766"><span class="ne-text">output：</span></summary><pre data-language="json" id="iOXbm" class="ne-codeblock language-json"><code>原始文本:  大家好,一起来参加DataWhale的《ChatGPT使用指南》组队学习课乘吧！
纠错文本:  大家好,一起来参加datawhale的《chatgpt使用指南》组队学习课程吧！</code></pre></details>


```python
# 查看修改点
import operator
def get_errors(corrected_text, origin_text):
    sub_details = []
    for i, ori_char in enumerate(origin_text):
        if ori_char in [' ', '“', '”', '‘', '’', '琊', '\n', '…', '—', '擤']:
            # add unk word
            corrected_text = corrected_text[:i] + ori_char + corrected_text[i:]
            continue
        if i >= len(corrected_text):
            continue
        if ori_char != corrected_text[i]:
            if ori_char.lower() == corrected_text[i]:
                # pass english upper char
                corrected_text = corrected_text[:i] + ori_char + corrected_text[i + 1:]
                continue
            sub_details.append((ori_char, corrected_text[i], i, i + 1))
    sub_details = sorted(sub_details, key=operator.itemgetter(2))
    return corrected_text, sub_details

correct_text, details = get_errors(output_text[:len(text)], text)
print(details)
```

<details class="lake-collapse"><summary id="u05e530b6"><span class="ne-text">output：</span></summary><pre data-language="json" id="ikwEb" class="ne-codeblock language-json"><code>[('乘', '程', 37, 38)]</code></pre></details>


## 基于OpenAI接口的文本纠错实验
```python
def correct_text(text):
    content = f"请对以下文本进行文本纠错:\n{text}"
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo", 
        messages=[{"role": "user", "content": content}]
    )
    corrected_text = response.get("choices")[0].get("message").get("content")
    return corrected_text

text = "大家好,一起来参加DataWhale的《ChatGPT使用指南》组队学习课乘吧！"
output_text = correct_text(text)
print("原始文本: ", text)
print("纠错文本: ", output_text)
```

<details class="lake-collapse"><summary id="ucc1d1705"><span class="ne-text">output：</span></summary><pre data-language="json" id="TcJDk" class="ne-codeblock language-json"><code>原始文本:  大家好,一起来参加DataWhale的《ChatGPT使用指南》组队学习课乘吧！
纠错文本:  大家好，一起来参加DataWhale的《ChatGPT使用指南》组队学习课程吧！</code></pre></details>


```python
from redlines import Redlines
from IPython.display import display, Markdown

diff = Redlines(' '.join(list(text)),' '.join(list(output_text)))
display(Markdown(diff.output_markdown))
```

<details class="lake-collapse"><summary id="u205c8ed2"><span class="ne-text">output：</span></summary><pre data-language="json" id="rb1J4" class="ne-codeblock language-json"><code>大 家 好 , ， 一 起 来 参 加 D a t a W h a l e 的 《 C h a t G P T 使 用 指 南 》 组 队 学 习 课 乘 程 吧 ！</code></pre></details>




