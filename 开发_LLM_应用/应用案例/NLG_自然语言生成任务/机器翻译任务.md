## 什么是机器翻译？
机器翻译，又称为自动翻译，是利用计算机将一种自然语言（源语言）转换为另一种自然语言（目标语言）的过程。据不完全统计，世界上约有7000种语言，两两配对约有 70002 种组合，这些语言中又不乏一词多义、垂类知识等现象，因此能够使用更少的标注数据，或者无监督地让计算机真正地理解输入语言的含义，并“信”、“达”、“雅”地转化为输出语言，是历来学者们的研究重心。

众所周知，机器翻译一直是自然语言处理领域备受关注的研究方向，也是自然语言处理技术最早展露头角的任务之一。如今市面上的机器翻译工具层出不穷，如大家常用的百度翻译、谷歌翻译，乃至小时候科幻片里才有的AI同声传译，如讯飞听见同传。简单来说可以将其划分为通用领域（多语种）、垂直领域、术语定制化、领域自适应、人工适应、语音翻译等。

## 常见的机器翻译技术
从机器翻译的发展历程来看，主要经历了如下几个阶段：

+ 基于规则的方法
+ 基于统计的方法
+ 基于神经网络的方法

基于规则的方法需要建立各类知识库，描述源语言和目标语言的词法、句法以及语义知识，有时知识无关的世界知识。

基于统计的方法认为对于一条源语言 R，任何一条目标语言 T 都可能是它的译文，只是可能性有高有低。对于源语言中的每个词 ri 及目标语言中的每个词 tj，判断词对齐的概率，再通过期望最大算法（如EM算法）得到最大词对齐概率的对齐方式。这便是基于词的翻译模型。显然，将翻译的最小单位设计成词是不符合语法的，因此后来又延申出了基于短语的翻译方法，将最小翻译单位设计成连续的词串。

2013年，一种用于机器翻译的新型端到端编码器-解码器架构问世，将CNN用于隐含表征挖掘，将RNN用于将隐含向量转化为目标语言，标志了神经机器翻译开端。后来，Attention、Transformer、BERT等技术被相继提出，大大提升了翻译的质量。

以下是一个基于`transformers`实现机器翻译的简单样例。

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("Helsinki-NLP/opus-mt-zh-en")
model = AutoModelForSeq2SeqLM.from_pretrained("Helsinki-NLP/opus-mt-zh-en")

text = "大家好，一起来参加DataWhale的《ChatGPT使用指南》组队学习课程吧！"

inputs = tokenizer(text, return_tensors="pt", )
outputs = model.generate(inputs["input_ids"], max_length=40, num_beams=4, early_stopping=True)
translated_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)
print('原始文本: ', text)
print('翻译文本: ', translated_sentence)
```

<br/>tips
原始文本:  大家好，一起来参加DataWhale的《ChatGPT使用指南》组队学习课程吧！  
翻译文本:  Hey, guys, let's join the ChatGPT team at DataWhale.

<br/>

## 基于OpenAI接口的机器翻译实验
###  简单上手版：短文本英翻中
```python
def translate_text(text):
    content = f"请将以下中文文本翻译成英文:\n{text}"
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo", 
        messages=[{"role": "user", "content": content}]
    )
    translated_text = response.get("choices")[0].get("message").get("content")
    return translated_text

text_to_translate = "大家好，一起来参加DataWhale的《ChatGPT使用指南》组队学习课程吧！"
translated_text = translate_text(text_to_translate)
print("原始文本: ", text_to_translate)
print("输出文本: ", translated_text)
```

<br/>tips
原始文本:  大家好，一起来参加DataWhale的《ChatGPT使用指南》组队学习课程吧！  
输出文本:  Hello everyone, let's join the team learning course of "ChatGPT User Guide" organized by DataWhale together!

<br/>

可以看到，ChatGPT明显比Helsinki-NLP在中翻英上的效果更好，将《ChatGPT使用指南》翻译得更加具体。

### 进阶深度版：长书籍英翻中
导入书籍，数据来源：[datamining/tensorflow-program/nlp/word2vec/dataset at master · LouisScorpio/datamining](https://github.com/LouisScorpio/datamining/tree/master/tensorflow-program/nlp/word2vec/dataset)

```python
with open("dataset/哈利波特1-7英文原版.txt", "r") as f:
    text = f.read()
```

```python
print('全书字符数: ', len(text))
# 整本书的字符数约有635万，但我们知道，chatgpt的api调用是根据token数量来的，
# tokenizer本身的作用是将句子切分成单词，再将单词转化为数值型的输入，我们可以简单地使用tokenizer来统计token数量
```

<br/>tips
全书字符数:  6350735

<br/>

```python
from transformers import GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")  # GPT-2的tokenizer和GPT-3是一样的
token_counts = len(tokenizer.encode(text))
print('全书token数: ', token_counts)

# chatgpt的api调用价格是 1000 token 0.01美元，因此可以大致计算翻译一本书的价格
translate_cost = 0.01 / 1000 * token_counts
print(f'翻译全书约需{translate_cost}美元')
```

```python
Token indices sequence length is longer than the specified maximum sequence length for this model (1673251 > 1024). Running this sequence through the model will result in indexing errors
```

<br/>tips
全书token数:  1673251  
翻译全书约需16.73251美元

<br/>

```python
# 翻译全书约需115.14 rmb成本，有点贵了，我们试着只翻译第一本
end_idx = text.find('2.Harry Potter and The Chamber Of Secrets.txt')
text = text[:end_idx]
print('第一册字符数: ', len(text))

tokenizer = GPT2Tokenizer.from_pretrained("gpt2") 
token_counts = len(tokenizer.encode(text))
print('第一册token数: ', token_counts)

translate_cost = 0.01 / 1000 * token_counts
print(f'翻译第一册约需{translate_cost}美元')
```

<br/>tips
第一册字符数:  442815

<br/>

<br/>tips
Token indices sequence length is longer than the specified maximum sequence length for this model (119873 > 1024). Running this sequence through the model will result in indexing errors

<br/>

<br/>tips
第一册token数:  119873  
翻译第一册约需1.19873美元

<br/>

GPT-3的token限制大约在4096左右（据说GPT-4最多输入3.2万token），因此无法直接将12万token的文本输进去。

我们可以将使用一个简单的方法，将文本分成若干份，每一份使用chatgpt翻译，最终再拼接起来。

首先，我们最好能保证每份文本本身的语义连贯性，如果从一个句子中间将上下文拆成两块，则翻译时容易存在歧义。

一个比较直观的想法是，将每个段落当成一个文本块，每次翻译一段。

但是本书的段落非常多，一段一段翻译显然会降低翻译的效率。同时，由于每段的上下文较少，导致翻译错误的可能性上升。

```python
paragraphs = text.split('\n')
print('段落数: ', len(paragraphs))

ntokens = []
for paragraph in paragraphs:
    ntokens.append(len(tokenizer.encode(paragraph)))
print('最长段落的token数: ', max(ntokens))
```

<br/>tips
段落数:  3038  
最长段落的token数:  275

<br/>

因此，我们选定一个阈值，如500，每次加入一个文本段落，如果总数超过500，则开启一个文本块。

```python
def group_paragraphs(paragraphs, ntokens, max_len=1000):
    """
    合并短段落为文本块，用于丰富上下文语境，提升文本连贯性，并提升运算效率。
    :param paragraphs: 段落集合
    :param ntokens: token数集合
    :param max_len: 最大文本块token数
    :return: 组合好的文本块
    """
    batches = []
    cur_batch = ""
    cur_tokens = 0

    # 对于每个文本段落做处理
    for paragraph, ntoken in zip(paragraphs, ntokens):
        if ntoken + cur_tokens + 1 > max_len:  # '1' 指的是'\n'
            # 如果加入这段文本，总token数超过阈值，则开启新的文本块
            batches.append(cur_batch)
            cur_batch = paragraph
            cur_tokens = ntoken
        else:
            # 否则将段落插入文本块中
            cur_batch += "\n" + paragraph
            cur_tokens += (1 + ntoken)
    batches.append(cur_batch)  # 记录最后一个文本块
    return batches

batchs = group_paragraphs(paragraphs, ntokens, max_len=500)
print('文本块数: ', len(batchs))

new_tokens = []
for batch in batchs:
    new_tokens.append(len(tokenizer.encode(batch)))
print('最长文本块的token数: ', max(new_tokens))
```

<br/>tips
文本块数:  256  
最长文本块的token数:  500

<br/>

```python
# 展示第一段文本
print(batchs[0])
```

<br/>tips
1.Harry Potter and the Sorcerer's Stone.txt  
  
　　Harry Potter and the Sorcerer's Stone  
　　CHAPTER ONE  
　　THE BOY WHO LIVED  
　　Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense.  
　　Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere.  
　　The Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. They didn't think they could bear it if anyone found out about the Potters. Mrs. Potter was Mrs. Dursley's sister, but they hadn't met for several years; in fact, Mrs. Dursley pretended she didn't have a sister, because her sister and her good-for-nothing husband were as unDursleyish as it was possible to be. The Dursleys shuddered to think what the neighbors would say if the Potters arrived in the street. The Dursleys knew that the Potters had a small son, too, but they had never even seen him. This boy was another good reason for keeping the Potters away; they didn't want Dudley mixing with a child like that.  
　　When Mr. and Mrs. Dursley woke up on the dull, gray Tuesday our story starts, there was nothing about the cloudy sky outside to suggest that strange and mysterious things would soon be happening all over the country. Mr. Dursley hummed as he picked out his most boring tie for work, and Mrs. Dursley gossiped away happily as she wrestled a screaming Dudley into his high chair.

<br/>

```python
# 实操中发现，使用chatgpt翻译长文本很慢，这里改用Davinci实现，感兴趣的同学可以尝试优化
# 速率限制见：https://platform.openai.com/docs/guides/rate-limits/overview
# def translate_text(text):
#     content = f"请将以下英文文本翻译成中文:\n{text}"
#     response = openai.ChatCompletion.create(
#         model="gpt-3.5-turbo", 
#         messages=[{"role": "user", "content": content}]
#     )
#     translated_text = response.get("choices")[0].get("message").get("content")
#     return translated_text
def translate_text(text):
    response = openai.Completion.create(
        engine="text-davinci-003",
        prompt=f"请将以下英文翻译成中文:\n{text}",
        max_tokens=2048
    )

    translate_text = response.choices[0].text.strip()
    return translate_text
print(translate_text(batchs[0]))
```

<br/>tips
欣欣夫妇对4号普里维特路的房子非常自豪，他们乐呵呵地表示，自己完全是一家正常家庭，也就不沾任何奇怪或神秘的事情。要是提到像这样的废话，他们实在是有点不屑一顾。欣欣先生当时正在格林宁公司任出品部主任，他长得又胖又壮，脖子很粗，只有一撮大胡子。欣欣太太瘦得非常，并且，出乎意料的是，她的脖子竟然站地比正常人都长，这样就非常省事，因为欣欣太太经常会在花园栅栏上俯瞰，窥探邻居的一举一动。欣欣夫妇有一个小儿子，叫达力，他们认为，没有比这孩子更好的了。  
  
欣欣夫妇有很多东西，但其实他们也拥有一个秘密，最怕的是被有人发现。他们实在不敢想象，如果波特家来他们街上，会有什么样的下场……波特太太就是欣欣太太的妹妹，但是他们已经好几年没有见过面了。实际上，欣欣太太压根就假装没有妹妹，因为妹妹和他那没用的丈夫，实在是与欣欣家的一切都格格不入。同时，欣欣夫妇还知道波特家有一个小儿子，但是他们从来都没有见过。这孩子可就是欣欣夫妇珍爱达力的另一个绝好理由，不要让达力和他有接触。  
  
当本故事发生的比较乏味的周二早晨，欣欣夫妇醒来，天空阴沉沉的，毫不暗示即将发生的奇异神秘的事情。欣欣先生嗯嗯哼哼地挑出今天上班穿的最没气质的领带，欣欣太太讨论着什么有趣的闲事，忙着把尖叫的达力抱进高脚高位椅里。

<br/>

接下来，我们对每个文本块做翻译，并将结果合并起来。

```python
from tqdm import tqdm
translated_batchs = []
```

```python
# 有的时候由于VPN等问题，可能会出现断联，也即443 timeout，可以在断点batch处重连
translated_batchs_bak = translated_batchs.copy()
cur_len = len(translated_batchs)
for i in tqdm(range(cur_len, len(batchs))):
    translated_batchs.append(translate_text(batchs[i]))
```

<br/>tips
100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [07:50<00:00, 58.79s/it]

<br/>

```python
# 另一种方法是，参考openai的util函数，加入retry机制，如果失败则尝试重连
from tenacity import retry, stop_after_attempt, wait_random_exponential
@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))
def translate_text(text):
    response = openai.Completion.create(
        engine="text-davinci-003",
        prompt=f"请将以下英文翻译成中文:\n{text}",
        temperature=0.3,
        max_tokens=2048
    )

    translate_text = response.choices[0].text.strip()
    return translate_text
```

```python
for i in tqdm(range(len(batchs))):
    translated_batchs.append(translate_text(batchs[i]))
```

<br/>tips
100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [25:31<00:00, 76.55s/it]

<br/>

```python
# 保存结果至txt文件
result = '\n'.join(translated_batchs)

with open('dataset/哈利波特1中文版翻译.txt','w', encoding='utf-8') as f:
    f.write(result)
```



