## 什么是文本摘要？
文本摘要任务指的是用精炼的文本来概括整篇文章的大意，使得用户能够通过阅读摘要来大致了解文章的主要内容。

## 常见的文本摘要技术
<br/>color2
从实现手法来说，文本摘要任务主要分为以下三种：

+ **抽取式摘要**：从原文档中提取现成的句子作为摘要句。
+ **压缩式摘要**：对原文档的冗余信息进行过滤，压缩文本作为摘要。
+ **生成式摘要**：基于`NLG`技术，根据源文档内容，由算法模型自己生成自然语言描述。

<br/>

以下是一个基于mT5模型（T5模型的多语言版）的文本摘要样例。**注：**下载模型较大，可前往`huggingface->Hosted inference API `在线测试：

[https://huggingface.co/csebuetnlp/mT5_multilingual_XLSum](https://huggingface.co/csebuetnlp/mT5_multilingual_XLSum)

```python
import re
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# 载入模型 
tokenizer = AutoTokenizer.from_pretrained("csebuetnlp/mT5_multilingual_XLSum")
model = AutoModelForSeq2SeqLM.from_pretrained("csebuetnlp/mT5_multilingual_XLSum")

WHITESPACE_HANDLER = lambda k: re.sub('\s+', ' ', re.sub('\n+', ' ', k.strip()))

text = """自动信任协商主要解决跨安全域的信任建立问题,使陌生实体通过反复的、双向的访问控制策略和数字证书的相互披露而逐步建立信任关系。由于信任建立的方式独特和应用环境复杂,自动信任协商面临多方面的安全威胁,针对协商的攻击大多超出常规防范措施所保护的范围,因此有必要对自动信任协商中的攻击手段进行专门分析。按攻击特点对自动信任协商中存在的各种攻击方式进行分类,并介绍了相应的防御措施,总结了当前研究工作的不足,对未来的研究进行了展望"""
text = WHITESPACE_HANDLER(text)
input_ids = tokenizer([text], return_tensors="pt", padding="max_length", truncation=True, max_length=512)["input_ids"]

# 生成结果文本
output_ids = model.generate(input_ids=input_ids, max_length=84, no_repeat_ngram_size=2, num_beams=4)[0]
output_text = tokenizer.decode(output_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)

print("原始文本: ", text)
print("摘要文本: ", output_text)
```

<details class="lake-collapse"><summary id="ud90c2766"><span class="ne-text">output：</span></summary><pre data-language="json" id="iOXbm" class="ne-codeblock language-json"><code>C:\Softwares\Programming\Python\Anaconda\envs\chatgpt\lib\site-packages\tqdm\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
from .autonotebook import tqdm as notebook_tqdm
C:\Softwares\Programming\Python\Anaconda\envs\chatgpt\lib\site-packages\transformers\convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
warnings.warn(</code></pre></details>
<details class="lake-collapse"><summary id="u1cc48467"><span class="ne-text">output：</span></summary><pre data-language="json" id="OyLCR" class="ne-codeblock language-json"><code>原始文本:  自动信任协商主要解决跨安全域的信任建立问题,使陌生实体通过反复的、双向的访问控制策略和数字证书的相互披露而逐步建立信任关系。由于信任建立的方式独特和应用环境复杂,自动信任协商面临多方面的安全威胁,针对协商的攻击大多超出常规防范措施所保护的范围,因此有必要对自动信任协商中的攻击手段进行专门分析。按攻击特点对自动信任协商中存在的各种攻击方式进行分类,并介绍了相应的防御措施,总结了当前研究工作的不足,对未来的研究进行了展望
摘要文本:  自动信任协商(AI)是互信关系建立的最新研究工作的一部分。</code></pre></details>
## 基于OpenAI接口的文本摘要实验
### 调用预训练模型
#### GPT 3.5
```python
def summarize_text(text):
    response = openai.Completion.create(
        engine="text-davinci-003",
        prompt=f"请对以下文本进行总结，注意总结的凝炼性，将总结字数控制在20个字以内:\n{text}",
        temperature=0.3,
        max_tokens=500,
    )

    summarized_text = response.choices[0].text.strip()
    return summarized_text

text = "自动信任协商主要解决跨安全域的信任建立问题,使陌生实体通过反复的、双向的访问控制策略和数字证书的相互披露而逐步建立信任关系。由于信任建立的方式独特和应用环境复杂,自动信任协商面临多方面的安全威胁,针对协商的攻击大多超出常规防范措施所保护的范围,因此有必要对自动信任协商中的攻击手段进行专门分析。按攻击特点对自动信任协商中存在的各种攻击方式进行分类,并介绍了相应的防御措施,总结了当前研究工作的不足,对未来的研究进行了展望。"""
output_text = summarize_text(text)
print("原始文本: ", text)
print("摘要文本: ", output_text)
print("摘要文本长度: ", len(output_text))
```

<details class="lake-collapse"><summary id="u6bfa6e8a"><span class="ne-text">output：</span></summary><pre data-language="json" id="oadGt" class="ne-codeblock language-json"><code>原始文本:  自动信任协商主要解决跨安全域的信任建立问题,使陌生实体通过反复的、双向的访问控制策略和数字证书的相互披露而逐步建立信任关系。由于信任建立的方式独特和应用环境复杂,自动信任协商面临多方面的安全威胁,针对协商的攻击大多超出常规防范措施所保护的范围,因此有必要对自动信任协商中的攻击手段进行专门分析。按攻击特点对自动信任协商中存在的各种攻击方式进行分类,并介绍了相应的防御措施,总结了当前研究工作的不足,对未来的研究进行了展望。
摘要文本:  自动信任协商解决跨安全域信任建立问题，但面临多种安全威胁，需要分析攻击方式及防御措施。
摘要文本长度:  43</code></pre></details>


#### ChatGPT
```python
def summarize_text(text):
    content = f"请对以下文本进行总结，注意总结的凝炼性，将总结字数控制在20个字以内:\n{text}"
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo", 
        messages=[{"role": "user", "content": content}],
        temperature=0.3
    )
    summarized_text = response.get("choices")[0].get("message").get("content")
    return summarized_text

text = """自动信任协商主要解决跨安全域的信任建立问题,使陌生实体通过反复的、双向的访问控制策略和数字证书的相互披露而逐步建立信任关系。由于信任建立的方式独特和应用环境复杂,自动信任协商面临多方面的安全威胁,针对协商的攻击大多超出常规防范措施所保护的范围,因此有必要对自动信任协商中的攻击手段进行专门分析。按攻击特点对自动信任协商中存在的各种攻击方式进行分类,并介绍了相应的防御措施,总结了当前研究工作的不足,对未来的研究进行了展望。"""
output_text = summarize_text(text)

print("原始文本: ", text)
print("摘要文本: ", output_text)
print("摘要文本长度: ", len(output_text))
# 注意，chatgpt并不能完美限制摘要输出的字数
```

<details class="lake-collapse"><summary id="ubf3172c3"><span class="ne-text">output：</span></summary><pre data-language="json" id="D5mQC" class="ne-codeblock language-json"><code>原始文本:  自动信任协商主要解决跨安全域的信任建立问题,使陌生实体通过反复的、双向的访问控制策略和数字证书的相互披露而逐步建立信任关系。由于信任建立的方式独特和应用环境复杂,自动信任协商面临多方面的安全威胁,针对协商的攻击大多超出常规防范措施所保护的范围,因此有必要对自动信任协商中的攻击手段进行专门分析。按攻击特点对自动信任协商中存在的各种攻击方式进行分类,并介绍了相应的防御措施,总结了当前研究工作的不足,对未来的研究进行了展望。
摘要文本:  自动信任协商解决跨域信任建立，但面临多方面安全威胁，需分类防御。研究不足，未来展望。
摘要文本长度:  42</code></pre></details>


### 基于自定义语料`fine tune`
对于垂直领域的数据或任务，有时直接使用LLM的效果不佳。

当然，由于ChatGPT强大的内在理解能力，在某些情况下使用一个比较好的Prompt，通过`Zero-Shot`或者`Few-Shot`也能得到一个不错的结果。

我们这里简单地介绍如何通过自定义语料库对模型进行`fine tune`。目前OpenAI仅开放了`ada, babbage, curie, davinci`四个小模型的`fine tune`接口，其中最大的`davinci`模型约175亿参数。

[https://platform.openai.com/docs/guides/fine-tuning](https://platform.openai.com/docs/guides/fine-tuning)

```python
# 查看所有的fine tune模型
!openai api fine_tunes.list
```

<details class="lake-collapse"><summary id="uaca238cd"><span class="ne-text" style="color: rgba(0, 0, 0, 0.87)">output：</span></summary><pre data-language="json" id="f0Dtb" class="ne-codeblock language-json"><code>{
  &quot;data&quot;: [
    {
      &quot;created_at&quot;: 1681558899,
      &quot;fine_tuned_model&quot;: null,
      &quot;hyperparams&quot;: {
        &quot;batch_size&quot;: null,
        &quot;learning_rate_multiplier&quot;: null,
        &quot;n_epochs&quot;: 4,
        &quot;prompt_loss_weight&quot;: 0.01
      },
      &quot;id&quot;: &quot;ft-dsshfnyndpY14OqgnRya8ExI&quot;,
      &quot;model&quot;: &quot;davinci&quot;,
      &quot;object&quot;: &quot;fine-tune&quot;,
      &quot;organization_id&quot;: &quot;org-U35hu1wdD7w3HnkgJ5fdBW8m&quot;,
      &quot;result_files&quot;: [],
      &quot;status&quot;: &quot;failed&quot;,
      &quot;training_files&quot;: [
        {
          &quot;bytes&quot;: 380384,
          &quot;created_at&quot;: 1681558899,
          &quot;filename&quot;: &quot;./dataset/csl_summarize_finetune_prepared.jsonl&quot;,
          &quot;id&quot;: &quot;file-0akQ6d59yrShNHtrCrh93U4w&quot;,
          &quot;object&quot;: &quot;file&quot;,
          &quot;purpose&quot;: &quot;fine-tune&quot;,
          &quot;status&quot;: &quot;processed&quot;,
          &quot;status_details&quot;: null
        }
      ],
      &quot;updated_at&quot;: 1681558911,
      &quot;validation_files&quot;: []
    },
    {
      &quot;created_at&quot;: 1681559488,
      &quot;fine_tuned_model&quot;: &quot;ada:ft-personal-2023-04-15-11-57-25&quot;,
      &quot;hyperparams&quot;: {
        &quot;batch_size&quot;: 1,
        &quot;learning_rate_multiplier&quot;: 0.1,
        &quot;n_epochs&quot;: 4,
        &quot;prompt_loss_weight&quot;: 0.01
      },
      &quot;id&quot;: &quot;ft-lcIkh8dG2t1V4GAWWsFC644V&quot;,
      &quot;model&quot;: &quot;ada&quot;,
      &quot;object&quot;: &quot;fine-tune&quot;,
      &quot;organization_id&quot;: &quot;org-U35hu1wdD7w3HnkgJ5fdBW8m&quot;,
      &quot;result_files&quot;: [
        {
          &quot;bytes&quot;: 114680,
          &quot;created_at&quot;: 1681559846,
          &quot;filename&quot;: &quot;compiled_results.csv&quot;,
          &quot;id&quot;: &quot;file-CwFfIH8HHXqpk3YCg0x1Yphn&quot;,
          &quot;object&quot;: &quot;file&quot;,
          &quot;purpose&quot;: &quot;fine-tune-results&quot;,
          &quot;status&quot;: &quot;processed&quot;,
          &quot;status_details&quot;: null
        }
      ],
      &quot;status&quot;: &quot;succeeded&quot;,
      &quot;training_files&quot;: [
        {
          &quot;bytes&quot;: 380384,
          &quot;created_at&quot;: 1681559487,
          &quot;filename&quot;: &quot;./dataset/csl_summarize_finetune_prepared.jsonl&quot;,
          &quot;id&quot;: &quot;file-0d35aGDx0Mn33tZ6x070HzmV&quot;,
          &quot;object&quot;: &quot;file&quot;,
          &quot;purpose&quot;: &quot;fine-tune&quot;,
          &quot;status&quot;: &quot;processed&quot;,
          &quot;status_details&quot;: null
        }
      ],
      &quot;updated_at&quot;: 1681559846,
      &quot;validation_files&quot;: []
    },
    {
      &quot;created_at&quot;: 1681562888,
      &quot;fine_tuned_model&quot;: &quot;ada:ft-personal-2023-04-15-12-54-03&quot;,
      &quot;hyperparams&quot;: {
        &quot;batch_size&quot;: 1,
        &quot;learning_rate_multiplier&quot;: 0.1,
        &quot;n_epochs&quot;: 4,
        &quot;prompt_loss_weight&quot;: 0.01
      },
      &quot;id&quot;: &quot;ft-cSvqpGrrohBdPPmE7oxR2Xy3&quot;,
      &quot;model&quot;: &quot;ada&quot;,
      &quot;object&quot;: &quot;fine-tune&quot;,
      &quot;organization_id&quot;: &quot;org-U35hu1wdD7w3HnkgJ5fdBW8m&quot;,
      &quot;result_files&quot;: [
        {
          &quot;bytes&quot;: 114651,
          &quot;created_at&quot;: 1681563244,
          &quot;filename&quot;: &quot;compiled_results.csv&quot;,
          &quot;id&quot;: &quot;file-Zfbqegb2TiJztX9R30ikbiG1&quot;,
          &quot;object&quot;: &quot;file&quot;,
          &quot;purpose&quot;: &quot;fine-tune-results&quot;,
          &quot;status&quot;: &quot;processed&quot;,
          &quot;status_details&quot;: null
        }
      ],
      &quot;status&quot;: &quot;succeeded&quot;,
      &quot;training_files&quot;: [
        {
          &quot;bytes&quot;: 380384,
          &quot;created_at&quot;: 1681562888,
          &quot;filename&quot;: &quot;./dataset/csl_summarize_finetune_prepared.jsonl&quot;,
          &quot;id&quot;: &quot;file-Cet8LADkX8SiGnUtAcktsoZ7&quot;,
          &quot;object&quot;: &quot;file&quot;,
          &quot;purpose&quot;: &quot;fine-tune&quot;,
          &quot;status&quot;: &quot;processed&quot;,
          &quot;status_details&quot;: null
        }
      ],
      &quot;updated_at&quot;: 1681563245,
      &quot;validation_files&quot;: []
    },
    {
      &quot;created_at&quot;: 1681564395,
      &quot;fine_tuned_model&quot;: &quot;ada:ft-personal-2023-04-15-13-19-25&quot;,
      &quot;hyperparams&quot;: {
        &quot;batch_size&quot;: 1,
        &quot;learning_rate_multiplier&quot;: 0.1,
        &quot;n_epochs&quot;: 4,
        &quot;prompt_loss_weight&quot;: 0.01
      },
      &quot;id&quot;: &quot;ft-UzytubaVgNI8SAwqLuZba4T9&quot;,
      &quot;model&quot;: &quot;ada&quot;,
      &quot;object&quot;: &quot;fine-tune&quot;,
      &quot;organization_id&quot;: &quot;org-U35hu1wdD7w3HnkgJ5fdBW8m&quot;,
      &quot;result_files&quot;: [
        {
          &quot;bytes&quot;: 114523,
          &quot;created_at&quot;: 1681564766,
          &quot;filename&quot;: &quot;compiled_results.csv&quot;,
          &quot;id&quot;: &quot;file-kq3Dhk5R95SI6taHP0ERWqkc&quot;,
          &quot;object&quot;: &quot;file&quot;,
          &quot;purpose&quot;: &quot;fine-tune-results&quot;,
          &quot;status&quot;: &quot;processed&quot;,
          &quot;status_details&quot;: null
        }
      ],
      &quot;status&quot;: &quot;succeeded&quot;,
      &quot;training_files&quot;: [
        {
          &quot;bytes&quot;: 380384,
          &quot;created_at&quot;: 1681564395,
          &quot;filename&quot;: &quot;./dataset/csl_summarize_finetune_prepared.jsonl&quot;,
          &quot;id&quot;: &quot;file-ajKeL2f93LaU09WRNj4kF6uV&quot;,
          &quot;object&quot;: &quot;file&quot;,
          &quot;purpose&quot;: &quot;fine-tune&quot;,
          &quot;status&quot;: &quot;processed&quot;,
          &quot;status_details&quot;: null
        }
      ],
      &quot;updated_at&quot;: 1681564767,
      &quot;validation_files&quot;: []
    },
    {
      &quot;created_at&quot;: 1681565036,
      &quot;fine_tuned_model&quot;: &quot;ada:ft-personal-2023-04-15-13-29-50&quot;,
      &quot;hyperparams&quot;: {
        &quot;batch_size&quot;: 1,
        &quot;learning_rate_multiplier&quot;: 0.1,
        &quot;n_epochs&quot;: 4,
        &quot;prompt_loss_weight&quot;: 0.01
      },
      &quot;id&quot;: &quot;ft-LoKi6mOxlkOtfZcZTrmivKDa&quot;,
      &quot;model&quot;: &quot;ada&quot;,
      &quot;object&quot;: &quot;fine-tune&quot;,
      &quot;organization_id&quot;: &quot;org-U35hu1wdD7w3HnkgJ5fdBW8m&quot;,
      &quot;result_files&quot;: [
        {
          &quot;bytes&quot;: 112280,
          &quot;created_at&quot;: 1681565392,
          &quot;filename&quot;: &quot;compiled_results.csv&quot;,
          &quot;id&quot;: &quot;file-TTRfuuyBXuZ4BKwX9I4i2zA4&quot;,
          &quot;object&quot;: &quot;file&quot;,
          &quot;purpose&quot;: &quot;fine-tune-results&quot;,
          &quot;status&quot;: &quot;processed&quot;,
          &quot;status_details&quot;: null
        }
      ],
      &quot;status&quot;: &quot;succeeded&quot;,
      &quot;training_files&quot;: [
        {
          &quot;bytes&quot;: 380384,
          &quot;created_at&quot;: 1681565036,
          &quot;filename&quot;: &quot;./dataset/csl_summarize_finetune_prepared.jsonl&quot;,
          &quot;id&quot;: &quot;file-S3SIEZoJbqPXTGT16YxPThVO&quot;,
          &quot;object&quot;: &quot;file&quot;,
          &quot;purpose&quot;: &quot;fine-tune&quot;,
          &quot;status&quot;: &quot;processed&quot;,
          &quot;status_details&quot;: null
        }
      ],
      &quot;updated_at&quot;: 1681565392,
      &quot;validation_files&quot;: []
    }
  ],
  &quot;object&quot;: &quot;list&quot;
}</code></pre></details>


数据集来源：CSL摘要数据集，是计算机领域的论文摘要和标题数据，包含3500条数据，

+ 标题：平均字数 18，字数标准差 4，最大字数41，最小数字 6；
+ 正文：平均字数 200，字数标准差 63，最大字数 631，最小数字 41；

数据源地址：[https://github.com/liucongg/GPT2-NewsTitle](https://github.com/liucongg/GPT2-NewsTitle) 项目中的CSL摘要数据集

```python
import json
with open('dataset/csl_data.json', 'r', encoding='utf-8') as f:
    data = json.load(f)
```

```python
data[-1]
```

<details class="lake-collapse"><summary id="u4c01d25e"><span class="ne-text">output：</span></summary><pre data-language="json" id="NF9VD" class="ne-codeblock language-json"><code>{'title': '自动信任协商中的攻击与防范',
 'content': '自动信任协商主要解决跨安全域的信任建立问题,使陌生实体通过反复的、双向的访问控制策略和数字证书的相互披露而逐步建立信任关系。由于信任建立的方式独特和应用环境复杂,自动信任协商面临多方面的安全威胁,针对协商的攻击大多超出常规防范措施所保护的范围,因此有必要对自动信任协商中的攻击手段进行专门分析。按攻击特点对自动信任协商中存在的各种攻击方式进行分类,并介绍了相应的防御措施,总结了当前研究工作的不足,对未来的研究进行了展望。'}</code></pre></details>


```python
import pandas as pd
df = pd.DataFrame(data)
df = df[['content', 'title']]
df.columns = ["prompt", "completion"]
df_train = df.iloc[:500]
df_train.head(5)
```

![](https://cdn.nlark.com/yuque/0/2024/png/2639475/1735639722544-da5c25f0-f3dd-4375-a4db-14aa95d66482.png)

```python
df_train.to_json("dataset/csl_summarize_finetune.jsonl", orient='records', lines=True, force_ascii=False)
```

```python
!openai tools fine_tunes.prepare_data -f dataset/csl_summarize_finetune.jsonl -q
```

<details class="lake-collapse"><summary id="u39cb08a2"><span class="ne-text">output：</span></summary><pre data-language="json" id="crrpC" class="ne-codeblock language-json"><code>Analyzing...

- Your file contains 500 prompt-completion pairs
- More than a third of your `prompt` column/key is uppercase. Uppercase prompts tends to perform worse than a mixture of case encountered in normal language. We recommend to lower case the data if that makes sense in your domain. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details
- More than a third of your `completion` column/key is uppercase. Uppercase completions tends to perform worse than a mixture of case encountered in normal language. We recommend to lower case the data if that makes sense in your domain. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details
- Your data does not contain a common separator at the end of your prompts. Having a separator string appended to the end of the prompt makes it clearer to the fine-tuned model where the completion should begin. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples. If you intend to do open-ended generation, then you should leave the prompts empty
- Your data does not contain a common ending at the end of your completions. Having a common ending string appended to the end of the completion makes it clearer to the fine-tuned model where the completion should end. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples.
- The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details

Based on the analysis we will perform the following actions:
- [Recommended] Lowercase all your data in column/key `prompt` [Y/n]: Y
- [Recommended] Lowercase all your data in column/key `completion` [Y/n]: Y
- [Recommended] Add a suffix separator ` -&gt;` to all prompts [Y/n]: Y
- [Recommended] Add a suffix ending `\n` to all completions [Y/n]: Y
- [Recommended] Add a whitespace character to the beginning of the completion [Y/n]: Y


Your data will be written to a new JSONL file. Proceed [Y/n]: Y

Wrote modified file to `dataset/csl_summarize_finetune_prepared (1).jsonl`
Feel free to take a look!

Now use that file when fine-tuning:
&gt; openai api fine_tunes.create -t &quot;dataset/csl_summarize_finetune_prepared (1).jsonl&quot;

After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string ` -&gt;` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[&quot;\n&quot;]` so that the generated texts ends at the expected place.
Once your model starts training, it'll approximately take 9.31 minutes to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.</code></pre></details>


```python
import os
os.environ.setdefault("OPENAI_API_KEY", OPENAI_API_KEY)
```

```python
!openai api fine_tunes.create \
    -t "./dataset/csl_summarize_finetune_prepared.jsonl" \
    -m ada\
    --no_check_if_files_exist
```

<details class="lake-collapse"><summary id="u42828408"><span class="ne-text" style="color: rgba(0, 0, 0, 0.87)">output：</span></summary><pre data-language="json" id="ckVo7" class="ne-codeblock language-json"><code>Uploaded file from ./dataset/csl_summarize_finetune_prepared.jsonl: file-gPzuOBUizUDCGO7t0oDYoWQB
Upload progress:   0%|          | 0.00/380k [00:00&lt;?, ?it/s]
Upload progress: 100%|██████████| 380k/380k [00:00&lt;00:00, 239Mit/s]

Created fine-tune: ft-px9hve11l6YjizCQ8I6MyLCK
Streaming events until fine-tuning is complete...

(Ctrl-C will interrupt the stream, but not cancel the fine-tune)
[2023-05-07 20:27:26] Created fine-tune: ft-px9hve11l6YjizCQ8I6MyLCK
[2023-05-07 20:27:45] Fine-tune costs $0.43
[2023-05-07 20:27:45] Fine-tune enqueued. Queue number: 0
[2023-05-07 20:27:46] Fine-tune started

Stream interrupted (client disconnected).
To resume the stream, run:

  openai api fine_tunes.follow -i ft-px9hve11l6YjizCQ8I6MyLCK</code></pre></details>


```python
# 根据上一步的输出，得到fine tune运行的key ft-LoKi6mOxlkOtfZcZTrmivKDa，
# 我们可以通过get来获取当前执行进度，
# 如发现与openai的连接断开，可通过follow重新排队连接
# !openai api fine_tunes.follow -i ft-LoKi6mOxlkOtfZcZTrmivKDa
!openai api fine_tunes.get -i ft-LoKi6mOxlkOtfZcZTrmivKDa
```

<details class="lake-collapse"><summary id="uef617dfa"><span class="ne-text" style="color: rgba(0, 0, 0, 0.87)">output：</span></summary><pre data-language="json" id="fLZ67" class="ne-codeblock language-json"><code>{
  &quot;created_at&quot;: 1681565036,
  &quot;events&quot;: [
    {
      &quot;created_at&quot;: 1681565036,
      &quot;level&quot;: &quot;info&quot;,
      &quot;message&quot;: &quot;Created fine-tune: ft-LoKi6mOxlkOtfZcZTrmivKDa&quot;,
      &quot;object&quot;: &quot;fine-tune-event&quot;
    },
    {
      &quot;created_at&quot;: 1681565045,
      &quot;level&quot;: &quot;info&quot;,
      &quot;message&quot;: &quot;Fine-tune costs $0.43&quot;,
      &quot;object&quot;: &quot;fine-tune-event&quot;
    },
    {
      &quot;created_at&quot;: 1681565045,
      &quot;level&quot;: &quot;info&quot;,
      &quot;message&quot;: &quot;Fine-tune enqueued. Queue number: 0&quot;,
      &quot;object&quot;: &quot;fine-tune-event&quot;
    },
    {
      &quot;created_at&quot;: 1681565046,
      &quot;level&quot;: &quot;info&quot;,
      &quot;message&quot;: &quot;Fine-tune started&quot;,
      &quot;object&quot;: &quot;fine-tune-event&quot;
    },
    {
      &quot;created_at&quot;: 1681565139,
      &quot;level&quot;: &quot;info&quot;,
      &quot;message&quot;: &quot;Completed epoch 1/4&quot;,
      &quot;object&quot;: &quot;fine-tune-event&quot;
    },
    {
      &quot;created_at&quot;: 1681565216,
      &quot;level&quot;: &quot;info&quot;,
      &quot;message&quot;: &quot;Completed epoch 2/4&quot;,
      &quot;object&quot;: &quot;fine-tune-event&quot;
    },
    {
      &quot;created_at&quot;: 1681565293,
      &quot;level&quot;: &quot;info&quot;,
      &quot;message&quot;: &quot;Completed epoch 3/4&quot;,
      &quot;object&quot;: &quot;fine-tune-event&quot;
    },
    {
      &quot;created_at&quot;: 1681565369,
      &quot;level&quot;: &quot;info&quot;,
      &quot;message&quot;: &quot;Completed epoch 4/4&quot;,
      &quot;object&quot;: &quot;fine-tune-event&quot;
    },
    {
      &quot;created_at&quot;: 1681565391,
      &quot;level&quot;: &quot;info&quot;,
      &quot;message&quot;: &quot;Uploaded model: ada:ft-personal-2023-04-15-13-29-50&quot;,
      &quot;object&quot;: &quot;fine-tune-event&quot;
    },
    {
      &quot;created_at&quot;: 1681565392,
      &quot;level&quot;: &quot;info&quot;,
      &quot;message&quot;: &quot;Uploaded result file: file-TTRfuuyBXuZ4BKwX9I4i2zA4&quot;,
      &quot;object&quot;: &quot;fine-tune-event&quot;
    },
    {
      &quot;created_at&quot;: 1681565392,
      &quot;level&quot;: &quot;info&quot;,
      &quot;message&quot;: &quot;Fine-tune succeeded&quot;,
      &quot;object&quot;: &quot;fine-tune-event&quot;
    }
  ],
  &quot;fine_tuned_model&quot;: &quot;ada:ft-personal-2023-04-15-13-29-50&quot;,
  &quot;hyperparams&quot;: {
    &quot;batch_size&quot;: 1,
    &quot;learning_rate_multiplier&quot;: 0.1,
    &quot;n_epochs&quot;: 4,
    &quot;prompt_loss_weight&quot;: 0.01
  },
  &quot;id&quot;: &quot;ft-LoKi6mOxlkOtfZcZTrmivKDa&quot;,
  &quot;model&quot;: &quot;ada&quot;,
  &quot;object&quot;: &quot;fine-tune&quot;,
  &quot;organization_id&quot;: &quot;org-U35hu1wdD7w3HnkgJ5fdBW8m&quot;,
  &quot;result_files&quot;: [
    {
      &quot;bytes&quot;: 112280,
      &quot;created_at&quot;: 1681565392,
      &quot;filename&quot;: &quot;compiled_results.csv&quot;,
      &quot;id&quot;: &quot;file-TTRfuuyBXuZ4BKwX9I4i2zA4&quot;,
      &quot;object&quot;: &quot;file&quot;,
      &quot;purpose&quot;: &quot;fine-tune-results&quot;,
      &quot;status&quot;: &quot;processed&quot;,
      &quot;status_details&quot;: null
    }
  ],
  &quot;status&quot;: &quot;succeeded&quot;,
  &quot;training_files&quot;: [
    {
      &quot;bytes&quot;: 380384,
      &quot;created_at&quot;: 1681565036,
      &quot;filename&quot;: &quot;./dataset/csl_summarize_finetune_prepared.jsonl&quot;,
      &quot;id&quot;: &quot;file-S3SIEZoJbqPXTGT16YxPThVO&quot;,
      &quot;object&quot;: &quot;file&quot;,
      &quot;purpose&quot;: &quot;fine-tune&quot;,
      &quot;status&quot;: &quot;processed&quot;,
      &quot;status_details&quot;: null
    }
  ],
  &quot;updated_at&quot;: 1681565392,
  &quot;validation_files&quot;: []
}</code></pre></details>


```python
# 保存openai fine tune过程的记录
!openai api fine_tunes.results -i ft-cSvqpGrrohBdPPmE7oxR2Xy3 > dataset/metric.csv
```

```python
def summarize_text(text, model_name):
    response = openai.Completion.create(
        engine=model_name,
        prompt=f"请对以下文本进行总结，注意总结的凝炼性，将总结字数控制在20个字以内:\n{text}",
        temperature=0.7,
        max_tokens=100,
    )

    summarized_text = response.choices[0].text.strip()
    return summarized_text

text = "自动信任协商主要解决跨安全域的信任建立问题,使陌生实体通过反复的、双向的访问控制策略和数字证书的相互披露而逐步建立信任关系。由于信任建立的方式独特和应用环境复杂,自动信任协商面临多方面的安全威胁,针对协商的攻击大多超出常规防范措施所保护的范围,因此有必要对自动信任协商中的攻击手段进行专门分析。按攻击特点对自动信任协商中存在的各种攻击方式进行分类,并介绍了相应的防御措施,总结了当前研究工作的不足,对未来的研究进行了展望。"""
print("原始文本: ", text)
print("ada摘要文本: ", summarize_text(text, model_name='ada'))
print("ada fine-tune摘要文本: ", summarize_text(text, model_name='ada:ft-personal-2023-04-15-13-29-50'))
```

<details class="lake-collapse"><summary id="u68b01490"><span class="ne-text" style="color: rgba(0, 0, 0, 0.87)">output：</span></summary><pre data-language="json" id="doopC" class="ne-codeblock language-json"><code>原始文本:  自动信任协商主要解决跨安全域的信任建立问题,使陌生实体通过反复的、双向的访问控制策略和数字证书的相互披露而逐步建立信任关系。由于信任建立的方式独特和应用环境复杂,自动信任协商面临多方面的安全威胁,针对协商的攻击大多超出常规防范措施所保护的范围,因此有必要对自动信任协商中的攻击手段进行专门分析。按攻击特点对自动信任协商中存在的各种攻击方式进行分类,并介绍了相应的防御措施,总结了当前研究工作的不足,对未来的研究进行了展望。
ada摘要文本:  因此,为了在未来进行研究,本次研究也许能给学术界其他学者带来建议,更多读者本次研究期间的能查
ada fine-tune摘要文本:  -&gt; 分布式防御措施的自动信任协商

面向自动信任协商的防御措施研究

自动信任协商的攻击面临</code></pre></details>


由于资费与效率原因，本次实验基于Ada模型进行fine tune。可以看到，原始的Ada模型几乎完全没有get到文本摘要任务，只是在文本背景上生成了一段新的文本。在经过简单的fine tune后，虽然生成的文本仍然远不及ChatGPT或者其他在该任务上做过精细微调的大模型，但是已经能在一定程度上生成一个还算不错的摘要了。

如果有需要在一个微调模型上继续微调，直接将fine_tunes.create的-m参数改为微调后的模型名称即可。例如对于以上案例，可使用：

```python
!openai api fine_tunes.create \
    -t "./dataset/csl_summarize_finetune_prepared.jsonl" \
    -m ada:ft-personal-2023-04-15-13-29-50\
    --no_check_if_files_exist
```

<details class="lake-collapse"><summary id="u5e6ef8b2"><span class="ne-text" style="color: rgba(0, 0, 0, 0.87)">output：</span></summary><pre data-language="json" id="Ig7f5" class="ne-codeblock language-json"><code>Uploaded file from ./dataset/csl_summarize_finetune_prepared.jsonl: file-adsjU97Wo9bPNmdAa1LTMkQC
Created fine-tune: ft-d6qvvl7cr6WYvkSOBu7YVO2p
Streaming events until fine-tuning is complete...

(Ctrl-C will interrupt the stream, but not cancel the fine-tune)
[2023-05-07 15:44:48] Created fine-tune: ft-d6qvvl7cr6WYvkSOBu7YVO2p
[2023-05-07 15:45:03] Fine-tune costs $0.43
[2023-05-07 15:45:03] Fine-tune enqueued. Queue number: 0
[2023-05-07 15:45:05] Fine-tune started

Stream interrupted (client disconnected).
To resume the stream, run:

  openai api fine_tunes.follow -i ft-d6qvvl7cr6WYvkSOBu7YVO2p
Upload progress:   0%|          | 0.00/380k [00:00&lt;?, ?it/s]
Upload progress: 100%|██████████| 380k/380k [00:00&lt;?, ?it/s]</code></pre></details>




