![画板](https://cdn.nlark.com/yuque/0/2025/jpeg/2639475/1736928998022-fafff281-7bb7-4739-9266-9a34fb6c0e72.jpeg)

---

找点资料

# 地址：
1. github：[llm-action/README.md at main · liguodongiot/llm-action](https://github.com/liguodongiot/llm-action/blob/main/README.md#llm%E9%87%8F%E5%8C%96)
2. llm-resource ：[https://github.com/liguodongiot/llm-resource](https://github.com/liguodongiot/llm-resource)
3. ai-system：[https://github.com/liguodongiot/ai-system](https://github.com/liguodongiot/ai-system)
4. 微信：[AI工程化核心课程推荐：基础、大模型理论、模型压缩等](https://mp.weixin.qq.com/s?__biz=MzU3Mzg5ODgxMg==&mid=2247488019&idx=1&sn=90ca4657a643431f21df44fa199e695f&chksm=fd3bfb40ca4c72565fdb91717c8a27739bcb71d10fae89c7994f141d375d21059be19e702164&token=701439972&lang=zh_CN#rd)

## LLM训练
### LLM训练实战
下面汇总了我在大模型实践中训练相关的所有教程。从6B到65B，从全量微调到高效微调（LoRA，QLoRA，P-Tuning v2），再到RLHF（基于人工反馈的强化学习）。

| LLM | 预训练/SFT/RLHF... | 参数 | 教程 | 代码 |
| --- | --- | --- | --- | --- |
| Alpaca | full fine-turning | 7B | [从0到1复现斯坦福羊驼（Stanford Alpaca 7B）](https://zhuanlan.zhihu.com/p/618321077) | [配套代码](https://github.com/liguodongiot/llm-action/tree/main/llm-train/alpaca) |
| Alpaca(LLaMA) | LoRA | 7B~65B | 1.[足够惊艳，使用Alpaca-Lora基于LLaMA(7B)二十分钟完成微调，效果比肩斯坦福羊驼](https://zhuanlan.zhihu.com/p/619426866)   2. [使用 LoRA 技术对 LLaMA 65B 大模型进行微调及推理](https://zhuanlan.zhihu.com/p/632492604) | [配套代码](https://github.com/liguodongiot/llm-action/tree/main/llm-train/alpaca-lora) |
| BELLE(LLaMA/Bloom) | full fine-turning | 7B | 1.[基于LLaMA-7B/Bloomz-7B1-mt复现开源中文对话大模型BELLE及GPTQ量化](https://zhuanlan.zhihu.com/p/618876472)    2. [BELLE(LLaMA-7B/Bloomz-7B1-mt)大模型使用GPTQ量化后推理性能测试](https://zhuanlan.zhihu.com/p/621128368) | N/A |
| ChatGLM | LoRA | 6B | [从0到1基于ChatGLM-6B使用LoRA进行参数高效微调](https://zhuanlan.zhihu.com/p/621793987) | [配套代码](https://github.com/liguodongiot/llm-action/tree/main/train/chatglm-lora) |
| ChatGLM | full fine-turning/P-Tuning v2 | 6B | [使用DeepSpeed/P-Tuning v2对ChatGLM-6B进行微调](https://zhuanlan.zhihu.com/p/622351059) | [配套代码](https://github.com/liguodongiot/llm-action/tree/main/train/chatglm) |
| Vicuna(LLaMA) | full fine-turning | 7B | [大模型也内卷，Vicuna训练及推理指南，效果碾压斯坦福羊驼](https://zhuanlan.zhihu.com/p/624012908) | N/A |
| OPT | RLHF | 0.1B~66B | 1.[一键式 RLHF 训练 DeepSpeed Chat（一）：理论篇](https://zhuanlan.zhihu.com/p/626159553)    2. [一键式 RLHF 训练 DeepSpeed Chat（二）：实践篇](https://zhuanlan.zhihu.com/p/626214655) | [配套代码](https://github.com/liguodongiot/llm-action/tree/main/train/deepspeedchat) |
| MiniGPT-4(LLaMA) | full fine-turning | 7B | [大杀器，多模态大模型MiniGPT-4入坑指南](https://zhuanlan.zhihu.com/p/627671257) | N/A |
| Chinese-LLaMA-Alpaca(LLaMA) | LoRA（预训练+微调） | 7B | [中文LLaMA&Alpaca大语言模型词表扩充+预训练+指令精调](https://zhuanlan.zhihu.com/p/631360711) | [配套代码](https://github.com/liguodongiot/llm-action/tree/main/train/chinese-llama-alpaca) |
| LLaMA | QLoRA | 7B/65B | [高效微调技术QLoRA实战，基于LLaMA-65B微调仅需48G显存，真香](https://zhuanlan.zhihu.com/p/636644164) | [配套代码](https://github.com/liguodongiot/llm-action/tree/main/train/qlora) |
| LLaMA | GaLore | 60M/7B | [突破内存瓶颈，使用 GaLore 一张4090消费级显卡也能预训练LLaMA-7B](https://zhuanlan.zhihu.com/p/686686751) | [配套代码](https://github.com/liguodongiot/llm-action/blob/main/train/galore/torchrun_main.py) |


### LLM微调技术原理
对于普通大众来说，进行大模型的预训练或者全量微调遥不可及。由此，催生了各种参数高效微调技术，让科研人员或者普通开发者有机会尝试微调大模型。

因此，该技术值得我们进行深入分析其背后的机理，本系列大体分七篇文章进行讲解。

+ [大模型参数高效微调技术原理综述（一）-背景、参数高效微调简介](https://zhuanlan.zhihu.com/p/635152813)
+ [大模型参数高效微调技术原理综述（二）-BitFit、Prefix Tuning、Prompt Tuning](https://zhuanlan.zhihu.com/p/635686756)
+ [大模型参数高效微调技术原理综述（三）-P-Tuning、P-Tuning v2](https://zhuanlan.zhihu.com/p/635848732)
+ [大模型参数高效微调技术原理综述（四）-Adapter Tuning及其变体](https://zhuanlan.zhihu.com/p/636038478)
+ [大模型参数高效微调技术原理综述（五）-LoRA、AdaLoRA、QLoRA](https://zhuanlan.zhihu.com/p/636215898)
+ [大模型参数高效微调技术原理综述（六）-MAM Adapter、UniPELT](https://zhuanlan.zhihu.com/p/636362246)
+ [大模型参数高效微调技术原理综述（七）-最佳实践、总结](https://zhuanlan.zhihu.com/p/649755252)

### LLM微调实战
下面给大家分享**大模型参数高效微调技术实战**，该系列主要针对 HuggingFace PEFT 框架支持的一些高效微调技术进行讲解。

| 教程 | 代码 | 框架 |
| --- | --- | --- |
| [大模型参数高效微调技术实战（一）-PEFT概述及环境搭建](https://zhuanlan.zhihu.com/p/651744834) | N/A | HuggingFace PEFT |
| [大模型参数高效微调技术实战（二）-Prompt Tuning](https://zhuanlan.zhihu.com/p/646748939) | [配套代码](https://github.com/liguodongiot/llm-action/blob/main/llm-action/peft/clm/peft_prompt_tuning_clm.ipynb) | HuggingFace PEFT |
| [大模型参数高效微调技术实战（三）-P-Tuning](https://zhuanlan.zhihu.com/p/646876256) | [配套代码](https://github.com/liguodongiot/llm-action/blob/main/llm-action/peft/clm/peft_p_tuning_clm.ipynb) | HuggingFace PEFT |
| [大模型参数高效微调技术实战（四）-Prefix Tuning / P-Tuning v2](https://zhuanlan.zhihu.com/p/648156780) | [配套代码](https://github.com/liguodongiot/llm-action/blob/main/llm-action/peft/clm/peft_p_tuning_v2_clm.ipynb) | HuggingFace PEFT |
| [大模型参数高效微调技术实战（五）-LoRA](https://zhuanlan.zhihu.com/p/649315197) | [配套代码](https://github.com/liguodongiot/llm-action/blob/main/llm-action/peft/clm/peft_lora_clm.ipynb) | HuggingFace PEFT |
| [大模型参数高效微调技术实战（六）-IA3](https://zhuanlan.zhihu.com/p/649707359) | [配套代码](https://github.com/liguodongiot/llm-action/blob/main/llm-action/peft/clm/peft_ia3_clm.ipynb) | HuggingFace PEFT |
| [大模型微调实战（七）-基于LoRA微调多模态大模型](https://zhuanlan.zhihu.com/p/670048482) | [配套代码](https://github.com/liguodongiot/llm-action/blob/main/llm-action/peft/multimodal/blip2_lora_int8_fine_tune.py) | HuggingFace PEFT |
| [大模型微调实战（八）-使用INT8/FP4/NF4微调大模型](https://zhuanlan.zhihu.com/p/670116171) | [配套代码](https://github.com/liguodongiot/llm-action/blob/main/llm-action/peft/multimodal/finetune_bloom_bnb_peft.ipynb) | PEFT、bitsandbytes |


### [LLM分布式训练并行技术](https://github.com/liguodongiot/llm-action/tree/main/docs/llm-base/distribution-parallelism)
近年来，随着Transformer、MOE架构的提出，使得深度学习模型轻松突破上万亿规模参数，传统的单机单卡模式已经无法满足超大模型进行训练的要求。因此，我们需要基于单机多卡、甚至是多机多卡进行分布式大模型的训练。

而利用AI集群，使深度学习算法更好地从大量数据中高效地训练出性能优良的大模型是分布式机器学习的首要目标。为了实现该目标，一般需要根据硬件资源与数据/模型规模的匹配情况，考虑对计算任务、训练数据和模型进行划分，从而进行分布式训练。因此，分布式训练相关技术值得我们进行深入分析其背后的机理。

下面主要对大模型进行分布式训练的并行技术进行讲解，本系列大体分九篇文章进行讲解。

+ [大模型分布式训练并行技术（一）-概述](https://zhuanlan.zhihu.com/p/598714869)
+ [大模型分布式训练并行技术（二）-数据并行](https://zhuanlan.zhihu.com/p/650002268)
+ [大模型分布式训练并行技术（三）-流水线并行](https://zhuanlan.zhihu.com/p/653860567)
+ [大模型分布式训练并行技术（四）-张量并行](https://zhuanlan.zhihu.com/p/657921100)
+ [大模型分布式训练并行技术（五）-序列并行](https://zhuanlan.zhihu.com/p/659792351)
+ [大模型分布式训练并行技术（六）-多维混合并行](https://zhuanlan.zhihu.com/p/661279318)
+ [大模型分布式训练并行技术（七）-自动并行](https://zhuanlan.zhihu.com/p/662517647)
+ [大模型分布式训练并行技术（八）-MOE并行](https://zhuanlan.zhihu.com/p/662518387)
+ [大模型分布式训练并行技术（九）-总结](https://zhuanlan.zhihu.com/p/667051845)



### 分布式AI框架
+ [PyTorch](https://github.com/liguodongiot/llm-action/tree/main/train/pytorch/)
    - PyTorch 单机多卡训练
    - PyTorch 多机多卡训练
+ [Megatron-LM](https://github.com/liguodongiot/llm-action/tree/main/train/megatron)
    - Megatron-LM 单机多卡训练
    - Megatron-LM 多机多卡训练
    - [基于Megatron-LM从0到1完成GPT2模型预训练、模型评估及推理](https://juejin.cn/post/7259682893648724029)
+ [DeepSpeed](https://github.com/liguodongiot/llm-action/tree/main/train/deepspeed)
    - DeepSpeed 单机多卡训练
    - DeepSpeed 多机多卡训练
+ [Megatron-DeepSpeed](https://github.com/liguodongiot/llm-action/tree/main/train/megatron-deepspeed)
    - 基于 Megatron-DeepSpeed 从 0 到1 完成 LLaMA 预训练
    - 基于 Megatron-DeepSpeed 从 0 到1 完成 Bloom 预训练



### 分布式训练网络通信
待更新...



### LLM训练优化技术
+ FlashAttention V1、V2
+ 混合精度训练
+ 重计算
+ MQA / GQA
+ 梯度累积



### LLM对齐技术
+ PPO（近端策略优化）
+ DPO
+ ORPO





## [LLM推理](https://github.com/liguodongiot/llm-action/tree/main/inference)
### 模型推理引擎
+ [大模型推理框架概述](https://www.zhihu.com/question/625415776/answer/3243562246)
+ [大模型的好伙伴，浅析推理加速引擎FasterTransformer](https://zhuanlan.zhihu.com/p/626008090)
+ [TensorRT-LLM保姆级教程（一）-快速入门](https://zhuanlan.zhihu.com/p/666849728)
+ [TensorRT-LLM保姆级教程（二）-离线环境搭建、模型量化及推理](https://zhuanlan.zhihu.com/p/667572720)
+ [TensorRT-LLM保姆级教程（三）-使用Triton推理服务框架部署模型](https://juejin.cn/post/7398122968200593419)
+ TensorRT-LLM保姆级教程（四）-新模型适配
+ vLLM
+ [LightLLM](https://github.com/ModelTC/lightllm)：纯python开发的大语言模型推理和服务框架
+ [MNN-LLM](https://github.com/alibaba/MNN)：基于MNN引擎开发的大型语言模型运行时解决方案



### 模型推理服务
+ [模型推理服务工具综述](https://zhuanlan.zhihu.com/p/721395381)
+ [模型推理服务化框架Triton保姆式教程（一）：快速入门](https://zhuanlan.zhihu.com/p/629336492)
+ [模型推理服务化框架Triton保姆式教程（二）：架构解析](https://zhuanlan.zhihu.com/p/634143650)
+ [模型推理服务化框架Triton保姆式教程（三）：开发实践](https://zhuanlan.zhihu.com/p/634444666)



### LLM推理优化技术
+ LLM推理优化技术-概述
+ [大模型推理优化技术-KV Cache](https://www.zhihu.com/question/653658936/answer/3569365986)
+ [大模型推理服务调度优化技术-Continuous batching](https://zhuanlan.zhihu.com/p/719610083)
+ [大模型低显存推理优化-Offload技术](https://juejin.cn/post/7405158045628596224)
+ [大模型推理优化技术-KV Cache量化](https://juejin.cn/post/7420231738558627874)
+ 大模型推理服务调度优化技术-Chunked Prefill
+ 大模型推理优化技术-KV Cache优化方法综述
+ 大模型吞吐优化技术-多LoRA推理服务
+ 大模型推理服务调度优化技术-公平性调度
+ 大模型访存优化技术-FlashAttention
+ 大模型显存优化技术-PagedAttention
+ 大模型解码优化-Speculative Decoding及其变体
+ 大模型推理优化-结构化文本生成
+ Flash Decoding
+ FlashDecoding++



## LLM压缩
近年来，随着Transformer、MOE架构的提出，使得深度学习模型轻松突破上万亿规模参数，从而导致模型变得越来越大，因此，我们需要一些大模型压缩技术来降低模型部署的成本，并提升模型的推理性能。  
模型压缩主要分为如下几类：

+ 模型剪枝（Pruning）
+ 知识蒸馏（Knowledge Distillation）
+ 模型量化（Quantization）
+ 低秩分解（Low-Rank Factorization）

### LLM量化
本系列将针对一些常见大模型量化方案（GPTQ、LLM.int8()、SmoothQuant、AWQ等）进行讲述。

+ [大模型量化概述](https://www.zhihu.com/question/627484732/answer/3261671478)
+ 量化感知训练：
    - [大模型量化感知训练技术原理：LLM-QAT](https://zhuanlan.zhihu.com/p/647589650)
    - 大模型量化感知微调技术原理：QLoRA
    - PEQA
+ 训练后量化：
    - [大模型量化技术原理：GPTQ、LLM.int8()](https://zhuanlan.zhihu.com/p/680212402)
    - [大模型量化技术原理：SmoothQuant](https://www.zhihu.com/question/576376372/answer/3388402085)
    - [大模型量化技术原理：AWQ、AutoAWQ](https://zhuanlan.zhihu.com/p/681578090)
    - [大模型量化技术原理：SpQR](https://zhuanlan.zhihu.com/p/682871823)
    - [大模型量化技术原理：ZeroQuant系列](https://zhuanlan.zhihu.com/p/683813769)
    - [大模型量化技术原理：FP8](https://www.zhihu.com/question/658712811/answer/3596678896)
    - [大模型量化技术原理：FP6](https://juejin.cn/post/7412893752090853386)
    - [大模型量化技术原理：KIVI、IntactKV、KVQuant](https://juejin.cn/post/7420231738558627874)
    - [大模型量化技术原理：Atom、QuaRot](https://juejin.cn/post/7424334647570513972)
    - [大模型量化技术原理：QoQ量化及QServe推理服务系统](https://zhuanlan.zhihu.com/p/8047106486)
    - 大模型量化技术原理：QuIP、QuIP#、OmniQuant
    - 大模型量化技术原理：FP4
+ [大模型量化技术原理：总结](https://zhuanlan.zhihu.com/p/11886909512)



### LLM剪枝
+ [万字长文谈深度神经网络剪枝综述](https://zhuanlan.zhihu.com/p/692858636?)



目前，大多数针对大模型模型的压缩技术都专注于模型量化领域，即降低单个权重的数值表示的精度。另一种模型压缩方法模型剪枝的研究相对较少，即删除网络元素，包括从单个权重（非结构化剪枝）到更高粒度的组件，如权重矩阵的整行/列（结构化剪枝）。

本系列将针对一些常见大模型剪枝方案（LLM-Pruner、SliceGPT、SparseGPT、Wanda等）进行讲述。

+ [大模型剪枝技术原理：概述](https://www.zhihu.com/question/652126515/answer/3457652467)
+ 大模型剪枝技术原理：LLM-Pruner、SliceGPT
+ 大模型剪枝技术原理：SparseGPT、Wanda
+ 大模型剪枝技术原理：总结



**结构化剪枝**：

+ LLM-Pruner(LLM-Pruner: On the Structural Pruning of Large Language Models)
+ LLM-Shearing(Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning)
+ SliceGPT: Compress Large Language Models by Deleting Rows and Columns
+ LoSparse



**非结构化剪枝**：

+ SparseGPT(SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot)
+ LoRAPrune(LoRAPrune: Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning)
+ Wanda(A Simple and Effective Pruning Approach for Large Language Models)
+ Flash-LLM(Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity)



### LLM知识蒸馏
+ [大模型知识蒸馏概述](https://www.zhihu.com/question/625415893/answer/3243565375)

**Standard KD**:

使学生模型学习教师模型(LLM)所拥有的常见知识，如输出分布和特征信息，这种方法类似于传统的KD。

+ MINILLM
+ GKD

**EA-based KD**:

不仅仅是将LLM的常见知识转移到学生模型中，还涵盖了蒸馏它们独特的涌现能力。具体来说，EA-based KD又分为了上下文学习（ICL）、思维链（CoT）和指令跟随（IF）。

In-Context Learning：

+ In-Context Learning distillation

Chain-of-Thought：

+ MT-COT
+ Fine-tune-CoT
+ DISCO
+ SCOTT
+ SOCRATIC CoT

Instruction Following：

+ Lion

### 低秩分解
低秩分解旨在通过将给定的权重矩阵分解成两个或多个较小维度的矩阵，从而对其进行近似。低秩分解背后的核心思想是找到一个大的权重矩阵W的分解，得到两个矩阵U和V，使得W≈U V，其中U是一个m×k矩阵，V是一个k×n矩阵，其中k远小于m和n。U和V的乘积近似于原始的权重矩阵，从而大幅减少了参数数量和计算开销。

在LLM研究的模型压缩领域，研究人员通常将多种技术与低秩分解相结合，包括修剪、量化等。

+ ZeroQuant-FP（低秩分解+量化）
+ LoRAPrune（低秩分解+剪枝）



## LLM测评
### LLM效果测评
+ [C-Eval](https://github.com/liguodongiot/ceval)：全面的中文基础模型评估套件，涵盖了52个不同学科的13948个多项选择题，分为四个难度级别。
+ [CMMLU](https://github.com/liguodongiot/CMMLU)：一个综合性的中文评估基准，专门用于评估语言模型在中文语境下的知识和推理能力。CMMLU涵盖了从基础学科到高级专业水平的67个主题。它包括：需要计算和推理的自然科学，需要知识的人文科学和社会科学,以及需要生活常识的中国驾驶规则等。此外，CMMLU中的许多任务具有中国特定的答案，可能在其他地区或语言中并不普遍适用。因此是一个完全中国化的中文测试基准。
+ [LVEval](https://github.com/liguodongiot/LVEval)：一个具备5个长度等级（16k、32k、64k、128k和256k）、最大文本测试长度达到256k的长文本评测基准。LV-Eval的平均文本长度达到102,380字，最小/最大文本长度为11,896/387,406字。LV-Eval主要有两类评测任务——单跳QA和多跳QA，共包含11个涵盖中英文的评测数据子集。LV-Eval设计时引入3个关键技术：干扰事实插入（Confusiong Facts Insertion，CFI）提高挑战性，关键词和短语替换（Keyword and Phrase Replacement，KPR）减少信息泄漏，以及基于关键词召回的评测指标（Answer Keywords，AK，指代结合答案关键词和字词黑名单的评价指标）提高评测数值客观性。
+ [IFEval: Instruction Following Eval](https://github.com/google-research/google-research/tree/master/instruction_following_eval)/[Paper](https://arxiv.org/abs/2311.07911)：专注评估大模型遵循指令的能力,包含关键词检测、标点控制、输出格式要求等25种任务。
+ [SuperCLUE](https://github.com/CLUEbenchmark/SuperCLUE)：一个综合性大模型评测基准，本次评测主要聚焦于大模型的四个能力象限，包括语言理解与生成、专业技能与知识、Agent智能体和安全性，进而细化为12项基础能力。
+ [AGIEval](https://github.com/ruixiangcui/AGIEval/)：用于评估基础模型在与人类认知和解决问题相关的任务中的能力。该基准源自 20 项面向普通考生的官方、公开、高标准的入学和资格考试，例如：普通大学入学考试（例如：中国高考（Gaokao）和美国 SAT）、法学院入学考试、数学竞赛、律师资格考试、国家公务员考试。
+ [OpenCompass](https://github.com/open-compass/opencompass/blob/main/README_zh-CN.md)：司南 2.0 大模型评测体系。

..... 后面有用再打开



